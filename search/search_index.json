{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>This libary contains code that I use frequently. I find useful to organize and document the code for future project.</p>"},{"location":"api/interpretability/","title":"API Introduction","text":"<p><code>easyroutine.interpretability</code> is the module that implement code for extract the hidden rappresentations of HuggingFace LLMs and intervening on the forward pass.</p>"},{"location":"api/interpretability/#simple-tutorial","title":"Simple Tutorial","text":"<pre><code># First we need to import the HookedModel and the config classes\nfrom easyroutine.interpretability import HookedModel, ExtractionConfig\n\n# Then we can create the hooked model\nhooked_model = HookedModel.from_pretrained(model_name=\"mistral-community/pixtral-12b\", device_map = \"auto\")\n\n# Now let's define a simple dataset\ndataset = [\n    \"This is a test\",\n    \"This is another test\"\n]\n\ntokenizer = hooked_model.get_tokenizer()\n\ndataset = tokenizer(dataset, padding=True, truncation=True, return_tensors=\"pt\") \n\ncache = hooked_model.extract_cache(\n    dataset,\n    target_token_positions = [\"last\"],\n    extraction_config = ExtractionConfig(\n        extract_resid_out = True\n    )\n)\n\n</code></pre>"},{"location":"api/interpretability/activation_cache/","title":"Activation cache","text":""},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache","title":"<code>ActivationCache</code>","text":"<p>Class to store and aggregate activation values from a model. It is a dictionary-like object with additional functionality to aggregate values.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>class ActivationCache():\n    r\"\"\"\n    Class to store and aggregate activation values from a model.\n    It is a dictionary-like object with additional functionality to aggregate values.\n    \"\"\"\n\n    def __init__(self):\n        self.cache = {}\n        self.logger = Logger(\n            logname=\"ActivationCache\",\n            level=\"INFO\",\n        )\n\n        self.valid_keys = (\n            re.compile(r\"resid_out_\\d+\"),\n            re.compile(r\"resid_in_\\d+\"),\n            re.compile(r\"resid_mid_\\d+\"),\n            re.compile(r\"attn_in_\\d+\"),\n            re.compile(r\"attn_out_\\d+\"),\n            re.compile(r\"avg_attn_pattern_L\\dH\\d+\"),\n            re.compile(r\"pattern_L\\dH\\d+\"),\n            re.compile(r\"values_\\d+\"),\n            re.compile(r\"input_ids\"),\n            re.compile(r\"mapping_index\"),\n            re.compile(r\"mlp_out_\\d+\"),\n        )\n\n        self.aggregation_strategies = {}\n        self.register_aggregation(\"mapping_index\", lambda values: values[0])  # First value\n        self.register_aggregation(\"pattern_\", lambda values: values)  # Keep as list\n        self.register_aggregation(\"input_ids\", lambda values: values)  # Keep as list\n        self.register_aggregation(\"offset\", lambda values: [item for sublist in values for item in sublist])  # Flatten lists\n\n        self.defferred_cache = False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Returns:\n            str: A string representation of the ActivationCache object.\n\n        Examples:\n            &gt;&gt;&gt; cache\n            ActivationCache(resid_out_0, resid_in_0, resid_mid_0, attn_in_0, attn_out_0, avg_attn_pattern_L1H1, pattern_L1H1, values_L1H1)\n        \"\"\"\n        return f\"ActivationCache(`{', '.join(self.cache.keys())}`)\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Returns:\n            str: A string representation of the ActivationCache object.\n\n        Examples:\n            &gt;&gt;&gt; print(cache)\n            ActivationCache(resid_out_0: torch.Tensor([1, 2, 3, 4]), resid_in_0: torch.Tensor([1, 2, 3, 4]))\n        \"\"\"\n        return f\"ActivationCache({', '.join([f'{key}: {value}' for key, value in self.cache.items()])})\"\n\n    def __setitem__(self, key:str, value):\n        \"\"\"\n        Set a key-value pair in the cache.\n\n        Arguments:\n            key (str): The key to store the value.\n            value (Any): The value to store.\n\n        Examples:\n            &gt;&gt;&gt; cache[\"resid_out_0\"] = torch.randn(1, 3, 16)\n        \"\"\"\n        if not any([pattern.match(key) for pattern in self.valid_keys]):\n            self.logger.warning(f\"Invalid key: {key}. Valid keys are: {self.valid_keys}. Could be a user-defined key.\")\n        self.cache[key] = value\n\n    def __getitem__(self, key:str):\n        \"\"\"\n        Get a value from the cache.\n\n        Arguments:\n            key (str): The key to retrieve the value.\n\n        Examples:\n            &gt;&gt;&gt; cache[\"resid_out_0\"]\n            torch.Tensor([1, 2, 3, 4])\n        \"\"\"\n        return self.cache[key]\n\n\n    def __delitem__(self, key:str):\n        \"\"\"\n        Remove a key-value pair from the cache.\n\n        Arguments:\n            key (str): The key to remove from the cache.\n        \"\"\"\n        del self.cache[key]\n\n    def __add__(self, other) -&gt; \"ActivationCache\":\n        \"\"\"\n        Overload the `+` operator to merge caches efficiently.\n        Arguments:\n            other (dict or ActivationCache): Another cache or dictionary to merge with.\n        Returns:\n            ActivationCache: A new ActivationCache object with merged data.\n        \"\"\"\n        if not isinstance(other, (dict, ActivationCache)):\n            raise TypeError(\"Can only add ActivationCache or dict objects.\")\n\n        new_cache = ActivationCache()\n        new_cache.cache = {**self.cache, **(other.cache if isinstance(other, ActivationCache) else other)}\n        return new_cache\n\n    def __contains__(self, key):\n        \"\"\"\n        Check if a key is present in the cache.\n        Arguments:\n            key (str): The key to check.\n        Returns:\n            bool: True if the key is present, False otherwise\n        \"\"\"\n        return key in self.cache\n\n    def get(self, key:str, default=None):\n        return self.cache.get(key, default)\n\n    def items(self):\n        \"\"\"\n        Just like the dictionary items method, returns a list of key-value pairs.\n        \"\"\"\n        return self.cache.items()\n\n    def keys(self):\n        \"\"\"\n        Just like the dictionary keys method, returns a list of keys.\n        \"\"\"\n        return self.cache.keys()\n\n    def values(self):\n        \"\"\"\n        Just like the dictionary values method, returns a list of values.\n        \"\"\"\n        return self.cache.values()\n\n    def update(self, other):\n        \"\"\"\n        Updates the cache with values from an additional dictionary or ActivationCache object.\n        Arguments:\n            other (Union[dict, ActivationCache]): Dictionary or ActivationCache object to update with.\n        \"\"\"\n        if isinstance(other, dict):\n            self.cache.update(other)\n        elif isinstance(other, ActivationCache):\n            self.cache.update(other.cache)\n        else:\n            raise TypeError(\"Can only update with dict or ActivationCache objects.\")\n\n\n\n    def to(self, device: Union[str, torch.device]):\n        \"\"\"\n        Moves the tensors in the cache to a specified device.\n\n        Args:\n            device (Union[str, torch.device]): The device to move the tensors to.\n        \"\"\"\n\n        for key, value in self.cache.items():\n            if hasattr(value, \"to\"):\n                self.cache[key] = value.to(device)\n\n    def cpu(self):\n        \"\"\"\n        Moves the tensors in the cache to the CPU.\n        \"\"\"\n        self.to(\"cpu\")\n\n    def cuda(self):\n        \"\"\"\n        Moves the tensors in the cache to the GPU.\n        \"\"\"\n        self.to(\"cuda\")\n\n\n    def register_aggregation(self, key_pattern, function):\n        \"\"\"\n        Register a custom aggregation strategy for keys matching a pattern. In this way, you can define how to aggregate values for specific keys when merging caches.\n\n        Arguments:\n            key_pattern (str): The key or prefix to match.\n            function (callable): The function to apply for aggregation.\n\n        Examples:\n            &gt;&gt;&gt; cache.register_aggregation(\"values_\", lambda values: torch.stack(values, dim=0))\n        \"\"\"\n        self.aggregation_strategies[key_pattern] = function\n\n\n    def default_aggregation(self, values):\n        \"\"\"\n        Default aggregation strategy for keys without a custom strategy.\n        Handles tensors, lists, and scalars.\n\n        Arguments:\n            values (List): List of values to aggregate.\n\n        Returns:\n            Union[torch.Tensor, List, Any]: The aggregated value.\n        \"\"\"\n        if isinstance(values[0], torch.Tensor):\n            try:\n                return torch.cat(values, dim=0)\n            except RuntimeError:\n                return torch.stack(values, dim=0)\n        elif isinstance(values[0], list):\n            return [item for sublist in values for item in sublist]\n        else:\n            return values[0]  # Fallback to the first value\n\n    @contextlib.contextmanager\n    def deferred_mode(self):\n        \"\"\"\n        Context manager to enable deferred aggregation.\n        Collects all external caches in a list and aggregates them at the end of the context.\n        This is most similar to the old way of using the `cat` method. It could (or could not) be more efficient.\n        The main difference to direct calls to `cat` is that the cache is not updated until the end of the context, in this way the torch.cat, torch.stack and the other strategies are called only once.\n        It will require more memory, but it could be more efficient.\n\n        Examples:\n            &gt;&gt;&gt; with cache.deferred_mode():\n            &gt;&gt;&gt;     cache.cat(external_cache1)\n            &gt;&gt;&gt;     cache.cat(external_cache2)\n        \"\"\"\n        self.deferred_cache = []\n        try:\n            yield self\n            # Perform aggregation at the end of the context\n            for external_cache in self.deferred_cache:\n                self.cat(external_cache)\n        finally:\n            # Clear the deferred cache\n            self.deferred_cache = None\n\n    def cat(self, external_cache):\n        \"\"\"\n        Merge the current cache with an external cache using aggregation strategies.\n\n        Arguments:\n            external_cache (ActivationCache): The external cache to merge with.\n\n\n        Examples:\n            &gt;&gt;&gt; a, b = ActivationCache(), ActivationCache()\n            &gt;&gt;&gt; a[\"values_0\"] = torch.tensor([1, 2])\n            &gt;&gt;&gt; b[\"values_0\"] = torch.tensor([1, 4])\n            &gt;&gt;&gt; a.cat(b)\n            &gt;&gt;&gt; print(a[\"values_0\"].shape)\n            torch.Size([2,1])\n            &gt;&gt;&gt; print(a[\"values_0\"])\n            tensor([[2], [4]]\n        \"\"\"\n        if not isinstance(external_cache, ActivationCache):\n            raise TypeError(\"external_cache must be an instance of ActivationCache\")\n\n        # Case 1: Initialize self if it's empty\n        if not self.cache and external_cache.cache:\n            self.update(external_cache.cache)\n            return\n\n        # Case 2: Ensure both caches have the same keys\n        self_keys = set(self.cache.keys())\n        external_keys = set(external_cache.cache.keys())\n\n        if self_keys != external_keys:\n            raise ValueError(\n                f\"Key mismatch: self has {self_keys - external_keys}, \"\n                f\"external has {external_keys - self_keys}\"\n            )\n\n        # Case 3: Aggregate matching keys using registered strategies or default\n        for key in self.cache:\n            # Check for a custom aggregation strategy\n            for pattern, strategy in self.aggregation_strategies.items():\n                if key.startswith(pattern):\n                    self.cache[key] = strategy([self.cache[key], external_cache[key]])\n                    break\n            else:\n                # Use the default aggregation if no custom strategy matches\n                self.cache[key] = self.default_aggregation(\n                    [self.cache[key], external_cache[key]]\n                )\n    def add_with_info(self, key: str, value, info: str):\n        \"\"\"\n        Stores the 'value' under 'key' but wraps it in an object that provides\n        a .info() method returning the 'info' string.\n\n        Arguments:\n            key (str): The cache key.\n            value (Any): The object to store, e.g. a tensor or list.\n            info (str): The associated info string.\n\n        Examples:\n            &gt;&gt;&gt; cache.add_with_info(\"resid_out_0\", torch.randn(1, 3, 16), \"shape: batch x seq x hidden\")\n            &gt;&gt;&gt; cache[\"resid_out_0\"].info()\n            shape: batch x seq x hidden\n            &gt;&gt;&gt; cache[\"resid_out_0\"].shape\n            torch.Size([1, 3, 16])\n            &gt;&gt;&gt; cache[\"resid_out_0\"]\n            tensor([[[1, 2, 3, 4]]])\n\n        \"\"\"\n\n        class ValueWithInfo:\n            \"\"\"\n            Thin wrapper around the original value to store extra info.\n            \"\"\"\n            __slots__ = (\"_value\", \"_info\")  # optional for memory efficiency\n\n            def __init__(self, value, info):\n                self._value = value\n                self._info = info\n\n            def info(self):\n                \"\"\"\n                Return the custom info string.\n                \"\"\"\n                return self._info\n\n            def value(self):\n                \"\"\"\n                Return the value.\n                \"\"\"\n                return self._value\n\n            def __getattr__(self, name):\n                \"\"\"\n                Forward attribute lookups to the wrapped value.\n                \"\"\"\n                return getattr(self._value, name)\n\n            def __repr__(self):\n                return f\"ValueWithInfo(value={self._value!r}, info={self._info!r})\"\n\n        wrapped = ValueWithInfo(value, info)\n        self[key] = wrapped\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__add__","title":"<code>__add__(other)</code>","text":"<p>Overload the <code>+</code> operator to merge caches efficiently. Arguments:     other (dict or ActivationCache): Another cache or dictionary to merge with. Returns:     ActivationCache: A new ActivationCache object with merged data.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __add__(self, other) -&gt; \"ActivationCache\":\n    \"\"\"\n    Overload the `+` operator to merge caches efficiently.\n    Arguments:\n        other (dict or ActivationCache): Another cache or dictionary to merge with.\n    Returns:\n        ActivationCache: A new ActivationCache object with merged data.\n    \"\"\"\n    if not isinstance(other, (dict, ActivationCache)):\n        raise TypeError(\"Can only add ActivationCache or dict objects.\")\n\n    new_cache = ActivationCache()\n    new_cache.cache = {**self.cache, **(other.cache if isinstance(other, ActivationCache) else other)}\n    return new_cache\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__contains__","title":"<code>__contains__(key)</code>","text":"<p>Check if a key is present in the cache. Arguments:     key (str): The key to check. Returns:     bool: True if the key is present, False otherwise</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __contains__(self, key):\n    \"\"\"\n    Check if a key is present in the cache.\n    Arguments:\n        key (str): The key to check.\n    Returns:\n        bool: True if the key is present, False otherwise\n    \"\"\"\n    return key in self.cache\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Remove a key-value pair from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to remove from the cache.</p> required Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __delitem__(self, key:str):\n    \"\"\"\n    Remove a key-value pair from the cache.\n\n    Arguments:\n        key (str): The key to remove from the cache.\n    \"\"\"\n    del self.cache[key]\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get a value from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cache[\"resid_out_0\"]\ntorch.Tensor([1, 2, 3, 4])\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __getitem__(self, key:str):\n    \"\"\"\n    Get a value from the cache.\n\n    Arguments:\n        key (str): The key to retrieve the value.\n\n    Examples:\n        &gt;&gt;&gt; cache[\"resid_out_0\"]\n        torch.Tensor([1, 2, 3, 4])\n    \"\"\"\n    return self.cache[key]\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the ActivationCache object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cache\nActivationCache(resid_out_0, resid_in_0, resid_mid_0, attn_in_0, attn_out_0, avg_attn_pattern_L1H1, pattern_L1H1, values_L1H1)\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Returns:\n        str: A string representation of the ActivationCache object.\n\n    Examples:\n        &gt;&gt;&gt; cache\n        ActivationCache(resid_out_0, resid_in_0, resid_mid_0, attn_in_0, attn_out_0, avg_attn_pattern_L1H1, pattern_L1H1, values_L1H1)\n    \"\"\"\n    return f\"ActivationCache(`{', '.join(self.cache.keys())}`)\"\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a key-value pair in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cache[\"resid_out_0\"] = torch.randn(1, 3, 16)\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __setitem__(self, key:str, value):\n    \"\"\"\n    Set a key-value pair in the cache.\n\n    Arguments:\n        key (str): The key to store the value.\n        value (Any): The value to store.\n\n    Examples:\n        &gt;&gt;&gt; cache[\"resid_out_0\"] = torch.randn(1, 3, 16)\n    \"\"\"\n    if not any([pattern.match(key) for pattern in self.valid_keys]):\n        self.logger.warning(f\"Invalid key: {key}. Valid keys are: {self.valid_keys}. Could be a user-defined key.\")\n    self.cache[key] = value\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.__str__","title":"<code>__str__()</code>","text":"<p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the ActivationCache object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(cache)\nActivationCache(resid_out_0: torch.Tensor([1, 2, 3, 4]), resid_in_0: torch.Tensor([1, 2, 3, 4]))\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns:\n        str: A string representation of the ActivationCache object.\n\n    Examples:\n        &gt;&gt;&gt; print(cache)\n        ActivationCache(resid_out_0: torch.Tensor([1, 2, 3, 4]), resid_in_0: torch.Tensor([1, 2, 3, 4]))\n    \"\"\"\n    return f\"ActivationCache({', '.join([f'{key}: {value}' for key, value in self.cache.items()])})\"\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.add_with_info","title":"<code>add_with_info(key, value, info)</code>","text":"<p>Stores the 'value' under 'key' but wraps it in an object that provides a .info() method returning the 'info' string.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The cache key.</p> required <code>value</code> <code>Any</code> <p>The object to store, e.g. a tensor or list.</p> required <code>info</code> <code>str</code> <p>The associated info string.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cache.add_with_info(\"resid_out_0\", torch.randn(1, 3, 16), \"shape: batch x seq x hidden\")\n&gt;&gt;&gt; cache[\"resid_out_0\"].info()\nshape: batch x seq x hidden\n&gt;&gt;&gt; cache[\"resid_out_0\"].shape\ntorch.Size([1, 3, 16])\n&gt;&gt;&gt; cache[\"resid_out_0\"]\ntensor([[[1, 2, 3, 4]]])\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def add_with_info(self, key: str, value, info: str):\n    \"\"\"\n    Stores the 'value' under 'key' but wraps it in an object that provides\n    a .info() method returning the 'info' string.\n\n    Arguments:\n        key (str): The cache key.\n        value (Any): The object to store, e.g. a tensor or list.\n        info (str): The associated info string.\n\n    Examples:\n        &gt;&gt;&gt; cache.add_with_info(\"resid_out_0\", torch.randn(1, 3, 16), \"shape: batch x seq x hidden\")\n        &gt;&gt;&gt; cache[\"resid_out_0\"].info()\n        shape: batch x seq x hidden\n        &gt;&gt;&gt; cache[\"resid_out_0\"].shape\n        torch.Size([1, 3, 16])\n        &gt;&gt;&gt; cache[\"resid_out_0\"]\n        tensor([[[1, 2, 3, 4]]])\n\n    \"\"\"\n\n    class ValueWithInfo:\n        \"\"\"\n        Thin wrapper around the original value to store extra info.\n        \"\"\"\n        __slots__ = (\"_value\", \"_info\")  # optional for memory efficiency\n\n        def __init__(self, value, info):\n            self._value = value\n            self._info = info\n\n        def info(self):\n            \"\"\"\n            Return the custom info string.\n            \"\"\"\n            return self._info\n\n        def value(self):\n            \"\"\"\n            Return the value.\n            \"\"\"\n            return self._value\n\n        def __getattr__(self, name):\n            \"\"\"\n            Forward attribute lookups to the wrapped value.\n            \"\"\"\n            return getattr(self._value, name)\n\n        def __repr__(self):\n            return f\"ValueWithInfo(value={self._value!r}, info={self._info!r})\"\n\n    wrapped = ValueWithInfo(value, info)\n    self[key] = wrapped\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.cat","title":"<code>cat(external_cache)</code>","text":"<p>Merge the current cache with an external cache using aggregation strategies.</p> <p>Parameters:</p> Name Type Description Default <code>external_cache</code> <code>ActivationCache</code> <p>The external cache to merge with.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; a, b = ActivationCache(), ActivationCache()\n&gt;&gt;&gt; a[\"values_0\"] = torch.tensor([1, 2])\n&gt;&gt;&gt; b[\"values_0\"] = torch.tensor([1, 4])\n&gt;&gt;&gt; a.cat(b)\n&gt;&gt;&gt; print(a[\"values_0\"].shape)\ntorch.Size([2,1])\n&gt;&gt;&gt; print(a[\"values_0\"])\ntensor([[2], [4]]\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def cat(self, external_cache):\n    \"\"\"\n    Merge the current cache with an external cache using aggregation strategies.\n\n    Arguments:\n        external_cache (ActivationCache): The external cache to merge with.\n\n\n    Examples:\n        &gt;&gt;&gt; a, b = ActivationCache(), ActivationCache()\n        &gt;&gt;&gt; a[\"values_0\"] = torch.tensor([1, 2])\n        &gt;&gt;&gt; b[\"values_0\"] = torch.tensor([1, 4])\n        &gt;&gt;&gt; a.cat(b)\n        &gt;&gt;&gt; print(a[\"values_0\"].shape)\n        torch.Size([2,1])\n        &gt;&gt;&gt; print(a[\"values_0\"])\n        tensor([[2], [4]]\n    \"\"\"\n    if not isinstance(external_cache, ActivationCache):\n        raise TypeError(\"external_cache must be an instance of ActivationCache\")\n\n    # Case 1: Initialize self if it's empty\n    if not self.cache and external_cache.cache:\n        self.update(external_cache.cache)\n        return\n\n    # Case 2: Ensure both caches have the same keys\n    self_keys = set(self.cache.keys())\n    external_keys = set(external_cache.cache.keys())\n\n    if self_keys != external_keys:\n        raise ValueError(\n            f\"Key mismatch: self has {self_keys - external_keys}, \"\n            f\"external has {external_keys - self_keys}\"\n        )\n\n    # Case 3: Aggregate matching keys using registered strategies or default\n    for key in self.cache:\n        # Check for a custom aggregation strategy\n        for pattern, strategy in self.aggregation_strategies.items():\n            if key.startswith(pattern):\n                self.cache[key] = strategy([self.cache[key], external_cache[key]])\n                break\n        else:\n            # Use the default aggregation if no custom strategy matches\n            self.cache[key] = self.default_aggregation(\n                [self.cache[key], external_cache[key]]\n            )\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.cpu","title":"<code>cpu()</code>","text":"<p>Moves the tensors in the cache to the CPU.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def cpu(self):\n    \"\"\"\n    Moves the tensors in the cache to the CPU.\n    \"\"\"\n    self.to(\"cpu\")\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.cuda","title":"<code>cuda()</code>","text":"<p>Moves the tensors in the cache to the GPU.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def cuda(self):\n    \"\"\"\n    Moves the tensors in the cache to the GPU.\n    \"\"\"\n    self.to(\"cuda\")\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.default_aggregation","title":"<code>default_aggregation(values)</code>","text":"<p>Default aggregation strategy for keys without a custom strategy. Handles tensors, lists, and scalars.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List</code> <p>List of values to aggregate.</p> required <p>Returns:</p> Type Description <p>Union[torch.Tensor, List, Any]: The aggregated value.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def default_aggregation(self, values):\n    \"\"\"\n    Default aggregation strategy for keys without a custom strategy.\n    Handles tensors, lists, and scalars.\n\n    Arguments:\n        values (List): List of values to aggregate.\n\n    Returns:\n        Union[torch.Tensor, List, Any]: The aggregated value.\n    \"\"\"\n    if isinstance(values[0], torch.Tensor):\n        try:\n            return torch.cat(values, dim=0)\n        except RuntimeError:\n            return torch.stack(values, dim=0)\n    elif isinstance(values[0], list):\n        return [item for sublist in values for item in sublist]\n    else:\n        return values[0]  # Fallback to the first value\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.deferred_mode","title":"<code>deferred_mode()</code>","text":"<p>Context manager to enable deferred aggregation. Collects all external caches in a list and aggregates them at the end of the context. This is most similar to the old way of using the <code>cat</code> method. It could (or could not) be more efficient. The main difference to direct calls to <code>cat</code> is that the cache is not updated until the end of the context, in this way the torch.cat, torch.stack and the other strategies are called only once. It will require more memory, but it could be more efficient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with cache.deferred_mode():\n&gt;&gt;&gt;     cache.cat(external_cache1)\n&gt;&gt;&gt;     cache.cat(external_cache2)\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>@contextlib.contextmanager\ndef deferred_mode(self):\n    \"\"\"\n    Context manager to enable deferred aggregation.\n    Collects all external caches in a list and aggregates them at the end of the context.\n    This is most similar to the old way of using the `cat` method. It could (or could not) be more efficient.\n    The main difference to direct calls to `cat` is that the cache is not updated until the end of the context, in this way the torch.cat, torch.stack and the other strategies are called only once.\n    It will require more memory, but it could be more efficient.\n\n    Examples:\n        &gt;&gt;&gt; with cache.deferred_mode():\n        &gt;&gt;&gt;     cache.cat(external_cache1)\n        &gt;&gt;&gt;     cache.cat(external_cache2)\n    \"\"\"\n    self.deferred_cache = []\n    try:\n        yield self\n        # Perform aggregation at the end of the context\n        for external_cache in self.deferred_cache:\n            self.cat(external_cache)\n    finally:\n        # Clear the deferred cache\n        self.deferred_cache = None\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.items","title":"<code>items()</code>","text":"<p>Just like the dictionary items method, returns a list of key-value pairs.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def items(self):\n    \"\"\"\n    Just like the dictionary items method, returns a list of key-value pairs.\n    \"\"\"\n    return self.cache.items()\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.keys","title":"<code>keys()</code>","text":"<p>Just like the dictionary keys method, returns a list of keys.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def keys(self):\n    \"\"\"\n    Just like the dictionary keys method, returns a list of keys.\n    \"\"\"\n    return self.cache.keys()\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.register_aggregation","title":"<code>register_aggregation(key_pattern, function)</code>","text":"<p>Register a custom aggregation strategy for keys matching a pattern. In this way, you can define how to aggregate values for specific keys when merging caches.</p> <p>Parameters:</p> Name Type Description Default <code>key_pattern</code> <code>str</code> <p>The key or prefix to match.</p> required <code>function</code> <code>callable</code> <p>The function to apply for aggregation.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cache.register_aggregation(\"values_\", lambda values: torch.stack(values, dim=0))\n</code></pre> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def register_aggregation(self, key_pattern, function):\n    \"\"\"\n    Register a custom aggregation strategy for keys matching a pattern. In this way, you can define how to aggregate values for specific keys when merging caches.\n\n    Arguments:\n        key_pattern (str): The key or prefix to match.\n        function (callable): The function to apply for aggregation.\n\n    Examples:\n        &gt;&gt;&gt; cache.register_aggregation(\"values_\", lambda values: torch.stack(values, dim=0))\n    \"\"\"\n    self.aggregation_strategies[key_pattern] = function\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.to","title":"<code>to(device)</code>","text":"<p>Moves the tensors in the cache to a specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, device]</code> <p>The device to move the tensors to.</p> required Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def to(self, device: Union[str, torch.device]):\n    \"\"\"\n    Moves the tensors in the cache to a specified device.\n\n    Args:\n        device (Union[str, torch.device]): The device to move the tensors to.\n    \"\"\"\n\n    for key, value in self.cache.items():\n        if hasattr(value, \"to\"):\n            self.cache[key] = value.to(device)\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.update","title":"<code>update(other)</code>","text":"<p>Updates the cache with values from an additional dictionary or ActivationCache object. Arguments:     other (Union[dict, ActivationCache]): Dictionary or ActivationCache object to update with.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def update(self, other):\n    \"\"\"\n    Updates the cache with values from an additional dictionary or ActivationCache object.\n    Arguments:\n        other (Union[dict, ActivationCache]): Dictionary or ActivationCache object to update with.\n    \"\"\"\n    if isinstance(other, dict):\n        self.cache.update(other)\n    elif isinstance(other, ActivationCache):\n        self.cache.update(other.cache)\n    else:\n        raise TypeError(\"Can only update with dict or ActivationCache objects.\")\n</code></pre>"},{"location":"api/interpretability/activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache.values","title":"<code>values()</code>","text":"<p>Just like the dictionary values method, returns a list of values.</p> Source code in <code>easyroutine/interpretability/activation_cache.py</code> <pre><code>def values(self):\n    \"\"\"\n    Just like the dictionary values method, returns a list of values.\n    \"\"\"\n    return self.cache.values()\n</code></pre>"},{"location":"api/interpretability/hooked_model/","title":"Hooked model","text":""},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.ExtractionConfig","title":"<code>ExtractionConfig</code>  <code>dataclass</code>","text":"<p>Configuration of the extraction of the activations of the model. It store what activations you want to extract from the model.</p> <p>Parameters:</p> Name Type Description Default <code>extract_resid_in</code> <code>bool</code> <p>if True, extract the input of the residual stream</p> <code>False</code> <code>extract_resid_mid</code> <code>bool</code> <p>if True, extract the output of the intermediate stream</p> <code>False</code> <code>extract_resid_out</code> <code>bool</code> <p>if True, extract the output of the residual stream</p> <code>False</code> <code>extract_resid_in_post_layernorm(bool)</code> <p>if True, extract the input of the residual stream after the layernorm</p> required <code>extract_attn_pattern</code> <code>bool</code> <p>if True, extract the attention pattern of the attn</p> <code>False</code> <code>extract_head_values_projected</code> <code>bool</code> <p>if True, extract the values vectors projected of the model</p> <code>False</code> <code>extract_head_values</code> <code>bool</code> <p>if True, extract the values of the attention</p> <code>False</code> <code>extract_head_out</code> <code>bool</code> <p>if True, extract the output of the heads [DEPRECATED]</p> <code>False</code> <code>extract_attn_out</code> <code>bool</code> <p>if True, extract the output of the attention of the attn_heads passed</p> <code>False</code> <code>extract_attn_in</code> <code>bool</code> <p>if True, extract the input of the attention of the attn_heads passed</p> <code>False</code> <code>extract_mlp_out</code> <code>bool</code> <p>if True, extract the output of the mlp of the attn</p> <code>False</code> <code>save_input_ids</code> <code>bool</code> <p>if True, save the input_ids in the cache</p> <code>False</code> <code>avg</code> <code>bool</code> <p>if True, extract the average of the activations over the target positions</p> <code>False</code> <code>avg_over_example</code> <code>bool</code> <p>if True, extract the average of the activations over the examples (it required a external cache to save the running avg)</p> <code>False</code> <code>attn_heads</code> <code>Union[list[dict], Literal['all']]</code> <p>list of dictionaries with the layer and head to extract the attention pattern or 'all' to</p> <code>'all'</code> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@dataclass\nclass ExtractionConfig:\n    \"\"\"\n    Configuration of the extraction of the activations of the model. It store what activations you want to extract from the model.\n\n    Arguments:\n        extract_resid_in (bool): if True, extract the input of the residual stream\n        extract_resid_mid (bool): if True, extract the output of the intermediate stream\n        extract_resid_out (bool): if True, extract the output of the residual stream\n        extract_resid_in_post_layernorm(bool): if True, extract the input of the residual stream after the layernorm\n        extract_attn_pattern (bool): if True, extract the attention pattern of the attn\n        extract_head_values_projected (bool): if True, extract the values vectors projected of the model\n        extract_head_values (bool): if True, extract the values of the attention\n        extract_head_out (bool): if True, extract the output of the heads [DEPRECATED]\n        extract_attn_out (bool): if True, extract the output of the attention of the attn_heads passed\n        extract_attn_in (bool): if True, extract the input of the attention of the attn_heads passed\n        extract_mlp_out (bool): if True, extract the output of the mlp of the attn\n        save_input_ids (bool): if True, save the input_ids in the cache\n        avg (bool): if True, extract the average of the activations over the target positions\n        avg_over_example (bool): if True, extract the average of the activations over the examples (it required a external cache to save the running avg)\n        attn_heads (Union[list[dict], Literal[\"all\"]]): list of dictionaries with the layer and head to extract the attention pattern or 'all' to\n    \"\"\"\n\n    extract_resid_in: bool = False\n    extract_resid_mid: bool = False\n    extract_resid_out: bool = False\n    extract_resid_in_post_layernorm: bool = False\n    extract_attn_pattern: bool = False\n    extract_head_values_projected: bool = False\n    # TODO: add extract_head_queries_projected\n    # TODO: add extract_head_keys_projected\n    extract_head_keys: bool = False\n    extract_head_values: bool = False\n    extract_head_queries: bool = False\n    extract_head_out: bool = False\n    extract_attn_out: bool = False\n    extract_attn_in: bool = False\n    extract_mlp_out: bool = False\n    save_input_ids: bool = False\n    avg: bool = False\n    avg_over_example: bool = False\n    attn_heads: Union[list[dict], Literal[\"all\"]] = \"all\"\n\n    def is_not_empty(self):\n        \"\"\"\n        Return True if at least one of the attributes is True, False otherwise, i.e. if the model should extract something!\n        \"\"\"\n        return any(\n            [\n                self.extract_resid_in,\n                self.extract_resid_mid,\n                self.extract_resid_out,\n                self.extract_attn_pattern,\n                self.extract_head_values_projected,\n                self.extract_head_keys,\n                self.extract_head_values,\n                self.extract_head_queries,\n                self.extract_head_out,\n                self.extract_attn_out,\n                self.extract_attn_in,\n                self.extract_mlp_out,\n                self.save_input_ids,\n                self.avg,\n                self.avg_over_example,\n            ]\n        )\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.ExtractionConfig.is_not_empty","title":"<code>is_not_empty()</code>","text":"<p>Return True if at least one of the attributes is True, False otherwise, i.e. if the model should extract something!</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def is_not_empty(self):\n    \"\"\"\n    Return True if at least one of the attributes is True, False otherwise, i.e. if the model should extract something!\n    \"\"\"\n    return any(\n        [\n            self.extract_resid_in,\n            self.extract_resid_mid,\n            self.extract_resid_out,\n            self.extract_attn_pattern,\n            self.extract_head_values_projected,\n            self.extract_head_keys,\n            self.extract_head_values,\n            self.extract_head_queries,\n            self.extract_head_out,\n            self.extract_attn_out,\n            self.extract_attn_in,\n            self.extract_mlp_out,\n            self.save_input_ids,\n            self.avg,\n            self.avg_over_example,\n        ]\n    )\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel","title":"<code>HookedModel</code>","text":"<p>This class is a wrapper around the huggingface model that allows to extract the activations of the model. It is support advanced mechanistic intepretability methods like ablation, patching, etc.</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>class HookedModel:\n    \"\"\"\n    This class is a wrapper around the huggingface model that allows to extract the activations of the model. It is support\n    advanced mechanistic intepretability methods like ablation, patching, etc.\n    \"\"\"\n\n    def __init__(self, config: HookedModelConfig, log_file_path: Optional[str] = None):\n        self.logger = Logger(\n            logname=\"HookedModel\",\n            level=\"info\",\n            log_file_path=log_file_path,\n        )\n\n        self.config = config\n        self.hf_model, self.hf_language_model, self.model_config = (\n            ModelFactory.load_model(\n                model_name=config.model_name,\n                device_map=config.device_map,\n                torch_dtype=config.torch_dtype,\n                attn_implementation=\"eager\"\n                if config.attn_implementation == \"custom_eager\"\n                else config.attn_implementation,\n            )\n        )\n        self.base_model = None\n        self.module_wrapper_manager = ModuleWrapperManager(model=self.hf_model)\n\n        tokenizer, processor = TokenizerFactory.load_tokenizer(\n            model_name=config.model_name,\n            torch_dtype=config.torch_dtype,\n            device_map=config.device_map,\n        )\n        self.hf_tokenizer = tokenizer\n        self.input_handler = InputHandler(model_name=config.model_name)\n        if processor is True:\n            self.processor = tokenizer\n            self.text_tokenizer = self.processor.tokenizer  # type: ignore\n        else:\n            self.processor = None\n            self.text_tokenizer = tokenizer\n\n        self.first_device = next(self.hf_model.parameters()).device\n        device_num = torch.cuda.device_count()\n        self.logger.info(\n            f\"Model loaded in {device_num} devices. First device: {self.first_device}\",\n            std_out=True,\n        )\n        self.act_type_to_hook_name = {\n            \"resid_in\": self.model_config.residual_stream_input_hook_name,\n            \"resid_out\": self.model_config.residual_stream_hook_name,\n            \"resid_mid\": self.model_config.intermediate_stream_hook_name,\n            \"attn_out\": self.model_config.attn_out_hook_name,\n            \"attn_in\": self.model_config.attn_in_hook_name,\n            \"values\": self.model_config.head_value_hook_name,\n            # Add other act_types if needed\n        }\n        self.additional_hooks = []\n        self.assert_all_modules_exist()\n\n        if self.config.attn_implementation == \"custom_eager\":\n            self.logger.info(\n                \"\"\"\n                            The model is using the custom eager attention implementation that support attention matrix hooks because I get config.attn_impelemntation == 'custom_eager'. If you don't want this, you can call HookedModel.restore_original_modules. \n                            However, we reccomend using this implementation since the base one do not contains attention matrix hook resulting in unexpected behaviours. \n                            \"\"\",\n                std_out=True,\n            )\n            self.set_custom_modules()\n\n    def __repr__(self):\n        return f\"\"\"HookedModel(model_name={self.config.model_name}):\n        {self.hf_model.__repr__()}\n    \"\"\"\n\n    @classmethod\n    def from_pretrained(cls, model_name: str, **kwargs):\n        return cls(HookedModelConfig(model_name=model_name, **kwargs))\n\n    def assert_module_exists(self, component: str):\n        # Remove '.input' or '.output' from the component\n        component = component.replace(\".input\", \"\").replace(\".output\", \"\")\n\n        # Check if '{}' is in the component, indicating layer indexing\n        if \"{}\" in component:\n            for i in range(0, self.model_config.num_hidden_layers):\n                attr_name = component.format(i)\n\n                try:\n                    get_attribute_by_name(self.hf_model, attr_name)\n                except AttributeError:\n                    try:\n                        if attr_name in self.module_wrapper_manager:\n                            self.set_custom_modules()\n                            get_attribute_by_name(self.hf_model, attr_name)\n                            self.restore_original_modules()\n                    except AttributeError:\n                        raise ValueError(\n                            f\"Component '{attr_name}' does not exist in the model. Please check the model configuration.\"\n                        )\n        else:\n            try:\n                get_attribute_by_name(self.hf_model, component)\n            except AttributeError:\n                raise ValueError(\n                    f\"Component '{component}' does not exist in the model. Please check the model configuration.\"\n                )\n\n    def assert_all_modules_exist(self):\n        # get the list of all attributes of model_config\n        all_attributes = [attr_name for attr_name in self.model_config.__dict__.keys()]\n        # save just the attributes that have \"hook\" in the name\n        hook_attributes = [\n            attr_name for attr_name in all_attributes if \"hook\" in attr_name\n        ]\n        for hook_attribute in hook_attributes:\n            self.assert_module_exists(getattr(self.model_config, hook_attribute))\n\n    def set_custom_modules(self):\n        self.logger.info(\"Setting custom modules.\", std_out=True)\n        self.module_wrapper_manager.substitute_attention_module(self.hf_model)\n\n    def restore_original_modules(self):\n        self.logger.info(\"Restoring original modules.\", std_out=True)\n        self.module_wrapper_manager.restore_original_attention_module(self.hf_model)\n\n    def use_full_model(self):\n        if self.processor is not None:\n            self.logger.info(\"Using full model capabilities\", std_out=True)\n        else:\n            if self.base_model is not None:\n                self.hf_model = self.base_model\n            self.logger.info(\"Using full text only model capabilities\", std_out=True)\n\n    def use_language_model_only(self):\n        if self.hf_language_model is None:\n            self.logger.warning(\n                \"The model does not have a separate language model that can be used\",\n                std_out=True,\n            )\n        else:\n            self.base_model = self.hf_model\n            self.hf_model = self.hf_language_model\n            self.logger.info(\"Using only language model capabilities\", std_out=True)\n\n    def get_tokenizer(self):\n        return self.hf_tokenizer\n\n    def get_text_tokenizer(self):\n        r\"\"\"\n        If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer\n\n        Args:\n            None\n\n        Returns:\n            tokenizer: the tokenizer of the model\n        \"\"\"\n        if self.processor is not None:\n            if not hasattr(self.processor, \"tokenizer\"):\n                raise ValueError(\"The processor does not have a tokenizer\")\n            return self.processor.tokenizer  # type: ignore\n        return self.hf_tokenizer\n\n    def get_processor(self):\n        r\"\"\"\n        Return the processor of the model (None if the model does not have a processor, i.e. text only model)\n\n        Args:\n            None\n\n        Returns:\n            processor: the processor of the model\n        \"\"\"\n        if self.processor is None:\n            raise ValueError(\"The model does not have a processor\")\n        return self.processor\n\n    def get_lm_head(self):\n        return get_attribute_by_name(self.hf_model, self.model_config.unembed_matrix)\n\n    def get_last_layernorm(self):\n        return get_attribute_by_name(self.hf_model, self.model_config.last_layernorm)\n\n    def eval(self):\n        r\"\"\"\n        Set the model in evaluation mode\n        \"\"\"\n        self.hf_model.eval()\n\n    def device(self):\n        r\"\"\"\n        Return the device of the model. If the model is in multiple devices, it will return the first device\n\n        Args:\n            None\n\n        Returns:\n            device: the device of the model\n        \"\"\"\n        return self.first_device\n\n    def register_forward_hook(self, component: str, hook_function: Callable):\n        r\"\"\"\n        Add a new hook to the model. The hook will be called in the forward pass of the model.\n\n        Args:\n            component (str): the component of the model where the hook will be added.\n            hook_function (Callable): the function that will be called in the forward pass of the model. The function must have the following signature:\n                def hook_function(module, input, output):\n                    pass\n\n        Returns:\n            None\n\n        Examples:\n            &gt;&gt;&gt; def hook_function(module, input, output):\n            &gt;&gt;&gt;     # your code here\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt; model.register_forward_hook(\"model.layers[0].self_attn\", hook_function)\n        \"\"\"\n        self.additional_hooks.append(\n            {\n                \"component\": component,\n                \"intervention\": hook_function,\n            }\n        )\n\n    def to_string_tokens(\n        self,\n        tokens: Union[list, torch.Tensor],\n    ):\n        r\"\"\"\n        Transform a list or a tensor of tokens in a list of string tokens.\n\n        Args:\n            tokens (Union[list, torch.Tensor]): the tokens to transform in string tokens\n\n        Returns:\n            string_tokens (list): the list of string tokens\n\n        Examples:\n            &gt;&gt;&gt; tokens = [101, 1234, 1235, 102]\n            &gt;&gt;&gt; model.to_string_tokens(tokens)\n            ['[CLS]', 'hello', 'world', '[SEP]']\n        \"\"\"\n        if isinstance(tokens, torch.Tensor):\n            if tokens.dim() == 1:\n                tokens = tokens.tolist()\n            else:\n                tokens = tokens.squeeze().tolist()\n        string_tokens = []\n        for tok in tokens:\n            string_tokens.append(self.hf_tokenizer.decode(tok))  # type: ignore\n        return string_tokens\n\n    def create_hooks(\n        self,\n        inputs,\n        cache: ActivationCache,\n        token_index: List,\n        token_dict: Dict,\n        # string_tokens: List[str],\n        extraction_config: ExtractionConfig = ExtractionConfig(),\n        patching_queries: Optional[Union[dict, pd.DataFrame]] = None,\n        ablation_queries: Optional[Union[dict, pd.DataFrame]] = None,\n        batch_idx: Optional[int] = None,\n        external_cache: Optional[ActivationCache] = None,\n    ):\n        r\"\"\"\n        Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.\n\n        Arguments:\n            inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n            cache (ActivationCache): dictionary where the activations of the model will be saved\n            extracted_token_position (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n            string_tokens (list[str]): list of string tokens\n            pivot_positions (Optional[list[int]]): list of split positions of the tokens\n            extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model (default = ExtractionConfig())\n            ablation_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the ablation queries to perform during forward pass\n            patching_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the patching queries to perform during forward pass\n            batch_idx (Optional[int]): index of the batch in the dataloader\n            external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n\n        Returns:\n            hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n        \"\"\"\n        hooks = []\n\n        # compute layer and head indexes\n        if (\n            isinstance(extraction_config.attn_heads, str)\n            and extraction_config.attn_heads == \"all\"\n        ):\n            layer_indexes = [i for i in range(0, self.model_config.num_hidden_layers)]\n            head_indexes = [\"all\"] * len(layer_indexes)\n        elif isinstance(extraction_config.attn_heads, list):\n            layer_head_indexes = [\n                (el[\"layer\"], el[\"head\"]) for el in extraction_config.attn_heads\n            ]\n            layer_indexes = [el[0] for el in layer_head_indexes]\n            head_indexes = [el[1] for el in layer_head_indexes]\n        else:\n            raise ValueError(\n                \"attn_heads must be 'all' or a list of dictionaries as [{'layer': 0, 'head': 0}]\"\n            )\n\n        if extraction_config.extract_resid_out:\n            # assert that the component exists in the model\n            hooks += [\n                {\n                    \"component\": self.model_config.residual_stream_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_out_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n        if extraction_config.extract_resid_in:\n            # assert that the component exists in the model\n            hooks += [\n                {\n                    \"component\": self.model_config.residual_stream_input_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_in_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_resid_in_post_layernorm:\n            hooks += [\n                {\n                    \"component\": self.model_config.residual_stream_input_post_layernorm_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_in_post_layernorm_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.save_input_ids:\n            hooks += [\n                {\n                    \"component\": self.model_config.embed_tokens,\n                    \"intervention\": partial(\n                        embed_hook,\n                        cache=cache,\n                        cache_key=\"input_ids\",\n                    ),\n                }\n            ]\n\n        if extraction_config.extract_head_queries:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_query_hook_name.format(i),\n                    \"intervention\": partial(\n                        query_key_value_hook,\n                        cache=cache,\n                        cache_key=\"queries_\",\n                        token_index=token_index,\n                        head_dim=self.model_config.head_dim,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_key_value_groups=self.model_config.num_key_value_groups,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_values:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_value_hook_name.format(i),\n                    \"intervention\": partial(\n                        query_key_value_hook,\n                        cache=cache,\n                        cache_key=\"values_\",\n                        token_index=token_index,\n                        head_dim=self.model_config.head_dim,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_key_value_groups=self.model_config.num_key_value_groups,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_keys:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_key_hook_name.format(i),\n                    \"intervention\": partial(\n                        query_key_value_hook,\n                        cache=cache,\n                        cache_key=\"keys_\",\n                        token_index=token_index,\n                        head_dim=self.model_config.head_dim,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_key_value_groups=self.model_config.num_key_value_groups,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_head_out:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_o_proj_input_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        head_out_hook,\n                        cache=cache,\n                        cache_key=\"head_out_\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                        layer=i,\n                        head=head,\n                        num_heads=self.model_config.num_attention_heads,\n                        head_dim=self.model_config.head_dim,\n                        o_proj_weight=get_attribute_from_name(\n                            self.hf_model,\n                            self.model_config.attn_out_proj_weight.format(i),\n                        ),\n                        o_proj_bias=get_attribute_from_name(\n                            self.hf_model,\n                            self.model_config.attn_out_proj_bias.format(i),\n                        ),\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_attn_in:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_in_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"attn_in_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        if extraction_config.extract_attn_out:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_out_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"attn_out_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        # if extraction_config.extract_avg:\n        #     # Define a hook that saves the activations of the residual stream\n        #     raise NotImplementedError(\n        #         \"The hook for the average is not working with token_index as a list\"\n        #     )\n\n        #     # hooks.extend(\n        #     #     [\n        #     #         {\n        #     #             \"component\": self.model_config.residual_stream_hook_name.format(\n        #     #                 i\n        #     #             ),\n        #     #             \"intervention\": partial(\n        #     #                 avg_hook,\n        #     #                 cache=cache,\n        #     #                 cache_key=\"resid_avg_{}\".format(i),\n        #     #                 last_image_idx=last_image_idxs, #type\n        #     #                 end_image_idx=end_image_idxs,\n        #     #             ),\n        #     #         }\n        #     #         for i in range(0, self.model_config.num_hidden_layers)\n        #     #     ]\n        #     # )\n        if extraction_config.extract_resid_mid:\n            hooks += [\n                {\n                    \"component\": self.model_config.intermediate_stream_hook_name.format(\n                        i\n                    ),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"resid_mid_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n            # if we want to extract the output of the heads\n        if extraction_config.extract_mlp_out:\n            hooks += [\n                {\n                    \"component\": self.model_config.mlp_out_hook_name.format(i),\n                    \"intervention\": partial(\n                        save_resid_hook,\n                        cache=cache,\n                        cache_key=f\"mlp_out_{i}\",\n                        token_index=token_index,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i in range(0, self.model_config.num_hidden_layers)\n            ]\n\n        # PATCHING\n        if patching_queries:\n            token_to_pos = partial(\n                map_token_to_pos,\n                _get_token_index=token_dict,\n                # string_tokens=string_tokens,\n                hf_tokenizer=self.hf_tokenizer,\n                inputs=inputs,\n            )\n            patching_queries = preprocess_patching_queries(\n                patching_queries=patching_queries,\n                map_token_to_pos=token_to_pos,\n                model_config=self.model_config,\n            )\n\n            def make_patch_tokens_hook(patching_queries_subset):\n                \"\"\"\n                Creates a hook function to patch the activations in the\n                current forward pass.\n                \"\"\"\n\n                def patch_tokens_hook(\n                    module, args, kwargs, output\n                ):  # TODO: Move to hook.py\n                    b = process_args_kwargs_output(args, kwargs, output)\n                    # Modify the tensor without affecting the computation graph\n                    act_to_patch = b.detach().clone()\n                    for pos, patch in zip(\n                        patching_queries_subset[\"pos_token_to_patch\"],\n                        patching_queries_subset[\"patching_activations\"],\n                    ):\n                        act_to_patch[0, pos, :] = patching_queries_subset[\n                            \"patching_activations\"\n                        ].values[0]\n\n                    if output is None:\n                        if isinstance(input, tuple):\n                            return (act_to_patch, *input[1:])\n                        elif input is not None:\n                            return act_to_patch\n                    else:\n                        if isinstance(output, tuple):\n                            return (act_to_patch, *output[1:])\n                        elif output is not None:\n                            return act_to_patch\n                    raise ValueError(\"No output or input found\")\n\n                return patch_tokens_hook\n\n            # Group the patching queries by 'layer' and 'act_type'\n            grouped_queries = patching_queries.groupby([\"layer\", \"activation_type\"])\n\n            for (layer, act_type), group in grouped_queries:\n                hook_name_template = self.act_type_to_hook_name.get(\n                    act_type[:-3]\n                )  # -3 because we remove {}\n                if not hook_name_template:\n                    raise ValueError(f\"Unknown activation type: {act_type}\")\n                    # continue  # Skip unknown activation types\n\n                hook_name = hook_name_template.format(layer)\n                hook_function = partial(make_patch_tokens_hook(group))\n\n                hooks.append(\n                    {\n                        \"component\": hook_name,\n                        \"intervention\": hook_function,\n                    }\n                )\n\n        if ablation_queries is not None:\n            # TODO: debug and test the ablation. Check with ale\n            token_to_pos = partial(\n                map_token_to_pos,\n                _get_token_index=token_dict,\n                # string_tokens=string_tokens,\n                hf_tokenizer=self.hf_tokenizer,\n                inputs=inputs,\n            )\n            if self.config.batch_size &gt; 1:\n                raise ValueError(\"Ablation is not supported with batch size &gt; 1\")\n            ablation_manager = AblationManager(\n                model_config=self.model_config,\n                token_to_pos=token_to_pos,\n                inputs=inputs,\n                model_attn_type=self.config.attn_implementation,\n                ablation_queries=pd.DataFrame(ablation_queries)\n                if isinstance(ablation_queries, dict)\n                else ablation_queries,\n            )\n            hooks.extend(ablation_manager.main())\n\n        if extraction_config.extract_head_values_projected:\n            hooks += [\n                {\n                    \"component\": self.model_config.head_value_hook_name.format(i),\n                    \"intervention\": partial(\n                        projected_value_vectors_head,\n                        cache=cache,\n                        token_index=token_index,\n                        layer=i,\n                        num_attention_heads=self.model_config.num_attention_heads,\n                        num_key_value_heads=self.model_config.num_key_value_heads,\n                        hidden_size=self.model_config.hidden_size,\n                        d_head=self.model_config.head_dim,\n                        out_proj_weight=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                        ),\n                        out_proj_bias=get_attribute_from_name(\n                            self.hf_model,\n                            f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                        ),\n                        head=head,\n                        avg=extraction_config.avg,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        if extraction_config.extract_attn_pattern:\n            if extraction_config.avg:\n                if external_cache is None:\n                    self.logger.warning(\n                        \"\"\"The external_cache is None. The average could not be computed since missing an external cache where store the iterations.\n                        \"\"\"\n                    )\n                elif batch_idx is None:\n                    self.logger.warning(\n                        \"\"\"The batch_idx is None. The average could not be computed since missing the batch index.\n\n                        \"\"\"\n                    )\n                else:\n                    # move the cache to the same device of the model\n                    external_cache.to(self.first_device)\n                    hooks += [\n                        {\n                            \"component\": self.model_config.attn_matrix_hook_name.format(\n                                i\n                            ),\n                            \"intervention\": partial(\n                                avg_attention_pattern_head,\n                                token_index=token_index,\n                                layer=i,\n                                attn_pattern_current_avg=external_cache,\n                                batch_idx=batch_idx,\n                                cache=cache,\n                                # avg=extraction_config.avg,\n                                extract_avg_value=extraction_config.extract_head_values_projected,\n                            ),\n                        }\n                        for i in range(0, self.model_config.num_hidden_layers)\n                    ]\n            else:\n                hooks += [\n                    {\n                        \"component\": self.model_config.attn_matrix_hook_name.format(i),\n                        \"intervention\": partial(\n                            attention_pattern_head,\n                            token_index=token_index,\n                            cache=cache,\n                            layer=i,\n                            head=head,\n                        ),\n                    }\n                    for i, head in zip(layer_indexes, head_indexes)\n                ]\n\n            # if additional hooks are not empty, add them to the hooks list\n        if self.additional_hooks:\n            hooks += self.additional_hooks\n        return hooks\n\n    @torch.no_grad()\n    def forward(\n        self,\n        inputs,\n        target_token_positions: List[str] = [\"last\"],\n        pivot_positions: Optional[List[int]] = None,\n        extraction_config: ExtractionConfig = ExtractionConfig(),\n        ablation_queries: Optional[List[dict]] = None,\n        patching_queries: Optional[List[dict]] = None,\n        external_cache: Optional[ActivationCache] = None,\n        attn_heads: Union[list[dict], Literal[\"all\"]] = \"all\",\n        batch_idx: Optional[int] = None,\n        move_to_cpu: bool = False,\n    ) -&gt; ActivationCache:\n        r\"\"\"\n        Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.\n\n        Args:\n            inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n            target_token_positions (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n            pivot_positions (Optional[list[int]]): list of split positions of the tokens\n            extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model\n            ablation_queries (Optional[pd.DataFrame | None]): dataframe with the ablation queries to perform during forward pass\n            patching_queries (Optional[pd.DataFrame | None]): dataframe with the patching queries to perform during forward pass\n            external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n            attn_heads (Union[list[dict], Literal[\"all\"]]): list of dictionaries with the layer and head to extract the attention pattern or 'all' to\n            batch_idx (Optional[int]): index of the batch in the dataloader\n            move_to_cpu (bool): if True, move the activations to the cpu\n\n        Returns:\n            cache (ActivationCache): dictionary with the activations of the model\n\n        Examples:\n            &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n            &gt;&gt;&gt; model.forward(inputs, target_token_positions=[\"last\"], extract_resid_out=True)\n            {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'input_ids': tensor([[101, 1234, 1235, 102]]), 'mapping_index': {'last': [0]}}\n        \"\"\"\n\n        if target_token_positions is None and extraction_config.is_not_empty():\n            raise ValueError(\n                \"target_token_positions must be passed if we want to extract the activations of the model\"\n            )\n        cache = ActivationCache()\n        string_tokens = self.to_string_tokens(\n            self.input_handler.get_input_ids(inputs).squeeze()\n        )\n        token_index, token_dict = TokenIndex(\n            self.config.model_name, pivot_positions=pivot_positions\n        ).get_token_index(\n            tokens=target_token_positions,\n            string_tokens=string_tokens,\n            return_type=\"all\",\n        )\n        assert isinstance(token_index, list), \"Token index must be a list\"\n        assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n\n        hooks = self.create_hooks(  # TODO: add **kwargs\n            inputs=inputs,\n            token_dict=token_dict,\n            token_index=token_index,\n            cache=cache,\n            extraction_config=extraction_config,\n            ablation_queries=ablation_queries,\n            patching_queries=patching_queries,\n            batch_idx=batch_idx,\n            external_cache=external_cache,\n        )\n\n        hook_handlers = self.set_hooks(hooks)\n        inputs = self.input_handler.prepare_inputs(\n            inputs, self.first_device, self.config.torch_dtype\n        )\n        # forward pass\n        output = self.hf_model(\n            **inputs,\n            # output_original_output=True,\n            # output_attentions=extract_attn_pattern,\n        )\n\n        cache[\"logits\"] = output.logits[:, -1]\n        # since attention_patterns are returned in the output, we need to adapt to the cache structure\n        if move_to_cpu:\n            cache.cpu()\n            if external_cache is not None:\n                external_cache.cpu()\n\n        mapping_index = {}\n        current_index = 0\n        for token in target_token_positions:\n            mapping_index[token] = []\n            if isinstance(token_dict, int):\n                mapping_index[token].append(current_index)\n                current_index += 1\n            elif isinstance(token_dict, dict):\n                for idx in range(len(token_dict[token])):\n                    mapping_index[token].append(current_index)\n                    current_index += 1\n            elif isinstance(token_dict, list):\n                for idx in range(len(token_dict)):\n                    mapping_index[token].append(current_index)\n                    current_index += 1\n            else:\n                raise ValueError(\"Token dict must be an int, a dict or a list\")\n        cache[\"mapping_index\"] = mapping_index\n\n        self.remove_hooks(hook_handlers)\n\n        return cache\n\n    def __call__(self, *args, **kwds) -&gt; ActivationCache:\n        r\"\"\"\n        Call the forward method of the model\n        \"\"\"\n        return self.forward(*args, **kwds)\n\n    def predict(self, k=10, **kwargs):\n        out = self.forward(**kwargs)\n        logits = out[\"logits\"]\n        probs = torch.softmax(logits, dim=-1)\n        probs = probs.squeeze()\n        topk = torch.topk(probs, k)\n        # return a dictionary with the topk tokens and their probabilities\n        string_tokens = self.to_string_tokens(topk.indices)\n        token_probs = {}\n        for token, prob in zip(string_tokens, topk.values):\n            if token not in token_probs:\n                token_probs[token] = prob.item()\n        return token_probs\n        # return {\n        #     token: prob.item() for token, prob in zip(string_tokens, topk.values)\n        # }\n\n    def get_module_from_string(self, component: str):\n        r\"\"\"\n        Return a module from the model given the string of the module.\n\n        Args:\n            component (str): the string of the module\n\n        Returns:\n            module (torch.nn.Module): the module of the model\n\n        Examples:\n            &gt;&gt;&gt; model.get_module_from_string(\"model.layers[0].self_attn\")\n            BertAttention(...)\n        \"\"\"\n        return self.hf_model.retrieve_modules_from_names(component)\n\n    def set_hooks(self, hooks: List[Dict[str, Any]]):\n        r\"\"\"\n        Set the hooks in the model\n\n        Args:\n            hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n\n        Returns:\n            hook_handlers (list): list of hook handlers\n        \"\"\"\n\n        if len(hooks) == 0:\n            return []\n\n        hook_handlers = []\n        for hook in hooks:\n            component = hook[\"component\"]\n            hook_function = hook[\"intervention\"]\n\n            # get the last module string (.input or .output) and remove it from the component string\n            last_module = component.split(\".\")[-1]\n            # now remove the last module from the component string\n            component = component[: -len(last_module) - 1]\n            # check if the component exists in the model\n            try:\n                self.assert_module_exists(component)\n            except ValueError as e:\n                self.logger.warning(\n                    f\"Error: {e}. Probably the module {component} do not exists in the model. If the module is the attention_matrix_hook, try callig HookedModel.set_custom_hooks() or setting attn_implementation == 'custom_eager'.  Now we will skip the hook for the component {component}\",\n                    std_out=True,\n                )\n                continue\n            if last_module == \"input\":\n                hook_handlers.append(\n                    get_module_by_path(\n                        self.hf_model, component\n                    ).register_forward_pre_hook(\n                        partial(hook_function, output=None), with_kwargs=True\n                    )\n                )\n            elif last_module == \"output\":\n                hook_handlers.append(\n                    get_module_by_path(self.hf_model, component).register_forward_hook(\n                        hook_function, with_kwargs=True\n                    )\n                )\n\n        return hook_handlers\n\n    def remove_hooks(self, hook_handlers):\n        \"\"\"\n        Remove all the hooks from the model\n        \"\"\"\n        for hook_handler in hook_handlers:\n            hook_handler.remove()\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs,\n        generation_config: Optional[GenerationConfig] = None,\n        target_token_positions: Optional[List[str]] = None,\n        return_text: bool = False,\n        **kwargs,\n    ) -&gt; ActivationCache:\n        r\"\"\"\n        __WARNING__: This method could be buggy in the return dict of the output. Pay attention!\n\n        Generate new tokens using the model and the inputs passed as argument\n        Args:\n            inputs (dict): dictionary with the inputs of the model {\"input_ids\": ..., \"attention_mask\": ..., \"pixel_values\": ...}\n            generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration\n            **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)\n        Returns:\n            output (ActivationCache): dictionary with the output of the model\n\n        Examples:\n            &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n            &gt;&gt;&gt; model.generate(inputs)\n            {'sequences': tensor([[101, 1234, 1235, 102]])}\n        \"\"\"\n        # Initialize cache for logits\n        # TODO FIX THIS. IT is not general and it is not working\n        # raise NotImplementedError(\"This method is not working. It needs to be fixed\")\n        hook_handlers = None\n        if target_token_positions is not None:\n            string_tokens = self.to_string_tokens(\n                self.input_handler.get_input_ids(inputs).squeeze()\n            )\n            token_index, token_dict = TokenIndex(\n                self.config.model_name, pivot_positions=None\n            ).get_token_index(tokens=[], string_tokens=string_tokens, return_type=\"all\")\n            assert isinstance(token_index, list), \"Token index must be a list\"\n            assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n            hooks = self.create_hooks(\n                inputs=inputs,\n                token_dict=token_dict,\n                token_index=token_index,\n                cache=ActivationCache(),\n                **kwargs,\n            )\n            hook_handlers = self.set_hooks(hooks)\n\n        inputs = self.input_handler.prepare_inputs(inputs, self.first_device)\n\n        output = self.hf_model.generate(\n            **inputs,  # type: ignore\n            generation_config=generation_config,\n            output_scores=False,  # type: ignore\n        )\n        if hook_handlers:\n            self.remove_hooks(hook_handlers)\n        if return_text:\n            return self.hf_tokenizer.decode(output[0], skip_special_tokens=True)  # type: ignore\n        return output  # type: ignore\n\n    @torch.no_grad()\n    def extract_cache(\n        self,\n        dataloader,\n        target_token_positions: List[str],\n        batch_saver: Callable = lambda x: None,\n        move_to_cpu_after_forward: bool = True,\n        # save_other_batch_elements: bool = False,\n        **kwargs,\n    ):\n        r\"\"\"\n        Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.\n\n        Args:\n            dataloader (iterable): dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)\n            extracted_token_position (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n            batch_saver (Callable): function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)\n            move_to_cpu_after_forward (bool): if True, move the activations to the cpu right after the any forward pass of the model\n            **kwargs: additional arguments to control hooks generation, basically accept any argument handled by the `.forward` method (i.e. ablation_queries, patching_queries, extract_resid_in)\n\n        Returns:\n            final_cache: dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n        Examples:\n            &gt;&gt;&gt; dataloader = [{\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]]), \"labels\": torch.tensor([1])}, ...]\n            &gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[\"last\"], batch_saver=lambda x: {\"labels\": x[\"labels\"]})\n            {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'labels': tensor([1]), 'mapping_index': {'last': [0]}}\n        \"\"\"\n\n        self.logger.info(\"Extracting cache\", std_out=True)\n\n        # get the function to save in the cache the additional element from the batch sime\n\n        self.logger.info(\"Forward pass started\", std_out=True)\n        all_cache = ActivationCache()  # a list of dictoionaries, each dictionary contains the activations of the model for a batch (so a dict of tensors)\n        attn_pattern = (\n            ActivationCache()\n        )  # Initialize the dictionary to hold running averages\n\n        example_dict = {}\n        n_batches = 0  # Initialize batch counter\n\n        for batch in tqdm(dataloader, total=len(dataloader), desc=\"Extracting cache:\"):\n            # log_memory_usage(\"Extract cache - Before batch\")\n            # tokens, others = batch\n            # inputs = {k: v.to(self.first_device) for k, v in tokens.items()}\n\n            # get input_ids, attention_mask, and if available, pixel_values from batch (that is a dictionary)\n            # then move them to the first device\n            inputs = self.input_handler.prepare_inputs(batch, self.first_device)\n            others = {k: v for k, v in batch.items() if k not in inputs}\n\n            cache = self.forward(\n                inputs,\n                target_token_positions=target_token_positions,\n                pivot_positions=batch.get(\"pivot_positions\", None),\n                external_cache=attn_pattern,\n                batch_idx=n_batches,\n                **kwargs,\n            )\n            # possible memory leak from here -___---------------&gt;\n            additional_dict = batch_saver(others)\n            if additional_dict is not None:\n                # cache = {**cache, **additional_dict}\n                cache.update(additional_dict)\n\n            if move_to_cpu_after_forward:\n                cache.cpu()\n\n            n_batches += 1  # Increment batch counter# Process and remove \"pattern_\" keys from cache\n            all_cache.cat(cache)\n\n            del cache\n            inputs = self.input_handler.prepare_inputs(batch, \"cpu\")\n            del inputs\n            torch.cuda.empty_cache()\n\n        self.logger.info(\n            \"Forward pass finished - started to aggregate different batch\", std_out=True\n        )\n        all_cache.update(attn_pattern)\n        all_cache[\"example_dict\"] = example_dict\n        self.logger.info(\"Aggregation finished\", std_out=True)\n\n        torch.cuda.empty_cache()\n        return all_cache\n\n    @torch.no_grad()\n    def compute_patching(\n        self,\n        target_token_positions: List[str],\n        # counterfactual_dataset,\n        base_dataloader,\n        target_dataloader,\n        patching_query=[\n            {\n                \"patching_elem\": \"@end-image\",\n                \"layers_to_patch\": [1, 2, 3, 4],\n                \"activation_type\": \"resid_in_{}\",\n            }\n        ],\n        base_dictonary_idxs: Optional[List[List[int]]] = None,\n        target_dictonary_idxs: Optional[List[List[int]]] = None,\n        return_logit_diff: bool = False,\n        batch_saver: Callable = lambda x: None,\n        **kwargs,\n    ) -&gt; ActivationCache:\n        r\"\"\"\n        Method for activation patching. This substitutes the activations of the model\n        with the activations of the counterfactual dataset.\n\n        It performs three forward passes:\n        1. Forward pass on the base dataset to extract the activations of the model (cat).\n        2. Forward pass on the target dataset to extract clean logits (dog)\n        [to compare against the patched logits].\n        3. Forward pass on the target dataset to patch (cat) into (dog)\n        and extract the patched logits.\n\n        Args:\n            target_token_positions (list[str]): List of tokens to extract the activations from.\n            base_dataloader (torch.utils.data.DataLoader): Dataloader with the base dataset. (dataset where we sample the activations from)\n            target_dataloader (torch.utils.data.DataLoader): Dataloader with the target dataset. (dataset where we patch the activations)\n            patching_query (list[dict]): List of dictionaries with the patching queries. Each dictionary must have the keys \"patching_elem\", \"layers_to_patch\" and \"activation_type\". The \"patching_elem\" is the token to patch, the \"layers_to_patch\" is the list of layers to patch and the \"activation_type\" is the type of the activation to patch. The activation type must be one of the following: \"resid_in_{}\", \"resid_out_{}\", \"resid_mid_{}\", \"attn_in_{}\", \"attn_out_{}\", \"values_{}\". The \"{}\" will be replaced with the layer index.\n            base_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n            target_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n            return_logit_diff (bool): If True, it will return the logit difference between the clean logits and the patched logits.\n\n\n        Returns:\n            final_cache (ActivationCache): dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n        Examples:\n            &gt;&gt;&gt; model.compute_patching(\n            &gt;&gt;&gt;     target_token_positions=[\"end-image\", \" last\"],\n            &gt;&gt;&gt;     base_dataloader=base_dataloader,\n            &gt;&gt;&gt;     target_dataloader=target_dataloader,\n            &gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,\n            &gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,\n            &gt;&gt;&gt;     patching_query=[\n            &gt;&gt;&gt;         {\n            &gt;&gt;&gt;             \"patching_elem\": \"@end-image\",\n            &gt;&gt;&gt;             \"layers_to_patch\": [1, 2, 3, 4],\n            &gt;&gt;&gt;             \"activation_type\": \"resid_in_{}\",\n            &gt;&gt;&gt;         }\n            &gt;&gt;&gt;     ],\n            &gt;&gt;&gt;     return_logit_diff=False,\n            &gt;&gt;&gt;     batch_saver=lambda x: None,\n            &gt;&gt;&gt; )\n            &gt;&gt;&gt; print(final_cache)\n            {\n                \"resid_out_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n                \"resid_mid_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n                ....\n                \"logit_diff_variation\": tensor of shape [batch] with the logit difference variation\n                \"logit_diff_in_clean\": tensor of shape [batch] with the logit difference in the clean logits\n                \"logit_diff_in_patched\": tensor of shape [batch] with the logit difference in the patched logits\n            }\n        \"\"\"\n        self.logger.info(\"Computing patching\", std_out=True)\n\n        self.logger.info(\"Forward pass started\", std_out=True)\n        self.logger.info(\n            f\"Patching elements: {[q['patching_elem'] for q in patching_query]} at {[query['activation_type'][:-3] for query in patching_query]}\",\n            std_out=True,\n        )\n\n        # get a random number in the range of the dataset to save a random batch\n        all_cache = ActivationCache()\n        # for each batch in the dataset\n        for index, (base_batch, target_batch) in tqdm(\n            enumerate(zip(base_dataloader, target_dataloader)),\n            desc=\"Computing patching on the dataset:\",\n            total=len(base_dataloader),\n        ):\n            torch.cuda.empty_cache()\n            inputs = self.input_handler.prepare_inputs(base_batch, self.first_device)\n\n            # set the right arguments for extract the patching activations\n            activ_type = [query[\"activation_type\"][:-3] for query in patching_query]\n\n            args = {\n                \"extract_resid_out\": True,\n                \"extract_resid_in\": False,\n                \"extract_resid_mid\": False,\n                \"extract_attn_in\": False,\n                \"extract_attn_out\": False,\n                \"extract_head_values\": False,\n                \"extract_head_out\": False,\n                \"extract_avg_attn_pattern\": False,\n                \"extract_avg_values_vectors_projected\": False,\n                \"extract_head_values_projected\": False,\n                \"extract_avg\": False,\n                \"ablation_queries\": None,\n                \"patching_queries\": None,\n                \"external_cache\": None,\n                \"attn_heads\": \"all\",\n                \"batch_idx\": None,\n                \"move_to_cpu\": False,\n            }\n\n            if \"resid_in\" in activ_type:\n                args[\"extract_resid_in\"] = True\n            if \"resid_out\" in activ_type:\n                args[\"extract_resid_out\"] = True\n            if \"resid_mid\" in activ_type:\n                args[\"extract_intermediate_states\"] = True\n            if \"attn_in\" in activ_type:\n                args[\"extract_attn_in\"] = True\n            if \"attn_out\" in activ_type:\n                args[\"extract_attn_out\"] = True\n            if \"values\" in activ_type:\n                args[\"extract_head_values\"] = True\n            # other cases\n\n            # first forward pass to extract the base activations\n            base_cache = self.forward(\n                inputs=inputs,\n                target_token_positions=target_token_positions,\n                pivot_positions=base_batch.get(\"pivot_positions\", None),\n                extraction_config=ExtractionConfig(**args),\n                ablation_queries=args[\"ablation_queries\"],\n                patching_queries=args[\"patching_queries\"],\n                external_cache=args[\"external_cache\"],\n                batch_idx=args[\"batch_idx\"],\n                move_to_cpu=args[\"move_to_cpu\"],\n            )\n\n            # extract the target activations\n            target_inputs = self.input_handler.prepare_inputs(\n                target_batch, self.first_device\n            )\n\n            requested_position_to_extract = []\n            for query in patching_query:\n                query[\"patching_activations\"] = base_cache\n                if (\n                    query[\"patching_elem\"].split(\"@\")[1]\n                    not in requested_position_to_extract\n                ):\n                    requested_position_to_extract.append(\n                        query[\"patching_elem\"].split(\"@\")[1]\n                    )\n                query[\"base_activation_index\"] = base_cache[\"mapping_index\"][\n                    query[\"patching_elem\"].split(\"@\")[1]\n                ]\n\n            # second forward pass to extract the clean logits\n            target_clean_cache = self.forward(\n                target_inputs,\n                target_token_positions=requested_position_to_extract,\n                pivot_positions=target_batch.get(\"pivot_positions\", None),\n                # move_to_cpu=True,\n            )\n\n            # merge requested_position_to_extract with extracted_token_positio\n            # third forward pass to patch the activations\n            target_patched_cache = self.forward(\n                target_inputs,\n                target_token_positions=list(\n                    set(target_token_positions + requested_position_to_extract)\n                ),\n                pivot_positions=target_batch.get(\"pivot_positions\", None),\n                patching_queries=patching_query,\n                **kwargs,\n            )\n\n            if return_logit_diff:\n                if base_dictonary_idxs is None or target_dictonary_idxs is None:\n                    raise ValueError(\n                        \"To compute the logit difference, you need to pass the base_dictonary_idxs and the target_dictonary_idxs\"\n                    )\n                self.logger.info(\"Computing logit difference\", std_out=True)\n                # get the target tokens (\" cat\" and \" dog\")\n                base_targets = base_dictonary_idxs[index]\n                target_targets = target_dictonary_idxs[index]\n\n                # compute the logit difference\n                result_diff = logit_diff(\n                    base_label_tokens=[s for s in base_targets],\n                    target_label_tokens=[c for c in target_targets],\n                    target_clean_logits=target_clean_cache[\"logits\"],\n                    target_patched_logits=target_patched_cache[\"logits\"],\n                )\n                target_patched_cache[\"logit_diff_variation\"] = result_diff[\n                    \"diff_variation\"\n                ]\n                target_patched_cache[\"logit_diff_in_clean\"] = result_diff[\n                    \"diff_in_clean\"\n                ]\n                target_patched_cache[\"logit_diff_in_patched\"] = result_diff[\n                    \"diff_in_patched\"\n                ]\n\n            # compute the KL divergence\n            result_kl = kl_divergence_diff(\n                base_logits=base_cache[\"logits\"],\n                target_clean_logits=target_clean_cache[\"logits\"],\n                target_patched_logits=target_patched_cache[\"logits\"],\n            )\n            for key, value in result_kl.items():\n                target_patched_cache[key] = value\n\n            target_patched_cache[\"base_logits\"] = base_cache[\"logits\"]\n            target_patched_cache[\"target_clean_logits\"] = target_clean_cache[\"logits\"]\n            # rename logits to target_patched_logits\n            target_patched_cache[\"target_patched_logits\"] = target_patched_cache[\n                \"logits\"\n            ]\n            del target_patched_cache[\"logits\"]\n\n            target_patched_cache.cpu()\n\n            # all_cache.append(target_patched_cache)\n            all_cache.cat(target_patched_cache)\n\n        self.logger.info(\n            \"Forward pass finished - started to aggregate different batch\", std_out=True\n        )\n        # final_cache = aggregate_cache_efficient(all_cache)\n\n        self.logger.info(\"Aggregation finished\", std_out=True)\n        return all_cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.__call__","title":"<code>__call__(*args, **kwds)</code>","text":"<p>Call the forward method of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def __call__(self, *args, **kwds) -&gt; ActivationCache:\n    r\"\"\"\n    Call the forward method of the model\n    \"\"\"\n    return self.forward(*args, **kwds)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.compute_patching","title":"<code>compute_patching(target_token_positions, base_dataloader, target_dataloader, patching_query=[{'patching_elem': '@end-image', 'layers_to_patch': [1, 2, 3, 4], 'activation_type': 'resid_in_{}'}], base_dictonary_idxs=None, target_dictonary_idxs=None, return_logit_diff=False, batch_saver=lambda x: None, **kwargs)</code>","text":"<p>Method for activation patching. This substitutes the activations of the model with the activations of the counterfactual dataset.</p> <p>It performs three forward passes: 1. Forward pass on the base dataset to extract the activations of the model (cat). 2. Forward pass on the target dataset to extract clean logits (dog) [to compare against the patched logits]. 3. Forward pass on the target dataset to patch (cat) into (dog) and extract the patched logits.</p> <p>Parameters:</p> Name Type Description Default <code>target_token_positions</code> <code>list[str]</code> <p>List of tokens to extract the activations from.</p> required <code>base_dataloader</code> <code>DataLoader</code> <p>Dataloader with the base dataset. (dataset where we sample the activations from)</p> required <code>target_dataloader</code> <code>DataLoader</code> <p>Dataloader with the target dataset. (dataset where we patch the activations)</p> required <code>patching_query</code> <code>list[dict]</code> <p>List of dictionaries with the patching queries. Each dictionary must have the keys \"patching_elem\", \"layers_to_patch\" and \"activation_type\". The \"patching_elem\" is the token to patch, the \"layers_to_patch\" is the list of layers to patch and the \"activation_type\" is the type of the activation to patch. The activation type must be one of the following: \"resid_in_{}\", \"resid_out_{}\", \"resid_mid_{}\", \"attn_in_{}\", \"attn_out_{}\", \"values_{}\". The \"{}\" will be replaced with the layer index.</p> <code>[{'patching_elem': '@end-image', 'layers_to_patch': [1, 2, 3, 4], 'activation_type': 'resid_in_{}'}]</code> <code>base_dictonary_idxs</code> <code>list[list[int]]</code> <p>List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.</p> <code>None</code> <code>target_dictonary_idxs</code> <code>list[list[int]]</code> <p>List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.</p> <code>None</code> <code>return_logit_diff</code> <code>bool</code> <p>If True, it will return the logit difference between the clean logits and the patched logits.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>final_cache</code> <code>ActivationCache</code> <p>dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model.compute_patching(\n&gt;&gt;&gt;     target_token_positions=[\"end-image\", \" last\"],\n&gt;&gt;&gt;     base_dataloader=base_dataloader,\n&gt;&gt;&gt;     target_dataloader=target_dataloader,\n&gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,\n&gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,\n&gt;&gt;&gt;     patching_query=[\n&gt;&gt;&gt;         {\n&gt;&gt;&gt;             \"patching_elem\": \"@end-image\",\n&gt;&gt;&gt;             \"layers_to_patch\": [1, 2, 3, 4],\n&gt;&gt;&gt;             \"activation_type\": \"resid_in_{}\",\n&gt;&gt;&gt;         }\n&gt;&gt;&gt;     ],\n&gt;&gt;&gt;     return_logit_diff=False,\n&gt;&gt;&gt;     batch_saver=lambda x: None,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; print(final_cache)\n{\n    \"resid_out_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n    \"resid_mid_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n    ....\n    \"logit_diff_variation\": tensor of shape [batch] with the logit difference variation\n    \"logit_diff_in_clean\": tensor of shape [batch] with the logit difference in the clean logits\n    \"logit_diff_in_patched\": tensor of shape [batch] with the logit difference in the patched logits\n}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@torch.no_grad()\ndef compute_patching(\n    self,\n    target_token_positions: List[str],\n    # counterfactual_dataset,\n    base_dataloader,\n    target_dataloader,\n    patching_query=[\n        {\n            \"patching_elem\": \"@end-image\",\n            \"layers_to_patch\": [1, 2, 3, 4],\n            \"activation_type\": \"resid_in_{}\",\n        }\n    ],\n    base_dictonary_idxs: Optional[List[List[int]]] = None,\n    target_dictonary_idxs: Optional[List[List[int]]] = None,\n    return_logit_diff: bool = False,\n    batch_saver: Callable = lambda x: None,\n    **kwargs,\n) -&gt; ActivationCache:\n    r\"\"\"\n    Method for activation patching. This substitutes the activations of the model\n    with the activations of the counterfactual dataset.\n\n    It performs three forward passes:\n    1. Forward pass on the base dataset to extract the activations of the model (cat).\n    2. Forward pass on the target dataset to extract clean logits (dog)\n    [to compare against the patched logits].\n    3. Forward pass on the target dataset to patch (cat) into (dog)\n    and extract the patched logits.\n\n    Args:\n        target_token_positions (list[str]): List of tokens to extract the activations from.\n        base_dataloader (torch.utils.data.DataLoader): Dataloader with the base dataset. (dataset where we sample the activations from)\n        target_dataloader (torch.utils.data.DataLoader): Dataloader with the target dataset. (dataset where we patch the activations)\n        patching_query (list[dict]): List of dictionaries with the patching queries. Each dictionary must have the keys \"patching_elem\", \"layers_to_patch\" and \"activation_type\". The \"patching_elem\" is the token to patch, the \"layers_to_patch\" is the list of layers to patch and the \"activation_type\" is the type of the activation to patch. The activation type must be one of the following: \"resid_in_{}\", \"resid_out_{}\", \"resid_mid_{}\", \"attn_in_{}\", \"attn_out_{}\", \"values_{}\". The \"{}\" will be replaced with the layer index.\n        base_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n        target_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.\n        return_logit_diff (bool): If True, it will return the logit difference between the clean logits and the patched logits.\n\n\n    Returns:\n        final_cache (ActivationCache): dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n    Examples:\n        &gt;&gt;&gt; model.compute_patching(\n        &gt;&gt;&gt;     target_token_positions=[\"end-image\", \" last\"],\n        &gt;&gt;&gt;     base_dataloader=base_dataloader,\n        &gt;&gt;&gt;     target_dataloader=target_dataloader,\n        &gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,\n        &gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,\n        &gt;&gt;&gt;     patching_query=[\n        &gt;&gt;&gt;         {\n        &gt;&gt;&gt;             \"patching_elem\": \"@end-image\",\n        &gt;&gt;&gt;             \"layers_to_patch\": [1, 2, 3, 4],\n        &gt;&gt;&gt;             \"activation_type\": \"resid_in_{}\",\n        &gt;&gt;&gt;         }\n        &gt;&gt;&gt;     ],\n        &gt;&gt;&gt;     return_logit_diff=False,\n        &gt;&gt;&gt;     batch_saver=lambda x: None,\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; print(final_cache)\n        {\n            \"resid_out_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n            \"resid_mid_0\": tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0\n            ....\n            \"logit_diff_variation\": tensor of shape [batch] with the logit difference variation\n            \"logit_diff_in_clean\": tensor of shape [batch] with the logit difference in the clean logits\n            \"logit_diff_in_patched\": tensor of shape [batch] with the logit difference in the patched logits\n        }\n    \"\"\"\n    self.logger.info(\"Computing patching\", std_out=True)\n\n    self.logger.info(\"Forward pass started\", std_out=True)\n    self.logger.info(\n        f\"Patching elements: {[q['patching_elem'] for q in patching_query]} at {[query['activation_type'][:-3] for query in patching_query]}\",\n        std_out=True,\n    )\n\n    # get a random number in the range of the dataset to save a random batch\n    all_cache = ActivationCache()\n    # for each batch in the dataset\n    for index, (base_batch, target_batch) in tqdm(\n        enumerate(zip(base_dataloader, target_dataloader)),\n        desc=\"Computing patching on the dataset:\",\n        total=len(base_dataloader),\n    ):\n        torch.cuda.empty_cache()\n        inputs = self.input_handler.prepare_inputs(base_batch, self.first_device)\n\n        # set the right arguments for extract the patching activations\n        activ_type = [query[\"activation_type\"][:-3] for query in patching_query]\n\n        args = {\n            \"extract_resid_out\": True,\n            \"extract_resid_in\": False,\n            \"extract_resid_mid\": False,\n            \"extract_attn_in\": False,\n            \"extract_attn_out\": False,\n            \"extract_head_values\": False,\n            \"extract_head_out\": False,\n            \"extract_avg_attn_pattern\": False,\n            \"extract_avg_values_vectors_projected\": False,\n            \"extract_head_values_projected\": False,\n            \"extract_avg\": False,\n            \"ablation_queries\": None,\n            \"patching_queries\": None,\n            \"external_cache\": None,\n            \"attn_heads\": \"all\",\n            \"batch_idx\": None,\n            \"move_to_cpu\": False,\n        }\n\n        if \"resid_in\" in activ_type:\n            args[\"extract_resid_in\"] = True\n        if \"resid_out\" in activ_type:\n            args[\"extract_resid_out\"] = True\n        if \"resid_mid\" in activ_type:\n            args[\"extract_intermediate_states\"] = True\n        if \"attn_in\" in activ_type:\n            args[\"extract_attn_in\"] = True\n        if \"attn_out\" in activ_type:\n            args[\"extract_attn_out\"] = True\n        if \"values\" in activ_type:\n            args[\"extract_head_values\"] = True\n        # other cases\n\n        # first forward pass to extract the base activations\n        base_cache = self.forward(\n            inputs=inputs,\n            target_token_positions=target_token_positions,\n            pivot_positions=base_batch.get(\"pivot_positions\", None),\n            extraction_config=ExtractionConfig(**args),\n            ablation_queries=args[\"ablation_queries\"],\n            patching_queries=args[\"patching_queries\"],\n            external_cache=args[\"external_cache\"],\n            batch_idx=args[\"batch_idx\"],\n            move_to_cpu=args[\"move_to_cpu\"],\n        )\n\n        # extract the target activations\n        target_inputs = self.input_handler.prepare_inputs(\n            target_batch, self.first_device\n        )\n\n        requested_position_to_extract = []\n        for query in patching_query:\n            query[\"patching_activations\"] = base_cache\n            if (\n                query[\"patching_elem\"].split(\"@\")[1]\n                not in requested_position_to_extract\n            ):\n                requested_position_to_extract.append(\n                    query[\"patching_elem\"].split(\"@\")[1]\n                )\n            query[\"base_activation_index\"] = base_cache[\"mapping_index\"][\n                query[\"patching_elem\"].split(\"@\")[1]\n            ]\n\n        # second forward pass to extract the clean logits\n        target_clean_cache = self.forward(\n            target_inputs,\n            target_token_positions=requested_position_to_extract,\n            pivot_positions=target_batch.get(\"pivot_positions\", None),\n            # move_to_cpu=True,\n        )\n\n        # merge requested_position_to_extract with extracted_token_positio\n        # third forward pass to patch the activations\n        target_patched_cache = self.forward(\n            target_inputs,\n            target_token_positions=list(\n                set(target_token_positions + requested_position_to_extract)\n            ),\n            pivot_positions=target_batch.get(\"pivot_positions\", None),\n            patching_queries=patching_query,\n            **kwargs,\n        )\n\n        if return_logit_diff:\n            if base_dictonary_idxs is None or target_dictonary_idxs is None:\n                raise ValueError(\n                    \"To compute the logit difference, you need to pass the base_dictonary_idxs and the target_dictonary_idxs\"\n                )\n            self.logger.info(\"Computing logit difference\", std_out=True)\n            # get the target tokens (\" cat\" and \" dog\")\n            base_targets = base_dictonary_idxs[index]\n            target_targets = target_dictonary_idxs[index]\n\n            # compute the logit difference\n            result_diff = logit_diff(\n                base_label_tokens=[s for s in base_targets],\n                target_label_tokens=[c for c in target_targets],\n                target_clean_logits=target_clean_cache[\"logits\"],\n                target_patched_logits=target_patched_cache[\"logits\"],\n            )\n            target_patched_cache[\"logit_diff_variation\"] = result_diff[\n                \"diff_variation\"\n            ]\n            target_patched_cache[\"logit_diff_in_clean\"] = result_diff[\n                \"diff_in_clean\"\n            ]\n            target_patched_cache[\"logit_diff_in_patched\"] = result_diff[\n                \"diff_in_patched\"\n            ]\n\n        # compute the KL divergence\n        result_kl = kl_divergence_diff(\n            base_logits=base_cache[\"logits\"],\n            target_clean_logits=target_clean_cache[\"logits\"],\n            target_patched_logits=target_patched_cache[\"logits\"],\n        )\n        for key, value in result_kl.items():\n            target_patched_cache[key] = value\n\n        target_patched_cache[\"base_logits\"] = base_cache[\"logits\"]\n        target_patched_cache[\"target_clean_logits\"] = target_clean_cache[\"logits\"]\n        # rename logits to target_patched_logits\n        target_patched_cache[\"target_patched_logits\"] = target_patched_cache[\n            \"logits\"\n        ]\n        del target_patched_cache[\"logits\"]\n\n        target_patched_cache.cpu()\n\n        # all_cache.append(target_patched_cache)\n        all_cache.cat(target_patched_cache)\n\n    self.logger.info(\n        \"Forward pass finished - started to aggregate different batch\", std_out=True\n    )\n    # final_cache = aggregate_cache_efficient(all_cache)\n\n    self.logger.info(\"Aggregation finished\", std_out=True)\n    return all_cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.create_hooks","title":"<code>create_hooks(inputs, cache, token_index, token_dict, extraction_config=ExtractionConfig(), patching_queries=None, ablation_queries=None, batch_idx=None, external_cache=None)</code>","text":"<p>Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</p> required <code>cache</code> <code>ActivationCache</code> <p>dictionary where the activations of the model will be saved</p> required <code>extracted_token_position</code> <code>list[str]</code> <p>list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])</p> required <code>string_tokens</code> <code>list[str]</code> <p>list of string tokens</p> required <code>pivot_positions</code> <code>Optional[list[int]]</code> <p>list of split positions of the tokens</p> required <code>extraction_config</code> <code>ExtractionConfig</code> <p>configuration of the extraction of the activations of the model (default = ExtractionConfig())</p> <code>ExtractionConfig()</code> <code>ablation_queries</code> <code>Optional[Union[dict, DataFrame]]</code> <p>dictionary or dataframe with the ablation queries to perform during forward pass</p> <code>None</code> <code>patching_queries</code> <code>Optional[Union[dict, DataFrame]]</code> <p>dictionary or dataframe with the patching queries to perform during forward pass</p> <code>None</code> <code>batch_idx</code> <code>Optional[int]</code> <p>index of the batch in the dataloader</p> <code>None</code> <code>external_cache</code> <code>Optional[ActivationCache]</code> <p>external cache to use in the forward pass</p> <code>None</code> <p>Returns:</p> Name Type Description <code>hooks</code> <code>list[dict]</code> <p>list of dictionaries with the component and the intervention to perform in the forward pass of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def create_hooks(\n    self,\n    inputs,\n    cache: ActivationCache,\n    token_index: List,\n    token_dict: Dict,\n    # string_tokens: List[str],\n    extraction_config: ExtractionConfig = ExtractionConfig(),\n    patching_queries: Optional[Union[dict, pd.DataFrame]] = None,\n    ablation_queries: Optional[Union[dict, pd.DataFrame]] = None,\n    batch_idx: Optional[int] = None,\n    external_cache: Optional[ActivationCache] = None,\n):\n    r\"\"\"\n    Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.\n\n    Arguments:\n        inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n        cache (ActivationCache): dictionary where the activations of the model will be saved\n        extracted_token_position (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n        string_tokens (list[str]): list of string tokens\n        pivot_positions (Optional[list[int]]): list of split positions of the tokens\n        extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model (default = ExtractionConfig())\n        ablation_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the ablation queries to perform during forward pass\n        patching_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the patching queries to perform during forward pass\n        batch_idx (Optional[int]): index of the batch in the dataloader\n        external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n\n    Returns:\n        hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n    \"\"\"\n    hooks = []\n\n    # compute layer and head indexes\n    if (\n        isinstance(extraction_config.attn_heads, str)\n        and extraction_config.attn_heads == \"all\"\n    ):\n        layer_indexes = [i for i in range(0, self.model_config.num_hidden_layers)]\n        head_indexes = [\"all\"] * len(layer_indexes)\n    elif isinstance(extraction_config.attn_heads, list):\n        layer_head_indexes = [\n            (el[\"layer\"], el[\"head\"]) for el in extraction_config.attn_heads\n        ]\n        layer_indexes = [el[0] for el in layer_head_indexes]\n        head_indexes = [el[1] for el in layer_head_indexes]\n    else:\n        raise ValueError(\n            \"attn_heads must be 'all' or a list of dictionaries as [{'layer': 0, 'head': 0}]\"\n        )\n\n    if extraction_config.extract_resid_out:\n        # assert that the component exists in the model\n        hooks += [\n            {\n                \"component\": self.model_config.residual_stream_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_out_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n    if extraction_config.extract_resid_in:\n        # assert that the component exists in the model\n        hooks += [\n            {\n                \"component\": self.model_config.residual_stream_input_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_in_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_resid_in_post_layernorm:\n        hooks += [\n            {\n                \"component\": self.model_config.residual_stream_input_post_layernorm_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_in_post_layernorm_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.save_input_ids:\n        hooks += [\n            {\n                \"component\": self.model_config.embed_tokens,\n                \"intervention\": partial(\n                    embed_hook,\n                    cache=cache,\n                    cache_key=\"input_ids\",\n                ),\n            }\n        ]\n\n    if extraction_config.extract_head_queries:\n        hooks += [\n            {\n                \"component\": self.model_config.head_query_hook_name.format(i),\n                \"intervention\": partial(\n                    query_key_value_hook,\n                    cache=cache,\n                    cache_key=\"queries_\",\n                    token_index=token_index,\n                    head_dim=self.model_config.head_dim,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_values:\n        hooks += [\n            {\n                \"component\": self.model_config.head_value_hook_name.format(i),\n                \"intervention\": partial(\n                    query_key_value_hook,\n                    cache=cache,\n                    cache_key=\"values_\",\n                    token_index=token_index,\n                    head_dim=self.model_config.head_dim,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_keys:\n        hooks += [\n            {\n                \"component\": self.model_config.head_key_hook_name.format(i),\n                \"intervention\": partial(\n                    query_key_value_hook,\n                    cache=cache,\n                    cache_key=\"keys_\",\n                    token_index=token_index,\n                    head_dim=self.model_config.head_dim,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_key_value_groups=self.model_config.num_key_value_groups,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_head_out:\n        hooks += [\n            {\n                \"component\": self.model_config.attn_o_proj_input_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    head_out_hook,\n                    cache=cache,\n                    cache_key=\"head_out_\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                    layer=i,\n                    head=head,\n                    num_heads=self.model_config.num_attention_heads,\n                    head_dim=self.model_config.head_dim,\n                    o_proj_weight=get_attribute_from_name(\n                        self.hf_model,\n                        self.model_config.attn_out_proj_weight.format(i),\n                    ),\n                    o_proj_bias=get_attribute_from_name(\n                        self.hf_model,\n                        self.model_config.attn_out_proj_bias.format(i),\n                    ),\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_attn_in:\n        hooks += [\n            {\n                \"component\": self.model_config.attn_in_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"attn_in_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    if extraction_config.extract_attn_out:\n        hooks += [\n            {\n                \"component\": self.model_config.attn_out_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"attn_out_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    # if extraction_config.extract_avg:\n    #     # Define a hook that saves the activations of the residual stream\n    #     raise NotImplementedError(\n    #         \"The hook for the average is not working with token_index as a list\"\n    #     )\n\n    #     # hooks.extend(\n    #     #     [\n    #     #         {\n    #     #             \"component\": self.model_config.residual_stream_hook_name.format(\n    #     #                 i\n    #     #             ),\n    #     #             \"intervention\": partial(\n    #     #                 avg_hook,\n    #     #                 cache=cache,\n    #     #                 cache_key=\"resid_avg_{}\".format(i),\n    #     #                 last_image_idx=last_image_idxs, #type\n    #     #                 end_image_idx=end_image_idxs,\n    #     #             ),\n    #     #         }\n    #     #         for i in range(0, self.model_config.num_hidden_layers)\n    #     #     ]\n    #     # )\n    if extraction_config.extract_resid_mid:\n        hooks += [\n            {\n                \"component\": self.model_config.intermediate_stream_hook_name.format(\n                    i\n                ),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"resid_mid_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n        # if we want to extract the output of the heads\n    if extraction_config.extract_mlp_out:\n        hooks += [\n            {\n                \"component\": self.model_config.mlp_out_hook_name.format(i),\n                \"intervention\": partial(\n                    save_resid_hook,\n                    cache=cache,\n                    cache_key=f\"mlp_out_{i}\",\n                    token_index=token_index,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i in range(0, self.model_config.num_hidden_layers)\n        ]\n\n    # PATCHING\n    if patching_queries:\n        token_to_pos = partial(\n            map_token_to_pos,\n            _get_token_index=token_dict,\n            # string_tokens=string_tokens,\n            hf_tokenizer=self.hf_tokenizer,\n            inputs=inputs,\n        )\n        patching_queries = preprocess_patching_queries(\n            patching_queries=patching_queries,\n            map_token_to_pos=token_to_pos,\n            model_config=self.model_config,\n        )\n\n        def make_patch_tokens_hook(patching_queries_subset):\n            \"\"\"\n            Creates a hook function to patch the activations in the\n            current forward pass.\n            \"\"\"\n\n            def patch_tokens_hook(\n                module, args, kwargs, output\n            ):  # TODO: Move to hook.py\n                b = process_args_kwargs_output(args, kwargs, output)\n                # Modify the tensor without affecting the computation graph\n                act_to_patch = b.detach().clone()\n                for pos, patch in zip(\n                    patching_queries_subset[\"pos_token_to_patch\"],\n                    patching_queries_subset[\"patching_activations\"],\n                ):\n                    act_to_patch[0, pos, :] = patching_queries_subset[\n                        \"patching_activations\"\n                    ].values[0]\n\n                if output is None:\n                    if isinstance(input, tuple):\n                        return (act_to_patch, *input[1:])\n                    elif input is not None:\n                        return act_to_patch\n                else:\n                    if isinstance(output, tuple):\n                        return (act_to_patch, *output[1:])\n                    elif output is not None:\n                        return act_to_patch\n                raise ValueError(\"No output or input found\")\n\n            return patch_tokens_hook\n\n        # Group the patching queries by 'layer' and 'act_type'\n        grouped_queries = patching_queries.groupby([\"layer\", \"activation_type\"])\n\n        for (layer, act_type), group in grouped_queries:\n            hook_name_template = self.act_type_to_hook_name.get(\n                act_type[:-3]\n            )  # -3 because we remove {}\n            if not hook_name_template:\n                raise ValueError(f\"Unknown activation type: {act_type}\")\n                # continue  # Skip unknown activation types\n\n            hook_name = hook_name_template.format(layer)\n            hook_function = partial(make_patch_tokens_hook(group))\n\n            hooks.append(\n                {\n                    \"component\": hook_name,\n                    \"intervention\": hook_function,\n                }\n            )\n\n    if ablation_queries is not None:\n        # TODO: debug and test the ablation. Check with ale\n        token_to_pos = partial(\n            map_token_to_pos,\n            _get_token_index=token_dict,\n            # string_tokens=string_tokens,\n            hf_tokenizer=self.hf_tokenizer,\n            inputs=inputs,\n        )\n        if self.config.batch_size &gt; 1:\n            raise ValueError(\"Ablation is not supported with batch size &gt; 1\")\n        ablation_manager = AblationManager(\n            model_config=self.model_config,\n            token_to_pos=token_to_pos,\n            inputs=inputs,\n            model_attn_type=self.config.attn_implementation,\n            ablation_queries=pd.DataFrame(ablation_queries)\n            if isinstance(ablation_queries, dict)\n            else ablation_queries,\n        )\n        hooks.extend(ablation_manager.main())\n\n    if extraction_config.extract_head_values_projected:\n        hooks += [\n            {\n                \"component\": self.model_config.head_value_hook_name.format(i),\n                \"intervention\": partial(\n                    projected_value_vectors_head,\n                    cache=cache,\n                    token_index=token_index,\n                    layer=i,\n                    num_attention_heads=self.model_config.num_attention_heads,\n                    num_key_value_heads=self.model_config.num_key_value_heads,\n                    hidden_size=self.model_config.hidden_size,\n                    d_head=self.model_config.head_dim,\n                    out_proj_weight=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_weight.format(i)}\",\n                    ),\n                    out_proj_bias=get_attribute_from_name(\n                        self.hf_model,\n                        f\"{self.model_config.attn_out_proj_bias.format(i)}\",\n                    ),\n                    head=head,\n                    avg=extraction_config.avg,\n                ),\n            }\n            for i, head in zip(layer_indexes, head_indexes)\n        ]\n\n    if extraction_config.extract_attn_pattern:\n        if extraction_config.avg:\n            if external_cache is None:\n                self.logger.warning(\n                    \"\"\"The external_cache is None. The average could not be computed since missing an external cache where store the iterations.\n                    \"\"\"\n                )\n            elif batch_idx is None:\n                self.logger.warning(\n                    \"\"\"The batch_idx is None. The average could not be computed since missing the batch index.\n\n                    \"\"\"\n                )\n            else:\n                # move the cache to the same device of the model\n                external_cache.to(self.first_device)\n                hooks += [\n                    {\n                        \"component\": self.model_config.attn_matrix_hook_name.format(\n                            i\n                        ),\n                        \"intervention\": partial(\n                            avg_attention_pattern_head,\n                            token_index=token_index,\n                            layer=i,\n                            attn_pattern_current_avg=external_cache,\n                            batch_idx=batch_idx,\n                            cache=cache,\n                            # avg=extraction_config.avg,\n                            extract_avg_value=extraction_config.extract_head_values_projected,\n                        ),\n                    }\n                    for i in range(0, self.model_config.num_hidden_layers)\n                ]\n        else:\n            hooks += [\n                {\n                    \"component\": self.model_config.attn_matrix_hook_name.format(i),\n                    \"intervention\": partial(\n                        attention_pattern_head,\n                        token_index=token_index,\n                        cache=cache,\n                        layer=i,\n                        head=head,\n                    ),\n                }\n                for i, head in zip(layer_indexes, head_indexes)\n            ]\n\n        # if additional hooks are not empty, add them to the hooks list\n    if self.additional_hooks:\n        hooks += self.additional_hooks\n    return hooks\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.device","title":"<code>device()</code>","text":"<p>Return the device of the model. If the model is in multiple devices, it will return the first device</p> <p>Returns:</p> Name Type Description <code>device</code> <p>the device of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def device(self):\n    r\"\"\"\n    Return the device of the model. If the model is in multiple devices, it will return the first device\n\n    Args:\n        None\n\n    Returns:\n        device: the device of the model\n    \"\"\"\n    return self.first_device\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.eval","title":"<code>eval()</code>","text":"<p>Set the model in evaluation mode</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def eval(self):\n    r\"\"\"\n    Set the model in evaluation mode\n    \"\"\"\n    self.hf_model.eval()\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.extract_cache","title":"<code>extract_cache(dataloader, target_token_positions, batch_saver=lambda x: None, move_to_cpu_after_forward=True, **kwargs)</code>","text":"<p>Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>iterable</code> <p>dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)</p> required <code>extracted_token_position</code> <code>list[str]</code> <p>list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])</p> required <code>batch_saver</code> <code>Callable</code> <p>function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)</p> <code>lambda x: None</code> <code>move_to_cpu_after_forward</code> <code>bool</code> <p>if True, move the activations to the cpu right after the any forward pass of the model</p> <code>True</code> <code>**kwargs</code> <p>additional arguments to control hooks generation, basically accept any argument handled by the <code>.forward</code> method (i.e. ablation_queries, patching_queries, extract_resid_in)</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>final_cache</code> <p>dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dataloader = [{\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]]), \"labels\": torch.tensor([1])}, ...]\n&gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[\"last\"], batch_saver=lambda x: {\"labels\": x[\"labels\"]})\n{'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'labels': tensor([1]), 'mapping_index': {'last': [0]}}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@torch.no_grad()\ndef extract_cache(\n    self,\n    dataloader,\n    target_token_positions: List[str],\n    batch_saver: Callable = lambda x: None,\n    move_to_cpu_after_forward: bool = True,\n    # save_other_batch_elements: bool = False,\n    **kwargs,\n):\n    r\"\"\"\n    Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.\n\n    Args:\n        dataloader (iterable): dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)\n        extracted_token_position (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n        batch_saver (Callable): function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)\n        move_to_cpu_after_forward (bool): if True, move the activations to the cpu right after the any forward pass of the model\n        **kwargs: additional arguments to control hooks generation, basically accept any argument handled by the `.forward` method (i.e. ablation_queries, patching_queries, extract_resid_in)\n\n    Returns:\n        final_cache: dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve\n\n    Examples:\n        &gt;&gt;&gt; dataloader = [{\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]]), \"labels\": torch.tensor([1])}, ...]\n        &gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[\"last\"], batch_saver=lambda x: {\"labels\": x[\"labels\"]})\n        {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'labels': tensor([1]), 'mapping_index': {'last': [0]}}\n    \"\"\"\n\n    self.logger.info(\"Extracting cache\", std_out=True)\n\n    # get the function to save in the cache the additional element from the batch sime\n\n    self.logger.info(\"Forward pass started\", std_out=True)\n    all_cache = ActivationCache()  # a list of dictoionaries, each dictionary contains the activations of the model for a batch (so a dict of tensors)\n    attn_pattern = (\n        ActivationCache()\n    )  # Initialize the dictionary to hold running averages\n\n    example_dict = {}\n    n_batches = 0  # Initialize batch counter\n\n    for batch in tqdm(dataloader, total=len(dataloader), desc=\"Extracting cache:\"):\n        # log_memory_usage(\"Extract cache - Before batch\")\n        # tokens, others = batch\n        # inputs = {k: v.to(self.first_device) for k, v in tokens.items()}\n\n        # get input_ids, attention_mask, and if available, pixel_values from batch (that is a dictionary)\n        # then move them to the first device\n        inputs = self.input_handler.prepare_inputs(batch, self.first_device)\n        others = {k: v for k, v in batch.items() if k not in inputs}\n\n        cache = self.forward(\n            inputs,\n            target_token_positions=target_token_positions,\n            pivot_positions=batch.get(\"pivot_positions\", None),\n            external_cache=attn_pattern,\n            batch_idx=n_batches,\n            **kwargs,\n        )\n        # possible memory leak from here -___---------------&gt;\n        additional_dict = batch_saver(others)\n        if additional_dict is not None:\n            # cache = {**cache, **additional_dict}\n            cache.update(additional_dict)\n\n        if move_to_cpu_after_forward:\n            cache.cpu()\n\n        n_batches += 1  # Increment batch counter# Process and remove \"pattern_\" keys from cache\n        all_cache.cat(cache)\n\n        del cache\n        inputs = self.input_handler.prepare_inputs(batch, \"cpu\")\n        del inputs\n        torch.cuda.empty_cache()\n\n    self.logger.info(\n        \"Forward pass finished - started to aggregate different batch\", std_out=True\n    )\n    all_cache.update(attn_pattern)\n    all_cache[\"example_dict\"] = example_dict\n    self.logger.info(\"Aggregation finished\", std_out=True)\n\n    torch.cuda.empty_cache()\n    return all_cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.forward","title":"<code>forward(inputs, target_token_positions=['last'], pivot_positions=None, extraction_config=ExtractionConfig(), ablation_queries=None, patching_queries=None, external_cache=None, attn_heads='all', batch_idx=None, move_to_cpu=False)</code>","text":"<p>Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</p> required <code>target_token_positions</code> <code>list[str]</code> <p>list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])</p> <code>['last']</code> <code>pivot_positions</code> <code>Optional[list[int]]</code> <p>list of split positions of the tokens</p> <code>None</code> <code>extraction_config</code> <code>ExtractionConfig</code> <p>configuration of the extraction of the activations of the model</p> <code>ExtractionConfig()</code> <code>ablation_queries</code> <code>Optional[DataFrame | None]</code> <p>dataframe with the ablation queries to perform during forward pass</p> <code>None</code> <code>patching_queries</code> <code>Optional[DataFrame | None]</code> <p>dataframe with the patching queries to perform during forward pass</p> <code>None</code> <code>external_cache</code> <code>Optional[ActivationCache]</code> <p>external cache to use in the forward pass</p> <code>None</code> <code>attn_heads</code> <code>Union[list[dict], Literal['all']]</code> <p>list of dictionaries with the layer and head to extract the attention pattern or 'all' to</p> <code>'all'</code> <code>batch_idx</code> <code>Optional[int]</code> <p>index of the batch in the dataloader</p> <code>None</code> <code>move_to_cpu</code> <code>bool</code> <p>if True, move the activations to the cpu</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cache</code> <code>ActivationCache</code> <p>dictionary with the activations of the model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n&gt;&gt;&gt; model.forward(inputs, target_token_positions=[\"last\"], extract_resid_out=True)\n{'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'input_ids': tensor([[101, 1234, 1235, 102]]), 'mapping_index': {'last': [0]}}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@torch.no_grad()\ndef forward(\n    self,\n    inputs,\n    target_token_positions: List[str] = [\"last\"],\n    pivot_positions: Optional[List[int]] = None,\n    extraction_config: ExtractionConfig = ExtractionConfig(),\n    ablation_queries: Optional[List[dict]] = None,\n    patching_queries: Optional[List[dict]] = None,\n    external_cache: Optional[ActivationCache] = None,\n    attn_heads: Union[list[dict], Literal[\"all\"]] = \"all\",\n    batch_idx: Optional[int] = None,\n    move_to_cpu: bool = False,\n) -&gt; ActivationCache:\n    r\"\"\"\n    Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.\n\n    Args:\n        inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)\n        target_token_positions (list[str]): list of tokens to extract the activations from ([\"last\", \"end-image\", \"start-image\", \"first\"])\n        pivot_positions (Optional[list[int]]): list of split positions of the tokens\n        extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model\n        ablation_queries (Optional[pd.DataFrame | None]): dataframe with the ablation queries to perform during forward pass\n        patching_queries (Optional[pd.DataFrame | None]): dataframe with the patching queries to perform during forward pass\n        external_cache (Optional[ActivationCache]): external cache to use in the forward pass\n        attn_heads (Union[list[dict], Literal[\"all\"]]): list of dictionaries with the layer and head to extract the attention pattern or 'all' to\n        batch_idx (Optional[int]): index of the batch in the dataloader\n        move_to_cpu (bool): if True, move the activations to the cpu\n\n    Returns:\n        cache (ActivationCache): dictionary with the activations of the model\n\n    Examples:\n        &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n        &gt;&gt;&gt; model.forward(inputs, target_token_positions=[\"last\"], extract_resid_out=True)\n        {'resid_out_0': tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), 'input_ids': tensor([[101, 1234, 1235, 102]]), 'mapping_index': {'last': [0]}}\n    \"\"\"\n\n    if target_token_positions is None and extraction_config.is_not_empty():\n        raise ValueError(\n            \"target_token_positions must be passed if we want to extract the activations of the model\"\n        )\n    cache = ActivationCache()\n    string_tokens = self.to_string_tokens(\n        self.input_handler.get_input_ids(inputs).squeeze()\n    )\n    token_index, token_dict = TokenIndex(\n        self.config.model_name, pivot_positions=pivot_positions\n    ).get_token_index(\n        tokens=target_token_positions,\n        string_tokens=string_tokens,\n        return_type=\"all\",\n    )\n    assert isinstance(token_index, list), \"Token index must be a list\"\n    assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n\n    hooks = self.create_hooks(  # TODO: add **kwargs\n        inputs=inputs,\n        token_dict=token_dict,\n        token_index=token_index,\n        cache=cache,\n        extraction_config=extraction_config,\n        ablation_queries=ablation_queries,\n        patching_queries=patching_queries,\n        batch_idx=batch_idx,\n        external_cache=external_cache,\n    )\n\n    hook_handlers = self.set_hooks(hooks)\n    inputs = self.input_handler.prepare_inputs(\n        inputs, self.first_device, self.config.torch_dtype\n    )\n    # forward pass\n    output = self.hf_model(\n        **inputs,\n        # output_original_output=True,\n        # output_attentions=extract_attn_pattern,\n    )\n\n    cache[\"logits\"] = output.logits[:, -1]\n    # since attention_patterns are returned in the output, we need to adapt to the cache structure\n    if move_to_cpu:\n        cache.cpu()\n        if external_cache is not None:\n            external_cache.cpu()\n\n    mapping_index = {}\n    current_index = 0\n    for token in target_token_positions:\n        mapping_index[token] = []\n        if isinstance(token_dict, int):\n            mapping_index[token].append(current_index)\n            current_index += 1\n        elif isinstance(token_dict, dict):\n            for idx in range(len(token_dict[token])):\n                mapping_index[token].append(current_index)\n                current_index += 1\n        elif isinstance(token_dict, list):\n            for idx in range(len(token_dict)):\n                mapping_index[token].append(current_index)\n                current_index += 1\n        else:\n            raise ValueError(\"Token dict must be an int, a dict or a list\")\n    cache[\"mapping_index\"] = mapping_index\n\n    self.remove_hooks(hook_handlers)\n\n    return cache\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.generate","title":"<code>generate(inputs, generation_config=None, target_token_positions=None, return_text=False, **kwargs)</code>","text":"<p>WARNING: This method could be buggy in the return dict of the output. Pay attention!</p> <p>Generate new tokens using the model and the inputs passed as argument Args:     inputs (dict): dictionary with the inputs of the model {\"input_ids\": ..., \"attention_mask\": ..., \"pixel_values\": ...}     generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration     **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries) Returns:     output (ActivationCache): dictionary with the output of the model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n&gt;&gt;&gt; model.generate(inputs)\n{'sequences': tensor([[101, 1234, 1235, 102]])}\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    inputs,\n    generation_config: Optional[GenerationConfig] = None,\n    target_token_positions: Optional[List[str]] = None,\n    return_text: bool = False,\n    **kwargs,\n) -&gt; ActivationCache:\n    r\"\"\"\n    __WARNING__: This method could be buggy in the return dict of the output. Pay attention!\n\n    Generate new tokens using the model and the inputs passed as argument\n    Args:\n        inputs (dict): dictionary with the inputs of the model {\"input_ids\": ..., \"attention_mask\": ..., \"pixel_values\": ...}\n        generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration\n        **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)\n    Returns:\n        output (ActivationCache): dictionary with the output of the model\n\n    Examples:\n        &gt;&gt;&gt; inputs = {\"input_ids\": torch.tensor([[101, 1234, 1235, 102]]), \"attention_mask\": torch.tensor([[1, 1, 1, 1]])}\n        &gt;&gt;&gt; model.generate(inputs)\n        {'sequences': tensor([[101, 1234, 1235, 102]])}\n    \"\"\"\n    # Initialize cache for logits\n    # TODO FIX THIS. IT is not general and it is not working\n    # raise NotImplementedError(\"This method is not working. It needs to be fixed\")\n    hook_handlers = None\n    if target_token_positions is not None:\n        string_tokens = self.to_string_tokens(\n            self.input_handler.get_input_ids(inputs).squeeze()\n        )\n        token_index, token_dict = TokenIndex(\n            self.config.model_name, pivot_positions=None\n        ).get_token_index(tokens=[], string_tokens=string_tokens, return_type=\"all\")\n        assert isinstance(token_index, list), \"Token index must be a list\"\n        assert isinstance(token_dict, dict), \"Token dict must be a dict\"\n        hooks = self.create_hooks(\n            inputs=inputs,\n            token_dict=token_dict,\n            token_index=token_index,\n            cache=ActivationCache(),\n            **kwargs,\n        )\n        hook_handlers = self.set_hooks(hooks)\n\n    inputs = self.input_handler.prepare_inputs(inputs, self.first_device)\n\n    output = self.hf_model.generate(\n        **inputs,  # type: ignore\n        generation_config=generation_config,\n        output_scores=False,  # type: ignore\n    )\n    if hook_handlers:\n        self.remove_hooks(hook_handlers)\n    if return_text:\n        return self.hf_tokenizer.decode(output[0], skip_special_tokens=True)  # type: ignore\n    return output  # type: ignore\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_module_from_string","title":"<code>get_module_from_string(component)</code>","text":"<p>Return a module from the model given the string of the module.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>str</code> <p>the string of the module</p> required <p>Returns:</p> Name Type Description <code>module</code> <code>Module</code> <p>the module of the model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model.get_module_from_string(\"model.layers[0].self_attn\")\nBertAttention(...)\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_module_from_string(self, component: str):\n    r\"\"\"\n    Return a module from the model given the string of the module.\n\n    Args:\n        component (str): the string of the module\n\n    Returns:\n        module (torch.nn.Module): the module of the model\n\n    Examples:\n        &gt;&gt;&gt; model.get_module_from_string(\"model.layers[0].self_attn\")\n        BertAttention(...)\n    \"\"\"\n    return self.hf_model.retrieve_modules_from_names(component)\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_processor","title":"<code>get_processor()</code>","text":"<p>Return the processor of the model (None if the model does not have a processor, i.e. text only model)</p> <p>Returns:</p> Name Type Description <code>processor</code> <p>the processor of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_processor(self):\n    r\"\"\"\n    Return the processor of the model (None if the model does not have a processor, i.e. text only model)\n\n    Args:\n        None\n\n    Returns:\n        processor: the processor of the model\n    \"\"\"\n    if self.processor is None:\n        raise ValueError(\"The model does not have a processor\")\n    return self.processor\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.get_text_tokenizer","title":"<code>get_text_tokenizer()</code>","text":"<p>If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer</p> <p>Returns:</p> Name Type Description <code>tokenizer</code> <p>the tokenizer of the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def get_text_tokenizer(self):\n    r\"\"\"\n    If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer\n\n    Args:\n        None\n\n    Returns:\n        tokenizer: the tokenizer of the model\n    \"\"\"\n    if self.processor is not None:\n        if not hasattr(self.processor, \"tokenizer\"):\n            raise ValueError(\"The processor does not have a tokenizer\")\n        return self.processor.tokenizer  # type: ignore\n    return self.hf_tokenizer\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.register_forward_hook","title":"<code>register_forward_hook(component, hook_function)</code>","text":"<p>Add a new hook to the model. The hook will be called in the forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>str</code> <p>the component of the model where the hook will be added.</p> required <code>hook_function</code> <code>Callable</code> <p>the function that will be called in the forward pass of the model. The function must have the following signature: def hook_function(module, input, output):     pass</p> required <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def hook_function(module, input, output):\n&gt;&gt;&gt;     # your code here\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt; model.register_forward_hook(\"model.layers[0].self_attn\", hook_function)\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def register_forward_hook(self, component: str, hook_function: Callable):\n    r\"\"\"\n    Add a new hook to the model. The hook will be called in the forward pass of the model.\n\n    Args:\n        component (str): the component of the model where the hook will be added.\n        hook_function (Callable): the function that will be called in the forward pass of the model. The function must have the following signature:\n            def hook_function(module, input, output):\n                pass\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; def hook_function(module, input, output):\n        &gt;&gt;&gt;     # your code here\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt; model.register_forward_hook(\"model.layers[0].self_attn\", hook_function)\n    \"\"\"\n    self.additional_hooks.append(\n        {\n            \"component\": component,\n            \"intervention\": hook_function,\n        }\n    )\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.remove_hooks","title":"<code>remove_hooks(hook_handlers)</code>","text":"<p>Remove all the hooks from the model</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def remove_hooks(self, hook_handlers):\n    \"\"\"\n    Remove all the hooks from the model\n    \"\"\"\n    for hook_handler in hook_handlers:\n        hook_handler.remove()\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.set_hooks","title":"<code>set_hooks(hooks)</code>","text":"<p>Set the hooks in the model</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[dict]</code> <p>list of dictionaries with the component and the intervention to perform in the forward pass of the model</p> required <p>Returns:</p> Name Type Description <code>hook_handlers</code> <code>list</code> <p>list of hook handlers</p> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def set_hooks(self, hooks: List[Dict[str, Any]]):\n    r\"\"\"\n    Set the hooks in the model\n\n    Args:\n        hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model\n\n    Returns:\n        hook_handlers (list): list of hook handlers\n    \"\"\"\n\n    if len(hooks) == 0:\n        return []\n\n    hook_handlers = []\n    for hook in hooks:\n        component = hook[\"component\"]\n        hook_function = hook[\"intervention\"]\n\n        # get the last module string (.input or .output) and remove it from the component string\n        last_module = component.split(\".\")[-1]\n        # now remove the last module from the component string\n        component = component[: -len(last_module) - 1]\n        # check if the component exists in the model\n        try:\n            self.assert_module_exists(component)\n        except ValueError as e:\n            self.logger.warning(\n                f\"Error: {e}. Probably the module {component} do not exists in the model. If the module is the attention_matrix_hook, try callig HookedModel.set_custom_hooks() or setting attn_implementation == 'custom_eager'.  Now we will skip the hook for the component {component}\",\n                std_out=True,\n            )\n            continue\n        if last_module == \"input\":\n            hook_handlers.append(\n                get_module_by_path(\n                    self.hf_model, component\n                ).register_forward_pre_hook(\n                    partial(hook_function, output=None), with_kwargs=True\n                )\n            )\n        elif last_module == \"output\":\n            hook_handlers.append(\n                get_module_by_path(self.hf_model, component).register_forward_hook(\n                    hook_function, with_kwargs=True\n                )\n            )\n\n    return hook_handlers\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModel.to_string_tokens","title":"<code>to_string_tokens(tokens)</code>","text":"<p>Transform a list or a tensor of tokens in a list of string tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Union[list, Tensor]</code> <p>the tokens to transform in string tokens</p> required <p>Returns:</p> Name Type Description <code>string_tokens</code> <code>list</code> <p>the list of string tokens</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokens = [101, 1234, 1235, 102]\n&gt;&gt;&gt; model.to_string_tokens(tokens)\n['[CLS]', 'hello', 'world', '[SEP]']\n</code></pre> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>def to_string_tokens(\n    self,\n    tokens: Union[list, torch.Tensor],\n):\n    r\"\"\"\n    Transform a list or a tensor of tokens in a list of string tokens.\n\n    Args:\n        tokens (Union[list, torch.Tensor]): the tokens to transform in string tokens\n\n    Returns:\n        string_tokens (list): the list of string tokens\n\n    Examples:\n        &gt;&gt;&gt; tokens = [101, 1234, 1235, 102]\n        &gt;&gt;&gt; model.to_string_tokens(tokens)\n        ['[CLS]', 'hello', 'world', '[SEP]']\n    \"\"\"\n    if isinstance(tokens, torch.Tensor):\n        if tokens.dim() == 1:\n            tokens = tokens.tolist()\n        else:\n            tokens = tokens.squeeze().tolist()\n    string_tokens = []\n    for tok in tokens:\n        string_tokens.append(self.hf_tokenizer.decode(tok))  # type: ignore\n    return string_tokens\n</code></pre>"},{"location":"api/interpretability/hooked_model/#easyroutine.interpretability.hooked_model.HookedModelConfig","title":"<code>HookedModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration of the HookedModel</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>the name of the model to load</p> required <code>device_map</code> <code>Literal['balanced', 'cuda', 'cpu', 'auto']</code> <p>the device to use for the model</p> <code>'balanced'</code> <code>torch_dtype</code> <code>dtype</code> <p>the dtype of the model</p> <code>bfloat16</code> <code>attn_implementation</code> <code>Literal['eager', 'flash_attention_2']</code> <p>the implementation of the attention</p> <code>'custom_eager'</code> <code>batch_size</code> <code>int</code> <p>the batch size of the model. FOR NOW, ONLY BATCH SIZE 1 IS SUPPORTED. USE AT YOUR OWN RISK</p> <code>1</code> Source code in <code>easyroutine/interpretability/hooked_model.py</code> <pre><code>@dataclass\nclass HookedModelConfig:\n    \"\"\"\n    Configuration of the HookedModel\n\n    Arguments:\n        model_name (str): the name of the model to load\n        device_map (Literal[\"balanced\", \"cuda\", \"cpu\", \"auto\"]): the device to use for the model\n        torch_dtype (torch.dtype): the dtype of the model\n        attn_implementation (Literal[\"eager\", \"flash_attention_2\"]): the implementation of the attention\n        batch_size (int): the batch size of the model. FOR NOW, ONLY BATCH SIZE 1 IS SUPPORTED. USE AT YOUR OWN RISK\n    \"\"\"\n\n    model_name: str\n    device_map: Literal[\"balanced\", \"cuda\", \"cpu\", \"auto\"] = \"balanced\"\n    torch_dtype: torch.dtype = torch.bfloat16\n    attn_implementation: Literal[\"eager\", \"custom_eager\"] = (\n        \"custom_eager\"  # TODO: add flash_attention_2 in custom module to support it\n    )\n    batch_size: int = 1\n</code></pre>"},{"location":"api/interpretability/hooks/","title":"Hooks","text":""},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.ablate_attn_mat_hook","title":"<code>ablate_attn_mat_hook(module, args, kwargs, output, ablation_queries)</code>","text":"<p>Hook function to ablate the tokens in the attention mask. It will set to 0 the value vector of the tokens to ablate</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def ablate_attn_mat_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    ablation_queries: pd.DataFrame,\n):\n    r\"\"\"\n    Hook function to ablate the tokens in the attention\n    mask. It will set to 0 the value vector of the\n    tokens to ablate\n    \"\"\"\n    # Get the shape of the attention matrix\n    b = process_args_kwargs_output(args, kwargs, output)\n    batch_size, num_heads, seq_len_q, seq_len_k = b.shape\n\n    q_positions = ablation_queries[\"queries\"].iloc[0]\n\n    # Used during generation\n    if seq_len_q &lt; len(q_positions):\n        q_positions = 0\n\n    k_positions = ablation_queries[\"keys\"].iloc[0]\n\n    # Create boolean masks for queries and keys\n    q_mask = torch.zeros(seq_len_q, dtype=torch.bool, device=b.device)\n    q_mask[q_positions] = True  # Set positions to True\n\n    k_mask = torch.zeros(seq_len_k, dtype=torch.bool, device=b.device)\n    k_mask[k_positions] = True  # Set positions to TrueW\n\n    # Create a 2D mask using outer product\n    head_mask = torch.outer(q_mask, k_mask)  # Shape: (seq_len_q, seq_len_k)\n\n    # Expand mask to match the dimensions of the attention matrix\n    # Shape after expand: (batch_size, num_heads, seq_len_q, seq_len_k)\n    head_mask = (\n        head_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, num_heads, -1, -1)\n    )\n\n    # Apply the ablation function directly to the attention matrix\n    b[head_mask] = zero_ablation(b[head_mask])\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.ablate_heads_hook","title":"<code>ablate_heads_hook(module, args, kwargs, output, ablation_queries)</code>","text":"<p>Hook function to ablate the heads in the attention mask. It will set to 0 the output of the heads to ablate</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def ablate_heads_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    ablation_queries: pd.DataFrame,\n):\n    r\"\"\"\n    Hook function to ablate the heads in the attention\n    mask. It will set to 0 the output of the heads to\n    ablate\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    attention_matrix = b.clone().data\n\n    for head in ablation_queries[\"head\"]:\n        attention_matrix[0, head, :, :] = 0\n\n    b.copy_(attention_matrix)\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook","title":"<code>ablate_pos_keep_self_attn_hook(module, args, kwargs, output, ablation_queries)</code>","text":"<p>Hook function to ablate the tokens in the attention mask but keeping the self attn weigths. It will set to 0 the row of tokens to ablate except for the las position</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def ablate_pos_keep_self_attn_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    ablation_queries: pd.DataFrame,\n):\n    r\"\"\"\n    Hook function to ablate the tokens in the attention\n    mask but keeping the self attn weigths.\n    It will set to 0 the row of tokens to ablate except for\n    the las position\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    Warning(\"This function is deprecated. Use ablate_attn_mat_hook instead\")\n    attn_matrix = b.data\n    # initial_shape = attn_matrix.shape\n\n    for head, pos in zip(\n        ablation_queries[\"head\"], ablation_queries[\"pos_token_to_ablate\"]\n    ):\n        attn_matrix[0, head, pos, :-1] = 0\n\n    b.copy_(attn_matrix)\n\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.ablate_tokens_hook_flash_attn","title":"<code>ablate_tokens_hook_flash_attn(module, args, kwargs, output, ablation_queries, num_layers=32)</code>","text":"<p>same of ablate_tokens_hook but for flash attention. This apply the ablation on the values vectors instead of the attention mask</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def ablate_tokens_hook_flash_attn(\n    module,\n    args,\n    kwargs,\n    output,\n    ablation_queries: pd.DataFrame,\n    num_layers: int = 32,\n):\n    r\"\"\"\n    same of ablate_tokens_hook but for flash attention. This apply the ablation on the values vectors instead of the attention mask\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    batch_size, seq, d_model = b.shape\n    if seq == 1:\n        return b\n    values = b.clone().data\n    device = values.device\n\n    ablation_queries.reset_index(\n        drop=True, inplace=True\n    )  # Reset index to avoid problems with casting to tensor\n    head_indices = torch.tensor(\n        ablation_queries[\"head\"], dtype=torch.long, device=device\n    )\n    pos_indices = torch.tensor(\n        ablation_queries[\"keys\"], dtype=torch.long, device=device\n    )\n    # if num_layers != len(head_indices) or not torch.all(pos_indices == pos_indices[0]) :\n    #     raise ValueError(\"Flash attention ablation should be done on all heads at the same layer and at the same token position\")\n    # if seq &lt; pos_indices[0]:\n    #     # during generation the desired value vector has already been ablated\n    #     return b\n    pos_indices = pos_indices[0]\n    # Use advanced indexing to set the specified slices to zero\n    values[..., pos_indices, :] = 0\n\n    b.copy_(values)\n\n    #!!dirty fix\n\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.attention_pattern_head","title":"<code>attention_pattern_head(module, args, kwargs, output, token_index, layer, cache, head='all', act_on_input=False)</code>","text":"<p>Hook function to extract the attention pattern of the heads. It will extract the attention pattern. As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>b</code> <p>the input of the hook function. It's the output of the attention pattern of the heads</p> required <code>-</code> <code>s</code> <p>the state of the hook function. It's the state of the model</p> required <code>-</code> <code>layer</code> <p>the layer of the model</p> required <code>-</code> <code>head</code> <p>the head of the model</p> required <code>-</code> <code>expand_head</code> <p>bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like \"pattern_L0H0\", \"pattern_L0H1\", ...             while if False, we will have only one key for each layer, like \"pattern_L0\" and the dimension of the head will be taken into account in the tensor.</p> required Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def attention_pattern_head(\n    module,\n    args,\n    kwargs,\n    output,\n    token_index,\n    layer,\n    cache,\n    head: Union[str, int] = \"all\",\n    act_on_input=False,\n):\n    \"\"\"\n    Hook function to extract the attention pattern of the heads. It will extract the attention pattern.\n    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)\n\n    Args:\n        - b: the input of the hook function. It's the output of the attention pattern of the heads\n        - s: the state of the hook function. It's the state of the model\n        - layer: the layer of the model\n        - head: the head of the model\n        - expand_head: bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like \"pattern_L0H0\", \"pattern_L0H1\", ...\n                        while if False, we will have only one key for each layer, like \"pattern_L0\" and the dimension of the head will be taken into account in the tensor.\n\n    \"\"\"\n    # first get the attention pattern\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    attn_pattern = b.data.detach().clone()  # (batch, num_heads,seq_len, seq_len)\n\n    if head == \"all\":\n        for head_idx in range(attn_pattern.size(1)):\n            key = f\"pattern_L{layer}H{head_idx}\"\n            cache[key] = attn_pattern[:, head_idx, token_index][:, :, token_index]\n    else:\n        cache[f\"pattern_L{layer}H{head}\"] = attn_pattern[:, head, token_index][\n            :, :, token_index\n        ]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.avg_attention_pattern_head","title":"<code>avg_attention_pattern_head(module, args, kwargs, output, token_index, layer, attn_pattern_current_avg, batch_idx, cache, avg=False, extract_avg_value=False, act_on_input=False)</code>","text":"<p>Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it. As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>b</code> <p>the input of the hook function. It's the output of the attention pattern of the heads</p> required <code>-</code> <code>s</code> <p>the state of the hook function. It's the state of the model</p> required <code>-</code> <code>layer</code> <p>the layer of the model</p> required <code>-</code> <code>head</code> <p>the head of the model</p> required <code>-</code> <code>attn_pattern_current_avg</code> <p>the current average attention pattern</p> required Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def avg_attention_pattern_head(\n    module,\n    args,\n    kwargs,\n    output,\n    token_index,\n    layer,\n    attn_pattern_current_avg,\n    batch_idx,\n    cache,\n    avg: bool = False,\n    extract_avg_value: bool = False,\n    act_on_input=False,\n):\n    \"\"\"\n    Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it.\n    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)\n\n    Args:\n        - b: the input of the hook function. It's the output of the attention pattern of the heads\n        - s: the state of the hook function. It's the state of the model\n        - layer: the layer of the model\n        - head: the head of the model\n        - attn_pattern_current_avg: the current average attention pattern\n    \"\"\"\n    # first get the attention pattern\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    attn_pattern = b.data.detach().clone()  # (batch, num_heads,seq_len, seq_len)\n    # attn_pattern = attn_pattern.to(torch.float32)\n    num_heads = attn_pattern.size(1)\n    for head in range(num_heads):\n        key = f\"avg_pattern_L{layer}H{head}\"\n        if key not in attn_pattern_current_avg:\n            attn_pattern_current_avg[key] = attn_pattern[:, head, token_index][\n                :, :, token_index\n            ]\n        else:\n            attn_pattern_current_avg[key] += (\n                attn_pattern[:, head, token_index][:, :, token_index]\n                - attn_pattern_current_avg[key]\n            ) / (batch_idx + 1)\n        attn_pattern_current_avg[key] = attn_pattern_current_avg[key]\n        # var_key = f\"M2_pattern_L{layer}H{head}\"\n        # if var_key not in attn_pattern_current_avg:\n        #     attn_pattern_current_avg[var_key] = torch.zeros_like(attn_pattern[:, head])\n        # attn_pattern_current_avg[var_key] = attn_pattern_current_avg[var_key] + (attn_pattern[:, head] - attn_pattern_current_avg[key]) * (attn_pattern[:, head] - attn_pattern_current_avg[var_key])\n\n        if extract_avg_value:\n            value_key = f\"projected_value_L{layer}H{head}\"\n            try:\n                values = cache[value_key]\n            except KeyError:\n                print(f\"Values not found for {value_key}\")\n                return\n            # get the attention pattern for the values\n            value_norm = torch.norm(values, dim=-1)\n\n            norm_matrix = (\n                value_norm.unsqueeze(1).expand_as(attn_pattern[:, head]).transpose(1, 2)\n            )\n\n            norm_matrix = norm_matrix * attn_pattern[:, head]\n\n            if value_key not in attn_pattern_current_avg:\n                attn_pattern_current_avg[value_key] = norm_matrix[..., token_index, :]\n            else:\n                attn_pattern_current_avg[value_key] += (\n                    norm_matrix[..., token_index, :]\n                    - attn_pattern_current_avg[value_key]\n                ) / (batch_idx + 1)\n\n            # remove values from cache\n            del cache[value_key]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.avg_hook","title":"<code>avg_hook(module, args, kwargs, output, cache, cache_key, last_image_idx, end_image_idx)</code>","text":"<p>It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def avg_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache,\n    cache_key,\n    last_image_idx,\n    end_image_idx,\n):\n    r\"\"\"\n    It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    img_avg = torch.mean(\n        b.data.detach().clone()[:, 1 : last_image_idx + 1, :],\n        dim=1,\n    )\n    text_avg = torch.mean(b.data.detach().clone()[:, end_image_idx:, :], dim=1)\n    all_avg = torch.mean(b.data.detach().clone()[:, :, :], dim=1)\n\n    cache[f\"avg_{cache_key}\"] = torch.cat(\n        [img_avg.unsqueeze(1), text_avg.unsqueeze(1), all_avg.unsqueeze(1)], dim=1\n    )\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.create_dynamic_hook","title":"<code>create_dynamic_hook(pyvene_hook, **kwargs)</code>","text":"<p>DEPRECATED: pyvene is not used anymore. This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def create_dynamic_hook(pyvene_hook: Callable, **kwargs):\n    r\"\"\"\n    DEPRECATED: pyvene is not used anymore.\n    This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.\n    \"\"\"\n    partial_hook = partial(pyvene_hook, **kwargs)\n\n    def wrap(*args, **kwargs):\n        return partial_hook(*args, **kwargs)\n\n    return wrap\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.embed_hook","title":"<code>embed_hook(module, input, output, cache, cache_key)</code>","text":"<p>Hook function to extract the embeddings of the tokens. It will save the embeddings in the cache (a global variable out the scope of the function)</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def embed_hook(module, input, output, cache, cache_key):\n    r\"\"\"\n    Hook function to extract the embeddings of the tokens. It will save the embeddings in the cache (a global variable out the scope of the function)\n    \"\"\"\n    if output is None:\n        b = input[0]\n    else:\n        b = output\n    cache[cache_key] = b.data.detach().clone()\n    return b\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.projected_value_vectors_head","title":"<code>projected_value_vectors_head(module, args, kwargs, output, layer, cache, token_index, num_attention_heads, num_key_value_heads, hidden_size, d_head, out_proj_weight, out_proj_bias, head='all', act_on_input=False, expand_head=True, avg=False)</code>","text":"<p>Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>the input of the hook function. It's the output of the values vectors of the heads</p> required <code>s</code> <p>the state of the hook function. It's the state of the model</p> required <code>layer</code> <p>the layer of the model</p> required <code>head</code> <code>Union[str, int]</code> <p>the head of the model. If \"all\" is passed, it will extract all the heads of the layer</p> <code>'all'</code> <code>expand_head</code> <code>bool</code> <p>bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like \"value_L0H0\", \"value_L0H1\", ...             while if False, we will have only one key for each layer, like \"value_L0\" and the dimension of the head will be taken into account in the tensor.</p> <code>True</code> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def projected_value_vectors_head(\n    module,\n    args,\n    kwargs,\n    output,\n    layer,\n    cache,\n    token_index,\n    num_attention_heads: int,\n    num_key_value_heads: int,\n    hidden_size: int,\n    d_head: int,\n    out_proj_weight,\n    out_proj_bias,\n    head: Union[str, int] = \"all\",\n    act_on_input=False,\n    expand_head: bool = True,\n    avg=False,\n):\n    r\"\"\"\n    Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection\n    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)\n\n    Args:\n        b: the input of the hook function. It's the output of the values vectors of the heads\n        s: the state of the hook function. It's the state of the model\n        layer: the layer of the model\n        head: the head of the model. If \"all\" is passed, it will extract all the heads of the layer\n        expand_head: bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like \"value_L0H0\", \"value_L0H1\", ...\n                        while if False, we will have only one key for each layer, like \"value_L0\" and the dimension of the head will be taken into account in the tensor.\n\n    \"\"\"\n    # first get the values vectors\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    values = b.data.detach().clone()  # (batch, num_heads,seq_len, head_dim)\n\n    # reshape the values vectors to have a separate dimension for the different heads\n    values = rearrange(\n        values,\n        \"batch seq_len (num_key_value_heads d_heads) -&gt; batch num_key_value_heads seq_len d_heads\",\n        num_key_value_heads=num_key_value_heads,\n        d_heads=d_head,\n    )\n\n    #        \"batch seq_len (num_key_value_heads d_heads) -&gt; batch seq_len num_key_value_heads d_heads\",\n\n    values = repeat_kv(values, num_attention_heads // num_key_value_heads)\n\n    values = rearrange(\n        values,\n        \"batch num_head seq_len d_model -&gt; batch seq_len num_head d_model\",\n    )\n\n    # reshape in order to get the blocks for each head\n    out_proj_weight = out_proj_weight.t().view(\n        num_attention_heads,\n        d_head,\n        hidden_size,\n    )\n\n    # apply bias if present (No in Chameleon)\n    if out_proj_bias is not None:\n        out_proj_bias = out_proj_bias.view(1, 1, 1, hidden_size)\n\n    # apply the projection for each head\n    projected_values = einsum(\n        values,\n        out_proj_weight,\n        \"batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model\",\n    )\n    if out_proj_bias is not None:\n        projected_values = projected_values + out_proj_bias\n\n    # rearrange the tensor to have dimension that we like more\n    projected_values = rearrange(\n        projected_values,\n        \"batch seq_len num_head d_model -&gt; batch num_head seq_len d_model\",\n    )\n\n    # slice for token index\n    projected_values = projected_values[:, :, token_index, :]\n\n    if avg:\n        projected_values = torch.mean(projected_values, dim=-1, keepdim=True)\n\n    # post-process the values vectors\n    if head == \"all\":\n        for head_idx in range(num_attention_heads):\n            cache[f\"projected_value_L{layer}H{head_idx}\"] = projected_values[\n                :, head_idx\n            ]\n    else:\n        cache[f\"projected_value_L{layer}H{head}\"] = projected_values[:, int(head)]\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.query_key_value_hook","title":"<code>query_key_value_hook(module, args, kwargs, output, cache, cache_key, token_index, layer, head_dim, num_key_value_groups, head='all', avg=False)</code>","text":"<p>Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def query_key_value_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache: ActivationCache,\n    cache_key,\n    token_index,\n    layer,\n    head_dim,\n    num_key_value_groups:int,\n    head: Union[str, int] = \"all\",\n    avg: bool = False,\n):\n    r\"\"\"\n    Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n    input_shape = b.shape[:-1]\n    hidden_shape = (*input_shape, -1, head_dim)\n    b = b.view(hidden_shape).transpose(1, 2)\n    # cache[cache_key] = b.data.detach().clone()[..., token_index, :]\n\n    info_string = \"Shape: batch seq_len d_head\"\n\n\n    heads = [idx for idx in range(b.size(1))] if head == \"all\" else [head]\n    for head_idx in heads:\n        # compute group index\n        group_idx = head_idx // (b.size(1) // num_key_value_groups)\n        if \"values_\" in cache_key or \"keys_\" in cache_key:\n            cache.add_with_info(\n                f\"{cache_key}L{layer}H{head_idx}\",\n                b.data.detach().clone()[:, group_idx, token_index, :],\n                info_string,\n            )\n        else:\n            cache.add_with_info(\n                f\"{cache_key}L{layer}H{head_idx}\",\n                b.data.detach().clone()[:, head_idx, token_index, :],\n                info_string,\n            )\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.save_resid_hook","title":"<code>save_resid_hook(module, args, kwargs, output, cache, cache_key, token_index, avg=False)</code>","text":"<p>It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def save_resid_hook(\n    module,\n    args,\n    kwargs,\n    output,\n    cache: ActivationCache,\n    cache_key,\n    token_index,\n    avg: bool = False,\n):\n    r\"\"\"\n    It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)\n    \"\"\"\n    b = process_args_kwargs_output(args, kwargs, output)\n\n    # slice the tensor to get the activations of the token we want to extract\n    cache[cache_key] = b.data.detach().clone()[..., token_index, :]\n\n    if avg:\n        cache[cache_key] = torch.mean(cache[cache_key], dim=-2, keepdim=True)\n</code></pre>"},{"location":"api/interpretability/hooks/#easyroutine.interpretability.hooks.zero_ablation","title":"<code>zero_ablation(tensor)</code>","text":"<p>Set the attention values to zero</p> Source code in <code>easyroutine/interpretability/hooks.py</code> <pre><code>def zero_ablation(tensor):\n    r\"\"\"\n    Set the attention values to zero\n    \"\"\"\n    return torch.zeros_like(tensor)\n</code></pre>"},{"location":"api/interpretability/models/","title":"Models","text":""},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration class for storing model specific parameters.</p> <p>Attributes:</p> Name Type Description <code>residual_stream_input_hook_name</code> <code>str</code> <p>Name of the residual stream torch module where attach the hook</p> <code>residual_stream_hook_name</code> <code>str</code> <p>Name of the residual stram torch module where attach the hook</p> <code>intermediate_stream_hook_name</code> <code>str</code> <p>Name of the intermediate stream torch module where attach the hook</p> <code>residual_stream_input_post_layernorm_hook_name</code> <code>str</code> <p>Name of the residual stream input post layer norm</p> <code>head_key_hook_name</code> <code>str</code> <p>Name of the attention key torch module where attach the hook</p> <code>head_value_hook_name</code> <code>str</code> <p>Name of the attention value torch module where attach the hook</p> <code>head_query_hook_name</code> <code>str</code> <p>Name of the attention key torch module where attach the hook</p> <code>attn_in_hook_name</code> <code>str</code> <p>Name of the attention input torch module where attach the hook</p> <code>attn_out_hook_name</code> <code>str</code> <p>Name of the attention output torch module where attach the hook</p> <code>attn_matrix_hook_name</code> <code>str</code> <p>Name of the attention matrix torch module where attach the hook</p> <code>mlp_out_hook_name</code> <code>str</code> <p>Name of the mlp output torch module where attach the hook</p> <code>attn_out_proj_weight</code> <code>str</code> <p>Name of the attention output projection weight</p> <code>attn_out_proj_bias</code> <code>str</code> <p>Name of the attention output projection bias</p> <code>embed_tokens</code> <code>str</code> <p>Name of the embedding tokens torch module where attach the hook</p> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers</p> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads</p> <code>hidden_size</code> <code>int</code> <p>Hidden size of the transformer model</p> <code>num_key_value_heads</code> <code>int</code> <p>Number of key value heads</p> <code>num_key_value_groups</code> <code>int</code> <p>Number of key value groups</p> <code>head_dim</code> <code>int</code> <p>Dimension of the attention head</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    r\"\"\"\n    Configuration class for storing model specific parameters.\n\n    Attributes:\n        residual_stream_input_hook_name (str): Name of the residual stream torch module where attach the hook\n        residual_stream_hook_name (str): Name of the residual stram torch module where attach the hook\n        intermediate_stream_hook_name (str): Name of the intermediate stream torch module where attach the hook\n        residual_stream_input_post_layernorm_hook_name (str): Name of the residual stream input post layer norm\n        head_key_hook_name (str): Name of the attention key torch module where attach the hook\n        head_value_hook_name (str): Name of the attention value torch module where attach the hook\n        head_query_hook_name (str): Name of the attention key torch module where attach the hook\n        attn_in_hook_name (str): Name of the attention input torch module where attach the hook\n        attn_out_hook_name (str): Name of the attention output torch module where attach the hook\n        attn_matrix_hook_name (str): Name of the attention matrix torch module where attach the hook\n        mlp_out_hook_name (str): Name of the mlp output torch module where attach the hook\n        attn_out_proj_weight (str): Name of the attention output projection weight\n        attn_out_proj_bias (str): Name of the attention output projection bias\n        embed_tokens (str): Name of the embedding tokens torch module where attach the hook\n        num_hidden_layers (int): Number of hidden layers\n        num_attention_heads (int): Number of attention heads\n        hidden_size (int): Hidden size of the transformer model\n        num_key_value_heads (int): Number of key value heads\n        num_key_value_groups (int): Number of key value groups\n        head_dim (int): Dimension of the attention head\n\n    \"\"\"\n\n    residual_stream_input_hook_name: str\n    residual_stream_hook_name: str\n    intermediate_stream_hook_name: str\n    residual_stream_input_post_layernorm_hook_name: str\n    head_key_hook_name: str\n    head_value_hook_name: str\n    head_query_hook_name: str\n    # head_out_hook_name: str\n    attn_in_hook_name: str\n    attn_in_hook_name: str\n    attn_out_hook_name: str\n    attn_o_proj_input_hook_name: str\n    attn_matrix_hook_name: str\n    mlp_out_hook_name: str\n\n    attn_out_proj_weight: str\n    attn_out_proj_bias: str\n    embed_tokens: str\n    unembed_matrix: str\n    last_layernorm: str\n\n    num_hidden_layers: int\n    num_attention_heads: int\n    hidden_size: int\n    num_key_value_heads: int\n    num_key_value_groups: int\n    head_dim: int\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>This class is a factory to load the model and the processor. It supports the following models:</p> Supported Models <p>The following models are supported by this factory:</p> <ul> <li>Chameleon-7b: A 7-billion parameter model for general-purpose tasks.</li> <li>Chameleon-30b: A larger version of the Chameleon series with 30 billion parameters.</li> <li>llava-hf/llava-v1.6-mistral-7b-hf: A 7-billion parameter model for multimodal tasks.</li> <li>Pixtral-12b: Optimized for image-to-text tasks.</li> <li>Emu3-Chat: Fine-tuned for conversational AI.</li> <li>Emu3-Gen: Specialized in text generation tasks.</li> <li>Emu3-Stage1: Pretrained for multi-stage training pipelines.</li> <li>hf-internal-testing: A tiny model for internal testing purposes.</li> </ul> Adding a New Model <p>To add a new model: 1. Implement its logic in the <code>load_model</code> method. 2. Ensure it is correctly initialized and validated.</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>class ModelFactory:\n    r\"\"\"\n    This class is a factory to load the model and the processor. It supports the following models:\n\n    Supported Models:\n        The following models are supported by this factory:\n\n        - **Chameleon-7b**: A 7-billion parameter model for general-purpose tasks.\n        - **Chameleon-30b**: A larger version of the Chameleon series with 30 billion parameters.\n        - **llava-hf/llava-v1.6-mistral-7b-hf**: A 7-billion parameter model for multimodal tasks.\n        - **Pixtral-12b**: Optimized for image-to-text tasks.\n        - **Emu3-Chat**: Fine-tuned for conversational AI.\n        - **Emu3-Gen**: Specialized in text generation tasks.\n        - **Emu3-Stage1**: Pretrained for multi-stage training pipelines.\n        - **hf-internal-testing**: A tiny model for internal testing purposes.\n\n    Adding a New Model:\n        To add a new model:\n        1. Implement its logic in the `load_model` method.\n        2. Ensure it is correctly initialized and validated.\n    \"\"\"\n\n    @staticmethod\n    def load_model(\n        model_name: str,\n        attn_implementation: str,\n        torch_dtype: torch.dtype,\n        device_map: str,\n    ):\n        r\"\"\"\n        Load the model and its configuration based on the model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            attn_implementation (str): Attention implementation type. (eager, flash-attn, sdp)\n            torch_dtype (torch.dtype): Data type of the model.\n            device_map (str): Device map for the model.\n\n        Returns:\n            model (HuggingFaceModel): Model instance.\n            model_config (ModelConfig): Model configuration.\n        \"\"\"\n        if attn_implementation != \"eager\":\n            LambdaLogger.log(\n                \"Using an attention type different from eager or custom eager could have unexpected behavior in some experiments!\",\n                \"WARNING\",\n            )\n\n        language_model = None\n        if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n            model = ChameleonForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"model.layers[{}].input\",\n                residual_stream_hook_name=\"model.layers[{}].output\",\n                intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n                mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n                attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n                attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"model.embed_tokens.input\",\n                unembed_matrix=\"lm_head.weight\",\n                last_layernorm=\"model.norm\",\n                num_hidden_layers=model.config.num_hidden_layers,\n                num_attention_heads=model.config.num_attention_heads,\n                hidden_size=model.config.hidden_size,\n                num_key_value_heads=model.config.num_key_value_heads,\n                num_key_value_groups=model.config.num_attention_heads\n                // model.config.num_key_value_heads,\n                head_dim=model.config.hidden_size // model.config.num_attention_heads,\n            )\n\n        elif model_name in [\n            \"mistral-community/pixtral-12b\",\n            \"llava-hf/llava-v1.6-mistral-7b-hf\",\n        ]:\n            if model_name == \"mistral-community/pixtral-12b\":\n                model = LlavaForConditionalGeneration.from_pretrained(\n                    model_name,\n                    torch_dtype=torch_dtype,\n                    device_map=device_map,\n                    attn_implementation=attn_implementation,\n                )\n                model_config = ModelConfig(\n                    residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                    residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                    intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                    residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                    head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                    head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                    attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                    attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                    attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                    mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                    attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                    attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                    embed_tokens=\"language_model.model.embed_tokens.input\",\n                    unembed_matrix=\"language_model.lm_head.weight\",\n                    last_layernorm=\"language_model.model.norm\",\n                    num_hidden_layers=model.language_model.config.num_hidden_layers,\n                    num_attention_heads=model.language_model.config.num_attention_heads,\n                    hidden_size=model.language_model.config.hidden_size,\n                    num_key_value_heads=model.language_model.config.num_key_value_heads,\n                    num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                    head_dim=model.language_model.config.head_dim\n                )\n            elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n                model = LlavaNextForConditionalGeneration.from_pretrained(\n                    model_name,\n                    torch_dtype=torch_dtype,\n                    device_map=device_map,\n                    attn_implementation=attn_implementation,\n                )\n                model_config = ModelConfig(\n                    residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                    residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                    intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                    residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                    head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                    head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                    attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                    attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                    attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                    attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                    mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                    attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                    attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                    embed_tokens=\"language_model.model.embed_tokens.input\",\n                    unembed_matrix=\"language_model.lm_head.weight\",\n                    last_layernorm=\"language_model.model.norm\",\n                    num_hidden_layers=model.language_model.config.num_hidden_layers,\n                    num_attention_heads=model.language_model.config.num_attention_heads,\n                    hidden_size=model.language_model.config.hidden_size,\n                    num_key_value_heads=model.language_model.config.num_key_value_heads,\n                    num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                    head_dim=model.language_model.config.head_dim\n                )\n            else:\n                raise ValueError(\"Unsupported model_name\")\n            language_model = model.language_model\n\n        elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n            raise NotImplementedError(\"Emu3 model not implemented yet\")\n\n        elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n            model = LlamaForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"model.layers[{}].input\",\n                residual_stream_hook_name=\"model.layers[{}].output\",\n                intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n                head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n                head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n                head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n                attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n                mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n                attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"model.embed_tokens.input\",\n                unembed_matrix=\"lm_head.weight\",\n                last_layernorm=\"model.norm\",\n                num_hidden_layers=model.config.num_hidden_layers,\n                num_attention_heads=model.config.num_attention_heads,\n                hidden_size=model.config.hidden_size,\n                num_key_value_heads=model.config.num_key_value_heads,\n                num_key_value_groups=model.config.num_attention_heads\n                // model.config.num_key_value_heads,\n                head_dim=model.config.hidden_size // model.config.num_attention_heads,\n            )\n\n        elif model_name in [\"CohereForAI/aya-101\"]:\n            model = T5ForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            language_model = None\n            model_config = ModelFactory._create_model_config(\n                model.config, prefix=\"encoder.\"\n            )\n\n        else:\n            raise ValueError(\"Unsupported model_name\")\n        return model, language_model, model_config\n\n    @staticmethod\n    def _create_model_config(**kwargs):\n        raise NotImplementedError(\"This method should be implemented in the if\")\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.ModelFactory.load_model","title":"<code>load_model(model_name, attn_implementation, torch_dtype, device_map)</code>  <code>staticmethod</code>","text":"<p>Load the model and its configuration based on the model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>attn_implementation</code> <code>str</code> <p>Attention implementation type. (eager, flash-attn, sdp)</p> required <code>torch_dtype</code> <code>dtype</code> <p>Data type of the model.</p> required <code>device_map</code> <code>str</code> <p>Device map for the model.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>HuggingFaceModel</code> <p>Model instance.</p> <code>model_config</code> <code>ModelConfig</code> <p>Model configuration.</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>@staticmethod\ndef load_model(\n    model_name: str,\n    attn_implementation: str,\n    torch_dtype: torch.dtype,\n    device_map: str,\n):\n    r\"\"\"\n    Load the model and its configuration based on the model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        attn_implementation (str): Attention implementation type. (eager, flash-attn, sdp)\n        torch_dtype (torch.dtype): Data type of the model.\n        device_map (str): Device map for the model.\n\n    Returns:\n        model (HuggingFaceModel): Model instance.\n        model_config (ModelConfig): Model configuration.\n    \"\"\"\n    if attn_implementation != \"eager\":\n        LambdaLogger.log(\n            \"Using an attention type different from eager or custom eager could have unexpected behavior in some experiments!\",\n            \"WARNING\",\n        )\n\n    language_model = None\n    if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n        model = ChameleonForConditionalGeneration.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        model_config = ModelConfig(\n            residual_stream_input_hook_name=\"model.layers[{}].input\",\n            residual_stream_hook_name=\"model.layers[{}].output\",\n            intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n            residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n            head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n            head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n            head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n            attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n            attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n            attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n            mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n            attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n            attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n            attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n            embed_tokens=\"model.embed_tokens.input\",\n            unembed_matrix=\"lm_head.weight\",\n            last_layernorm=\"model.norm\",\n            num_hidden_layers=model.config.num_hidden_layers,\n            num_attention_heads=model.config.num_attention_heads,\n            hidden_size=model.config.hidden_size,\n            num_key_value_heads=model.config.num_key_value_heads,\n            num_key_value_groups=model.config.num_attention_heads\n            // model.config.num_key_value_heads,\n            head_dim=model.config.hidden_size // model.config.num_attention_heads,\n        )\n\n    elif model_name in [\n        \"mistral-community/pixtral-12b\",\n        \"llava-hf/llava-v1.6-mistral-7b-hf\",\n    ]:\n        if model_name == \"mistral-community/pixtral-12b\":\n            model = LlavaForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"language_model.model.embed_tokens.input\",\n                unembed_matrix=\"language_model.lm_head.weight\",\n                last_layernorm=\"language_model.model.norm\",\n                num_hidden_layers=model.language_model.config.num_hidden_layers,\n                num_attention_heads=model.language_model.config.num_attention_heads,\n                hidden_size=model.language_model.config.hidden_size,\n                num_key_value_heads=model.language_model.config.num_key_value_heads,\n                num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                head_dim=model.language_model.config.head_dim\n            )\n        elif model_name == \"llava-hf/llava-v1.6-mistral-7b-hf\":\n            model = LlavaNextForConditionalGeneration.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n                attn_implementation=attn_implementation,\n            )\n            model_config = ModelConfig(\n                residual_stream_input_hook_name=\"language_model.model.layers[{}].input\",\n                residual_stream_hook_name=\"language_model.model.layers[{}].output\",\n                intermediate_stream_hook_name=\"language_model.model.layers[{}].post_attention_layernorm.output\",\n                residual_stream_input_post_layernorm_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                head_key_hook_name=\"language_model.model.layers[{}].self_attn.k_proj.output\",\n                head_value_hook_name=\"language_model.model.layers[{}].self_attn.v_proj.output\",\n                head_query_hook_name=\"language_model.model.layers[{}].self_attn.q_proj.output\",\n                attn_out_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.output\",\n                attn_o_proj_input_hook_name=\"language_model.model.layers[{}].self_attn.o_proj.input\",\n                attn_in_hook_name=\"language_model.model.layers[{}].self_attn.input\",\n                attn_matrix_hook_name=\"language_model.model.layers[{}].self_attn.attention_matrix_hook.output\",\n                mlp_out_hook_name=\"language_model.model.layers[{}].mlp.down_proj.output\",\n                attn_out_proj_weight=\"language_model.model.layers[{}].self_attn.o_proj.weight\",\n                attn_out_proj_bias=\"language_model.model.layers[{}].self_attn.o_proj.bias\",\n                embed_tokens=\"language_model.model.embed_tokens.input\",\n                unembed_matrix=\"language_model.lm_head.weight\",\n                last_layernorm=\"language_model.model.norm\",\n                num_hidden_layers=model.language_model.config.num_hidden_layers,\n                num_attention_heads=model.language_model.config.num_attention_heads,\n                hidden_size=model.language_model.config.hidden_size,\n                num_key_value_heads=model.language_model.config.num_key_value_heads,\n                num_key_value_groups=model.language_model.config.num_attention_heads // model.language_model.config.num_key_value_heads,\n                head_dim=model.language_model.config.head_dim\n            )\n        else:\n            raise ValueError(\"Unsupported model_name\")\n        language_model = model.language_model\n\n    elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n        raise NotImplementedError(\"Emu3 model not implemented yet\")\n\n    elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n        model = LlamaForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        model_config = ModelConfig(\n            residual_stream_input_hook_name=\"model.layers[{}].input\",\n            residual_stream_hook_name=\"model.layers[{}].output\",\n            intermediate_stream_hook_name=\"model.layers[{}].post_attention_layernorm.output\",\n            residual_stream_input_post_layernorm_hook_name=\"model.layers[{}].self_attn.input\",\n            head_query_hook_name=\"model.layers[{}].self_attn.q_proj.output\",\n            head_value_hook_name=\"model.layers[{}].self_attn.v_proj.output\",\n            head_key_hook_name=\"model.layers[{}].self_attn.k_proj.output\",\n            attn_out_hook_name=\"model.layers[{}].self_attn.o_proj.output\",\n            attn_o_proj_input_hook_name=\"model.layers[{}].self_attn.o_proj.input\",\n            attn_in_hook_name=\"model.layers[{}].self_attn.input\",\n            attn_matrix_hook_name=\"model.layers[{}].self_attn.attention_matrix_hook.output\",\n            mlp_out_hook_name=\"model.layers[{}].mlp.down_proj.output\",\n            attn_out_proj_weight=\"model.layers[{}].self_attn.o_proj.weight\",\n            attn_out_proj_bias=\"model.layers[{}].self_attn.o_proj.bias\",\n            embed_tokens=\"model.embed_tokens.input\",\n            unembed_matrix=\"lm_head.weight\",\n            last_layernorm=\"model.norm\",\n            num_hidden_layers=model.config.num_hidden_layers,\n            num_attention_heads=model.config.num_attention_heads,\n            hidden_size=model.config.hidden_size,\n            num_key_value_heads=model.config.num_key_value_heads,\n            num_key_value_groups=model.config.num_attention_heads\n            // model.config.num_key_value_heads,\n            head_dim=model.config.hidden_size // model.config.num_attention_heads,\n        )\n\n    elif model_name in [\"CohereForAI/aya-101\"]:\n        model = T5ForConditionalGeneration.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            attn_implementation=attn_implementation,\n        )\n        language_model = None\n        model_config = ModelFactory._create_model_config(\n            model.config, prefix=\"encoder.\"\n        )\n\n    else:\n        raise ValueError(\"Unsupported model_name\")\n    return model, language_model, model_config\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.TokenizerFactory","title":"<code>TokenizerFactory</code>","text":"<p>This class return the right tokenizer for the model. If the model is multimodal return is_a_process == True</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>class TokenizerFactory:\n    r\"\"\"\n    This class return the right tokenizer for the model. If the model is multimodal return is_a_process == True\n    \"\"\"\n\n    @staticmethod\n    def load_tokenizer(model_name: str, torch_dtype: torch.dtype, device_map: str):\n        r\"\"\"\n        Load the tokenizer based on the model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            torch_dtype (torch.dtype): Data type of the model.\n            device_map (str): Device map for the model.\n\n        Returns:\n            processor (Tokenizer): Processor instance.\n            is_a_processor (bool): True if the model is multimodal, False otherwise.\n        \"\"\"\n        if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n            processor = ChameleonProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"meta-llama/Llama-3.2-1B\", \"meta-llama/Llama-3.2-3B\"]:\n            processor = LlamaTokenizerFast.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = False\n        elif model_name in [\"mistral-community/pixtral-12b\"]:\n            processor = PixtralProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"llava-hf/llava-v1.6-mistral-7b-hf\"]:\n            processor = LlavaNextProcessor.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = True\n        elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n            raise NotImplementedError(\"Emu3 model not implemented yet\")\n        elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n            processor = LlamaTokenizer.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = False\n        elif model_name in [\"CohereForAI/aya-101\"]:\n            processor = T5TokenizerFast.from_pretrained(\n                model_name,\n                torch_dtype=torch_dtype,\n                device_map=device_map,\n            )\n            is_a_processor = False\n\n        else:\n            raise ValueError(\"Unsupported model_name\")\n\n        return processor, is_a_processor\n</code></pre>"},{"location":"api/interpretability/models/#easyroutine.interpretability.models.TokenizerFactory.load_tokenizer","title":"<code>load_tokenizer(model_name, torch_dtype, device_map)</code>  <code>staticmethod</code>","text":"<p>Load the tokenizer based on the model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>torch_dtype</code> <code>dtype</code> <p>Data type of the model.</p> required <code>device_map</code> <code>str</code> <p>Device map for the model.</p> required <p>Returns:</p> Name Type Description <code>processor</code> <code>Tokenizer</code> <p>Processor instance.</p> <code>is_a_processor</code> <code>bool</code> <p>True if the model is multimodal, False otherwise.</p> Source code in <code>easyroutine/interpretability/models.py</code> <pre><code>@staticmethod\ndef load_tokenizer(model_name: str, torch_dtype: torch.dtype, device_map: str):\n    r\"\"\"\n    Load the tokenizer based on the model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        torch_dtype (torch.dtype): Data type of the model.\n        device_map (str): Device map for the model.\n\n    Returns:\n        processor (Tokenizer): Processor instance.\n        is_a_processor (bool): True if the model is multimodal, False otherwise.\n    \"\"\"\n    if model_name in [\"facebook/chameleon-7b\", \"facebook/chameleon-30b\"]:\n        processor = ChameleonProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"meta-llama/Llama-3.2-1B\", \"meta-llama/Llama-3.2-3B\"]:\n        processor = LlamaTokenizerFast.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = False\n    elif model_name in [\"mistral-community/pixtral-12b\"]:\n        processor = PixtralProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"llava-hf/llava-v1.6-mistral-7b-hf\"]:\n        processor = LlavaNextProcessor.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = True\n    elif model_name in [\"Emu3-Chat\", \"Emu3-Gen\", \"Emu3-Stage1\"]:\n        raise NotImplementedError(\"Emu3 model not implemented yet\")\n    elif model_name in [\"hf-internal-testing/tiny-random-LlamaForCausalLM\"]:\n        processor = LlamaTokenizer.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = False\n    elif model_name in [\"CohereForAI/aya-101\"]:\n        processor = T5TokenizerFast.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n        )\n        is_a_processor = False\n\n    else:\n        raise ValueError(\"Unsupported model_name\")\n\n    return processor, is_a_processor\n</code></pre>"},{"location":"api/interpretability/module_wrapper/","title":"Module Wrapper","text":""},{"location":"api/interpretability/module_wrapper/#introduction","title":"Introduction","text":"<p>Module Wrapper is the submodule that is responsible for managing the module wrappers. The module wrappers are essential to add custom hook where in the original transfomer codebase the hook is not available. For example, the <code>transformer</code> module does not have a hook to get the attention matrix of a head. The module wrapper is used to add this hook. The <code>module_wrapper</code>  submodel is composed of the following files:     - <code>manager.py</code>: The manager file is responsible for managing the module wrappers. It is the standard interface to add the wrap around models.     - <code>base.py</code>: The base file is the base class for the module wrapper. Implement a base form of a Wrapper class.     - <code>model_name_attention.py</code>: The model name attention file is the module wrapper for the attention matrix of a single model. When add a new model, add a new file with the name <code>model_name_attention.py</code> and implement the <code>ModelNameAttention</code> class. It is basically a copy of the forward pass of the attention module with the addition of the hook to get the attention matrix. </p>"},{"location":"api/interpretability/module_wrapper/#manager-wrappers-and-abstract-base-class","title":"Manager Wrappers and Abstract Base Class","text":""},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory","title":"<code>AttentionWrapperFactory</code>","text":"<p>Maps a given model name to the correct attention wrapper class.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>class AttentionWrapperFactory:\n    \"\"\"\n    Maps a given model name to the correct attention wrapper class.\n    \"\"\"\n\n    AVAILABLE_MODULE_WRAPPERS:dict = {\n                                    ChameleonAttentionWrapper.original_name(): ChameleonAttentionWrapper, \n                                    LlamaAttentionWrapper.original_name(): LlamaAttentionWrapper, \n                                    T5AttentionWrapper.original_name(): T5AttentionWrapper, \n                                    MistralAttentionWrapper.original_name(): MistralAttentionWrapper \n    }\n\n    # MODEL_NAME_TO_WRAPPER = {\n    #     \"facebook/chameleon-7b\": ChameleonAttentionWrapper,\n    #     \"facebook/chameleon-30b\": ChameleonAttentionWrapper,\n    #     \"mistral-community/pixtral-12b\": LlamaAttentionWrapper,\n    #     \"llava-hf/llava-v1.6-mistral-7b-hf\": LlamaAttentionWrapper,\n    #     \"hf-internal-testing/tiny-random-LlamaForCausalLM\": LlamaAttentionWrapper,\n    #     \"ChoereForAI/aya-101\": T5AttentionWrapper,\n    # }\n\n    @staticmethod\n    def get_wrapper_class(\n        model: nn.Module,\n    ) -&gt; Union[\n        Type[ChameleonAttentionWrapper],\n        Type[LlamaAttentionWrapper],\n        Type[T5AttentionWrapper],\n        Type[MistralAttentionWrapper],\n    ]:\n        \"\"\"\n        Returns the attention wrapper class for the specified model name.\n        Raises a ValueError if the model is not supported.\n        \"\"\"\n        all_modules = find_all_modules(model, return_only_names=True)\n\n        for candidate_name, candidate_wrappers in AttentionWrapperFactory.AVAILABLE_MODULE_WRAPPERS.items():\n            if candidate_name in all_modules:\n                LambdaLogger().info(f\"Found a wrapper for {candidate_name}\")\n                return candidate_wrappers\n\n        LambdaLogger().warning(f\"Do not have any wrapper for {model}\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory.get_wrapper_class","title":"<code>get_wrapper_class(model)</code>  <code>staticmethod</code>","text":"<p>Returns the attention wrapper class for the specified model name. Raises a ValueError if the model is not supported.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>@staticmethod\ndef get_wrapper_class(\n    model: nn.Module,\n) -&gt; Union[\n    Type[ChameleonAttentionWrapper],\n    Type[LlamaAttentionWrapper],\n    Type[T5AttentionWrapper],\n    Type[MistralAttentionWrapper],\n]:\n    \"\"\"\n    Returns the attention wrapper class for the specified model name.\n    Raises a ValueError if the model is not supported.\n    \"\"\"\n    all_modules = find_all_modules(model, return_only_names=True)\n\n    for candidate_name, candidate_wrappers in AttentionWrapperFactory.AVAILABLE_MODULE_WRAPPERS.items():\n        if candidate_name in all_modules:\n            LambdaLogger().info(f\"Found a wrapper for {candidate_name}\")\n            return candidate_wrappers\n\n    LambdaLogger().warning(f\"Do not have any wrapper for {model}\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager","title":"<code>ModuleWrapperManager</code>","text":"<p>Handles the logic of replacing an original attention class within a given model with a custom attention wrapper, based on user-specified model_name. Also allows restoring the original modules if needed, using a single recursive function.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>class ModuleWrapperManager:\n    \"\"\"\n    Handles the logic of replacing an original attention class within a given model\n    with a custom attention wrapper, based on user-specified model_name.\n    Also allows restoring the original modules if needed, using a single\n    recursive function.\n    \"\"\"\n\n    def __init__(self, model: nn.Module, log_level: str = \"INFO\"):\n        \"\"\"\n        Initializes the manager with a given model name.\n        \"\"\"\n        self.logger = Logger(logname=\"ModuleWrapperManager\", level=log_level)\n\n        # Fetch the appropriate wrapper class for the given model name\n        self.attention_wrapper_class = AttentionWrapperFactory.get_wrapper_class(model) # TODO: extend to support multiple module type for model\n        # The original attention class name is fetched via a class method or attribute in the wrapper\n        self.target_module_name = self.attention_wrapper_class.original_name() # TODO: extend to support multiple module type for model\n\n        # Dictionary to store submodule_path -&gt; original attention module\n        self.original_modules = {}\n\n    def __contains__(self, module_name:str):\n        return module_name == self.target_module_name # TODO: extend to support multiple module type for model\n\n    def substitute_attention_module(self, model: nn.Module) -&gt; None:\n        \"\"\"\n        Public method that performs the substitution of attention modules in the model.\n        Logs each replacement. This will replace *all* modules whose class name\n        matches `self.target_module_name`.\n        \"\"\"\n        self._traverse_and_modify(model, parent_path=\"\", mode=\"substitute\")\n\n    def restore_original_attention_module(self, model: nn.Module) -&gt; None:\n        \"\"\"\n        Public method that restores the original attention modules in the model.\n        Logs each restoration.\n        \"\"\"\n        self._traverse_and_modify(model, parent_path=\"\", mode=\"restore\")\n\n    def _traverse_and_modify(self, module: nn.Module, parent_path: str, mode: str) -&gt; None:\n        \"\"\"\n        Recursively traverses `module` and either substitutes or restores each matching\n        submodule, depending on `mode`.\n\n        - mode=\"substitute\": Replaces the original module (with class name == self.target_module_name)\n                            with the wrapper, storing the original in self.original_modules.\n        - mode=\"restore\": Replaces the wrapper submodule (class name == self.attention_wrapper_class.__name__)\n                        with the original module from self.original_modules.\n\n        Args:\n            module (nn.Module): The current module to inspect.\n            parent_path (str): A string that tracks the 'path' of this submodule in the overall model hierarchy.\n            mode (str): Either \"substitute\" or \"restore\".\n        \"\"\"\n        for name, child in list(module.named_children()):\n            # Identify the submodule path (e.g. \"encoder.layer.0.attention\")\n            submodule_path = f\"{parent_path}.{name}\" if parent_path else name\n\n            if mode == \"substitute\":\n                # Look for the original module class name\n                if child.__class__.__name__ == self.target_module_name:\n                    # Store the original\n                    self.original_modules[submodule_path] = child\n                    # Wrap it\n                    wrapped_module = self.attention_wrapper_class(child)\n                    setattr(module, name, wrapped_module)\n\n                    self.logger.info(\n                        f\"Substituted '{submodule_path}' with wrapper for {self.target_module_name}.\"\n                    )\n                else:\n                    # Recurse\n                    self._traverse_and_modify(child, submodule_path, mode=\"substitute\")\n\n            elif mode == \"restore\":\n                # Look for the wrapper class name\n                if child.__class__.__name__ == self.attention_wrapper_class.__name__:\n                    if submodule_path in self.original_modules:\n                        original_module = self.original_modules[submodule_path]\n                        setattr(module, name, original_module)\n                        self.logger.info(\n                            f\"Restored '{submodule_path}' to original {self.target_module_name}.\"\n                        )\n                    else:\n                        self.logger.warning(\n                            f\"Found a wrapped submodule '{submodule_path}' but no original stored. Skipping.\"\n                        )\n                else:\n                    # Recurse\n                    self._traverse_and_modify(child, submodule_path, mode=\"restore\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.__init__","title":"<code>__init__(model, log_level='INFO')</code>","text":"<p>Initializes the manager with a given model name.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def __init__(self, model: nn.Module, log_level: str = \"INFO\"):\n    \"\"\"\n    Initializes the manager with a given model name.\n    \"\"\"\n    self.logger = Logger(logname=\"ModuleWrapperManager\", level=log_level)\n\n    # Fetch the appropriate wrapper class for the given model name\n    self.attention_wrapper_class = AttentionWrapperFactory.get_wrapper_class(model) # TODO: extend to support multiple module type for model\n    # The original attention class name is fetched via a class method or attribute in the wrapper\n    self.target_module_name = self.attention_wrapper_class.original_name() # TODO: extend to support multiple module type for model\n\n    # Dictionary to store submodule_path -&gt; original attention module\n    self.original_modules = {}\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.restore_original_attention_module","title":"<code>restore_original_attention_module(model)</code>","text":"<p>Public method that restores the original attention modules in the model. Logs each restoration.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def restore_original_attention_module(self, model: nn.Module) -&gt; None:\n    \"\"\"\n    Public method that restores the original attention modules in the model.\n    Logs each restoration.\n    \"\"\"\n    self._traverse_and_modify(model, parent_path=\"\", mode=\"restore\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.substitute_attention_module","title":"<code>substitute_attention_module(model)</code>","text":"<p>Public method that performs the substitution of attention modules in the model. Logs each replacement. This will replace all modules whose class name matches <code>self.target_module_name</code>.</p> Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code> <pre><code>def substitute_attention_module(self, model: nn.Module) -&gt; None:\n    \"\"\"\n    Public method that performs the substitution of attention modules in the model.\n    Logs each replacement. This will replace *all* modules whose class name\n    matches `self.target_module_name`.\n    \"\"\"\n    self._traverse_and_modify(model, parent_path=\"\", mode=\"substitute\")\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.AttentionMatrixHookModule","title":"<code>AttentionMatrixHookModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>Computation of the attention matrix. Note: it has been added just for adding custom hooks.</p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>class AttentionMatrixHookModule(nn.Module):\n    \"\"\"Computation of the attention matrix. *Note*: it has been added just for adding custom hooks.\"\"\"\n\n    def forward(\n            self,\n            attention_matrix: torch.Tensor,\n    ):\n        return attention_matrix\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper","title":"<code>BaseAttentionWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A base class for wrapping an original attention module.</p> Provides <p><code>_orig_module</code> to store the real (unwrapped) attention. A robust <code>__getattr__</code> that checks:     1) self.dict     2) self._modules     3) the base class     4) fallback to <code>_orig_module</code></p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>class BaseAttentionWrapper(nn.Module):\n    \"\"\"\n    A base class for wrapping an original attention module.\n\n    Provides:\n        `_orig_module` to store the real (unwrapped) attention.\n        A robust `__getattr__` that checks:\n            1) self.__dict__\n            2) self._modules\n            3) the base class\n            4) fallback to `_orig_module`\n    \"\"\"\n\n    def __init__(self, original_module: nn.Module):\n        super().__init__()\n        # store the original module in a private attribute\n        object.__setattr__(self, \"_orig_module\", original_module)\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        If name is not in this wrapper, fall back to the original module.\n        Also checks `self._modules` for submodules, because PyTorch\n        automatically places them there.\n        \"\"\"\n        # 1) get this wrapper's __dict__\n        wrapper_dict = object.__getattribute__(self, \"__dict__\")\n\n        # 2) if name is in our own instance dictionary, return it\n        if name in wrapper_dict:\n            return wrapper_dict[name]\n\n        # 3) if name is in our submodules, return it\n        modules_dict = wrapper_dict[\"_modules\"]\n        if name in modules_dict:\n            return modules_dict[name]\n\n        # 4) check if name is in our class (methods, etc.)\n        cls = object.__getattribute__(self, \"__class__\")\n        if hasattr(cls, name):\n            return getattr(cls, name)\n\n        # 5) fallback to _orig_module\n        orig = wrapper_dict[\"_orig_module\"]\n        return getattr(orig, name)\n\n    @staticmethod\n    def original_name() -&gt; str:\n        \"\"\"\n        By default, you might override this in each derived class if you want\n        your manager code to know which original class name this wrapper replaces.\n        \"\"\"\n        return \"BaseAttention\"\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>If name is not in this wrapper, fall back to the original module. Also checks <code>self._modules</code> for submodules, because PyTorch automatically places them there.</p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>def __getattr__(self, name: str):\n    \"\"\"\n    If name is not in this wrapper, fall back to the original module.\n    Also checks `self._modules` for submodules, because PyTorch\n    automatically places them there.\n    \"\"\"\n    # 1) get this wrapper's __dict__\n    wrapper_dict = object.__getattribute__(self, \"__dict__\")\n\n    # 2) if name is in our own instance dictionary, return it\n    if name in wrapper_dict:\n        return wrapper_dict[name]\n\n    # 3) if name is in our submodules, return it\n    modules_dict = wrapper_dict[\"_modules\"]\n    if name in modules_dict:\n        return modules_dict[name]\n\n    # 4) check if name is in our class (methods, etc.)\n    cls = object.__getattribute__(self, \"__class__\")\n    if hasattr(cls, name):\n        return getattr(cls, name)\n\n    # 5) fallback to _orig_module\n    orig = wrapper_dict[\"_orig_module\"]\n    return getattr(orig, name)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.original_name","title":"<code>original_name()</code>  <code>staticmethod</code>","text":"<p>By default, you might override this in each derived class if you want your manager code to know which original class name this wrapper replaces.</p> Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code> <pre><code>@staticmethod\ndef original_name() -&gt; str:\n    \"\"\"\n    By default, you might override this in each derived class if you want\n    your manager code to know which original class name this wrapper replaces.\n    \"\"\"\n    return \"BaseAttention\"\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#specific-module-wrappers","title":"Specific Module Wrappers","text":""},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper","title":"<code>LlamaAttentionWrapper</code>","text":"<p>               Bases: <code>BaseAttentionWrapper</code></p> <p>A wrapper around the original LlamaAttention. It has: - The same named attributes (q_proj, k_proj, etc.), which are references     to the original module's submodules/parameters. - A private reference (<code>_orig_attn</code>) to the entire original attention,     for falling back if something isn't found on the wrapper itself. - An additional <code>attention_matrix_hook</code> for intercepting attention.</p> Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code> <pre><code>class LlamaAttentionWrapper(BaseAttentionWrapper):\n    \"\"\"\n    A wrapper around the original LlamaAttention. It has:\n    - The same named attributes (q_proj, k_proj, etc.), which are references\n        to the original module's submodules/parameters.\n    - A private reference (`_orig_attn`) to the entire original attention,\n        for falling back if something isn't found on the wrapper itself.\n    - An additional `attention_matrix_hook` for intercepting attention.\n    \"\"\"\n\n    @staticmethod\n    def original_name():\n        return \"LlamaAttention\"\n\n    def __init__(self, original_attention: nn.Module):\n        \"\"\"\n        Store references to all relevant submodules so the wrapper\n        \"feels\" the same. Also store a reference to the original module\n        in a private attribute for fallback.\n        \"\"\"\n        super().__init__(original_attention)\n\n        # This is the private reference to the entire original attention.\n        # We'll fallback to it for any attribute we haven't explicitly set.\n        object.__setattr__(self, \"_orig_attn\", original_attention)\n\n        # Now replicate the original attention's submodules as attributes of *this* wrapper.\n        # These are direct references, not new modules:\n        self.q_proj = original_attention.q_proj\n        self.k_proj = original_attention.k_proj\n        self.v_proj = original_attention.v_proj\n        self.o_proj = original_attention.o_proj\n\n        # Copy over any scalar attributes you need\n        # self.num_heads = original_attention.num_heads\n        # self.num_key_value_heads = original_attention.num_key_value_heads\n        # self.num_key_value_groups = original_attention.num_key_value_groups\n        self.head_dim = original_attention.head_dim\n        # self.hidden_size = original_attention.hidden_size\n        self.attention_dropout = original_attention.attention_dropout\n        self.layer_idx = original_attention.layer_idx\n        self.config = original_attention.config\n\n        # Add your custom hook module\n        self.attention_matrix_hook = AttentionMatrixHookModule()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n        attention_mask: Optional[torch.Tensor],\n        past_key_value: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[FlashAttentionKwargs],\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        input_shape = hidden_states.shape[:-1]\n        hidden_shape = (*input_shape, -1, self.head_dim)\n\n        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n\n        cos, sin = position_embeddings\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(\n                key_states, value_states, self.layer_idx, cache_kwargs\n            )\n\n        # Inline eager_attention_forward logic\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n\n        if attention_mask is not None:\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = self.attention_matrix_hook(attn_weights)\n        attn_weights = nn.functional.dropout(\n            attn_weights,\n            p=0.0 if not self.training else self.attention_dropout,\n            training=self.training,\n        )\n\n        attn_output = torch.matmul(attn_weights, value_states).transpose(1, 2).contiguous()\n        # End inline\n\n        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n        return attn_output, attn_weights # type: ignore\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper.__init__","title":"<code>__init__(original_attention)</code>","text":"<p>Store references to all relevant submodules so the wrapper \"feels\" the same. Also store a reference to the original module in a private attribute for fallback.</p> Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code> <pre><code>def __init__(self, original_attention: nn.Module):\n    \"\"\"\n    Store references to all relevant submodules so the wrapper\n    \"feels\" the same. Also store a reference to the original module\n    in a private attribute for fallback.\n    \"\"\"\n    super().__init__(original_attention)\n\n    # This is the private reference to the entire original attention.\n    # We'll fallback to it for any attribute we haven't explicitly set.\n    object.__setattr__(self, \"_orig_attn\", original_attention)\n\n    # Now replicate the original attention's submodules as attributes of *this* wrapper.\n    # These are direct references, not new modules:\n    self.q_proj = original_attention.q_proj\n    self.k_proj = original_attention.k_proj\n    self.v_proj = original_attention.v_proj\n    self.o_proj = original_attention.o_proj\n\n    # Copy over any scalar attributes you need\n    # self.num_heads = original_attention.num_heads\n    # self.num_key_value_heads = original_attention.num_key_value_heads\n    # self.num_key_value_groups = original_attention.num_key_value_groups\n    self.head_dim = original_attention.head_dim\n    # self.hidden_size = original_attention.hidden_size\n    self.attention_dropout = original_attention.attention_dropout\n    self.layer_idx = original_attention.layer_idx\n    self.config = original_attention.config\n\n    # Add your custom hook module\n    self.attention_matrix_hook = AttentionMatrixHookModule()\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.llama_attention.repeat_kv","title":"<code>repeat_kv(hidden_states, n_rep)</code>","text":"<p>(batch, num_key_value_heads, seq_len, head_dim)     -&gt; (batch, num_attention_heads, seq_len, head_dim)</p> Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code> <pre><code>def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"\n    (batch, num_key_value_heads, seq_len, head_dim)\n        -&gt; (batch, num_attention_heads, seq_len, head_dim)\n    \"\"\"\n    bsz, num_kv_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(\n        bsz, num_kv_heads, n_rep, slen, head_dim\n    )\n    return hidden_states.reshape(bsz, num_kv_heads * n_rep, slen, head_dim)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.ChameleonAttentionWrapper","title":"<code>ChameleonAttentionWrapper</code>","text":"<p>               Bases: <code>BaseAttentionWrapper</code></p> <p>Attention wrapper for the Chameleon model.</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>class ChameleonAttentionWrapper(BaseAttentionWrapper):\n    \"\"\"\n    Attention wrapper for the Chameleon model.\n    \"\"\"\n\n    @staticmethod\n    def original_name():\n        return \"ChameleonAttention\"\n\n    def __init__(self, original_attention: nn.Module):\n        super().__init__(original_attention)\n\n        self.q_proj = original_attention.q_proj\n        self.k_proj = original_attention.k_proj\n        self.v_proj = original_attention.v_proj\n        self.q_norm = original_attention.q_norm\n        self.k_norm = original_attention.k_norm\n        self.o_proj = original_attention.o_proj\n        # self.softmax = original_attention.softmax\n        self.attention_dropout = original_attention.attention_dropout\n        self.training = original_attention.training\n        self.layer_idx = original_attention.layer_idx\n        self.num_heads = original_attention.num_heads\n        self.num_key_value_heads = original_attention.num_key_value_heads\n        self.num_key_value_groups = original_attention.num_key_value_groups\n        self.head_dim = original_attention.head_dim\n        self.hidden_size = original_attention.hidden_size\n        self.rotary_emb = original_attention.rotary_emb\n\n\n        self.attention_matrix_hook = AttentionMatrixHookModule()\n\n        self.original_attention = original_attention\n\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n        query_states = self.q_norm(query_states)\n\n        key_states = key_states.reshape(-1, self.num_key_value_heads, self.head_dim)\n        key_states = self.k_norm(key_states)\n\n        query_states = query_states.reshape(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.reshape(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        # attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_weights = self.attention_matrix_hook(attn_weights)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value # type: ignore\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1)</code>","text":"<p>Applies Rotary Position Embedding to the query and key tensors.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>`torch.Tensor`</code> <p>The query tensor.</p> required <code>k</code> <code>`torch.Tensor`</code> <p>The key tensor.</p> required <code>cos</code> <code>`torch.Tensor`</code> <p>The cosine part of the rotary embedding.</p> required <code>sin</code> <code>`torch.Tensor`</code> <p>The sine part of the rotary embedding.</p> required <code>position_ids</code> <code>`torch.Tensor`, *optional*</code> <p>Deprecated and unused.</p> <code>None</code> <code>unsqueeze_dim</code> <code>`int`, *optional*, defaults to 1</code> <p>The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.</p> <code>1</code> <p>Returns:     <code>tuple(torch.Tensor)</code> comprising of the query and key tensors rotated using the Rotary Position Embedding.</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>def apply_rotary_pos_emb( q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.repeat_kv","title":"<code>repeat_kv(hidden_states, n_rep)</code>","text":"<p>This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.chameleon_attention.rotate_half","title":"<code>rotate_half(x)</code>","text":"<p>Rotates half the hidden dims of the input.</p> Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code> <pre><code>def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper","title":"<code>T5AttentionWrapper</code>","text":"<p>               Bases: <code>BaseAttentionWrapper</code></p> Source code in <code>easyroutine/interpretability/module_wrappers/T5_attention.py</code> <pre><code>class T5AttentionWrapper(BaseAttentionWrapper):\n    @staticmethod\n    def original_name() -&gt; str:\n        return \"T5Attention\"\n\n    def __init__(self, original_attention: nn.Module):\n        super().__init__(original_attention)\n        self.q = original_attention.q\n        self.k = original_attention.k\n        self.v = original_attention.v\n        self.o = original_attention.o\n        self.dropout = original_attention.dropout\n        self.layer_idx = original_attention.layer_idx\n        self.n_heads = original_attention.n_heads\n        self.key_value_proj_dim = original_attention.key_value_proj_dim\n        self.inner_dim = original_attention.inner_dim\n        self.has_relative_attention_bias = (\n            original_attention.has_relative_attention_bias\n        )\n        self.pruned_heads = original_attention.pruned_heads\n        self.attention_matrix_hook = AttentionMatrixHookModule()\n        self.original_attention = original_attention\n\n    def forward(\n        self,\n        hidden_states,\n        mask=None,\n        key_value_states=None,\n        position_bias=None,\n        past_key_value=None,\n        layer_head_mask=None,\n        query_length=None,\n        use_cache=False,\n        output_attentions=False,\n        cache_position=None,\n    ):\n        \"\"\"\n        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n        \"\"\"\n        # Input is (batch_size, seq_length, dim)\n        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n        is_cross_attention = key_value_states is not None\n\n        query_states = self.q(hidden_states)\n        query_states = query_states.view(\n            batch_size, -1, self.n_heads, self.key_value_proj_dim\n        ).transpose(1, 2)\n\n        if past_key_value is not None:\n            is_updated = past_key_value.is_updated.get(self.layer_idx)\n            if is_cross_attention:\n                # after the first generated id, we can subsequently re-use all key/value_states from cache\n                curr_past_key_value = past_key_value.cross_attention_cache\n            else:\n                curr_past_key_value = past_key_value.self_attention_cache\n\n        current_states = key_value_states if is_cross_attention else hidden_states\n        if is_cross_attention and past_key_value is not None and is_updated:  # type: ignore\n            # reuse k,v, cross_attentions\n            key_states = curr_past_key_value.key_cache[self.layer_idx]  # type: ignore\n            value_states = curr_past_key_value.value_cache[self.layer_idx]  # type: ignore\n        else:\n            key_states = self.k(current_states)\n            value_states = self.v(current_states)\n            key_states = key_states.view(\n                batch_size, -1, self.n_heads, self.key_value_proj_dim\n            ).transpose(1, 2)\n            value_states = value_states.view(\n                batch_size, -1, self.n_heads, self.key_value_proj_dim\n            ).transpose(1, 2)\n\n            if past_key_value is not None:\n                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n                cache_position = cache_position if not is_cross_attention else None\n                key_states, value_states = curr_past_key_value.update(  # type: ignore\n                    key_states,\n                    value_states,\n                    self.layer_idx,\n                    {\"cache_position\": cache_position},\n                )\n                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n                if is_cross_attention:\n                    past_key_value.is_updated[self.layer_idx] = True\n\n        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd-&gt;bnqk\", query_states, key_states), compatible with onnx op&gt;9\n        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n\n        if position_bias is None:\n            key_length = key_states.shape[-2]\n            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n            real_seq_length = (\n                query_length if query_length is not None else cache_position[-1] + 1 # type: ignore\n            )  # type: ignore\n            if not self.has_relative_attention_bias:\n                position_bias = torch.zeros(\n                    (1, self.n_heads, seq_length, key_length),\n                    device=scores.device,\n                    dtype=scores.dtype,\n                )\n                if self.gradient_checkpointing and self.training:\n                    position_bias.requires_grad = True\n            else:\n                position_bias = self.compute_bias(\n                    real_seq_length,\n                    key_length,\n                    device=scores.device,\n                    cache_position=cache_position,\n                )\n                position_bias = position_bias[:, :, -seq_length:, :]\n\n            if mask is not None:\n                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                position_bias = position_bias + causal_mask\n\n        if self.pruned_heads:\n            mask = torch.ones(position_bias.shape[1])\n            mask[list(self.pruned_heads)] = 0\n            position_bias_masked = position_bias[:, mask.bool()]\n        else:\n            position_bias_masked = position_bias\n\n        scores += position_bias_masked\n\n        # (batch_size, n_heads, seq_length, key_length)\n        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n        attn_weights = self.attention_matrix_hook(attn_weights)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.dropout, training=self.training\n        )\n\n        # Mask heads if we want to\n        if layer_head_mask is not None:\n            attn_weights = attn_weights * layer_head_mask\n\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n        attn_output = self.o(attn_output)\n\n        outputs = (attn_output, past_key_value, position_bias)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n        return outputs\n</code></pre>"},{"location":"api/interpretability/module_wrapper/#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper.forward","title":"<code>forward(hidden_states, mask=None, key_value_states=None, position_bias=None, past_key_value=None, layer_head_mask=None, query_length=None, use_cache=False, output_attentions=False, cache_position=None)</code>","text":"<p>Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).</p> Source code in <code>easyroutine/interpretability/module_wrappers/T5_attention.py</code> <pre><code>def forward(\n    self,\n    hidden_states,\n    mask=None,\n    key_value_states=None,\n    position_bias=None,\n    past_key_value=None,\n    layer_head_mask=None,\n    query_length=None,\n    use_cache=False,\n    output_attentions=False,\n    cache_position=None,\n):\n    \"\"\"\n    Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n    \"\"\"\n    # Input is (batch_size, seq_length, dim)\n    # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n    batch_size, seq_length = hidden_states.shape[:2]\n\n    # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n    is_cross_attention = key_value_states is not None\n\n    query_states = self.q(hidden_states)\n    query_states = query_states.view(\n        batch_size, -1, self.n_heads, self.key_value_proj_dim\n    ).transpose(1, 2)\n\n    if past_key_value is not None:\n        is_updated = past_key_value.is_updated.get(self.layer_idx)\n        if is_cross_attention:\n            # after the first generated id, we can subsequently re-use all key/value_states from cache\n            curr_past_key_value = past_key_value.cross_attention_cache\n        else:\n            curr_past_key_value = past_key_value.self_attention_cache\n\n    current_states = key_value_states if is_cross_attention else hidden_states\n    if is_cross_attention and past_key_value is not None and is_updated:  # type: ignore\n        # reuse k,v, cross_attentions\n        key_states = curr_past_key_value.key_cache[self.layer_idx]  # type: ignore\n        value_states = curr_past_key_value.value_cache[self.layer_idx]  # type: ignore\n    else:\n        key_states = self.k(current_states)\n        value_states = self.v(current_states)\n        key_states = key_states.view(\n            batch_size, -1, self.n_heads, self.key_value_proj_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            batch_size, -1, self.n_heads, self.key_value_proj_dim\n        ).transpose(1, 2)\n\n        if past_key_value is not None:\n            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n            cache_position = cache_position if not is_cross_attention else None\n            key_states, value_states = curr_past_key_value.update(  # type: ignore\n                key_states,\n                value_states,\n                self.layer_idx,\n                {\"cache_position\": cache_position},\n            )\n            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n            if is_cross_attention:\n                past_key_value.is_updated[self.layer_idx] = True\n\n    # compute scores, equivalent of torch.einsum(\"bnqd,bnkd-&gt;bnqk\", query_states, key_states), compatible with onnx op&gt;9\n    scores = torch.matmul(query_states, key_states.transpose(3, 2))\n\n    if position_bias is None:\n        key_length = key_states.shape[-2]\n        # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n        real_seq_length = (\n            query_length if query_length is not None else cache_position[-1] + 1 # type: ignore\n        )  # type: ignore\n        if not self.has_relative_attention_bias:\n            position_bias = torch.zeros(\n                (1, self.n_heads, seq_length, key_length),\n                device=scores.device,\n                dtype=scores.dtype,\n            )\n            if self.gradient_checkpointing and self.training:\n                position_bias.requires_grad = True\n        else:\n            position_bias = self.compute_bias(\n                real_seq_length,\n                key_length,\n                device=scores.device,\n                cache_position=cache_position,\n            )\n            position_bias = position_bias[:, :, -seq_length:, :]\n\n        if mask is not None:\n            causal_mask = mask[:, :, :, : key_states.shape[-2]]\n            position_bias = position_bias + causal_mask\n\n    if self.pruned_heads:\n        mask = torch.ones(position_bias.shape[1])\n        mask[list(self.pruned_heads)] = 0\n        position_bias_masked = position_bias[:, mask.bool()]\n    else:\n        position_bias_masked = position_bias\n\n    scores += position_bias_masked\n\n    # (batch_size, n_heads, seq_length, key_length)\n    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n    attn_weights = self.attention_matrix_hook(attn_weights)\n    attn_weights = nn.functional.dropout(\n        attn_weights, p=self.dropout, training=self.training\n    )\n\n    # Mask heads if we want to\n    if layer_head_mask is not None:\n        attn_weights = attn_weights * layer_head_mask\n\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n    attn_output = self.o(attn_output)\n\n    outputs = (attn_output, past_key_value, position_bias)\n\n    if output_attentions:\n        outputs = outputs + (attn_weights,)\n    return outputs\n</code></pre>"},{"location":"api/interpretability/token_index/","title":"Token index","text":""},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex","title":"<code>TokenIndex</code>","text":"<p>TokenIndex is one of the core class of the interpretability module. It is used to find the right indexes that correspond to the tokens in the input of the model. In this way we are able to extract the right hidden states and attention weights, based on the tokens we are interested in. It support mixed modalities inputs, with both text and images.</p> Source code in <code>easyroutine/interpretability/token_index.py</code> <pre><code>class TokenIndex:\n    r\"\"\"\n    TokenIndex is one of the core class of the interpretability module.\n    It is used to find the right indexes that correspond to the tokens in the input of the model.\n    In this way we are able to extract the right hidden states and attention weights, based on the tokens we are interested in.\n    It support mixed modalities inputs, with both text and images.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        pivot_positions: Optional[List[int]] = None,\n        pivot_tokens: Optional[List[str]] = None,\n    ):\n        r\"\"\"\n        Args:\n            model_name: str (required): the name of the model\n            pivot_positions: List[int] (optional): a list of integers that represent the positions where to split the tokens.\n            pivot_tokens: List[str] (optional): a list of strings that represent the tokens where to split the tokens.\n\n\n        The pivot_positions and pivot_tokens are mutually exclusive.\n        The idea of the split is the following. Immagine to have an input string of tokens like this: [\"I\", \"love\", \"cats\", \"and\", \"dogs\". \"What\", \"about\", \"you?\"]\n        Then, i want to extract/ablate/intervene on the second sentence. I can do it by specifying the pivot_positions=[5] or pivot_tokens=[\"What\"].\n        In this way, the tokens will be split in two groups: [\"I\", \"love\", \"cats\", \"and\"] and [\"dogs\", \"What\", \"about\", \"you?\"] with names \"inputs-partition-0\" and \"inputs-partition-1\".\n        \"\"\"\n        self.model_name = model_name\n        self.pivot_tokens = pivot_tokens\n        self.pivot_positions = sorted(pivot_positions) if pivot_positions else []\n\n    def find_occurrences(self, lst: List[str], target: str) -&gt; List[int]:\n        return [i for i, x in enumerate(lst) if x == target]\n\n    def categorize_tokens(self, string_tokens: List[str]) -&gt; Dict[str, List[int]]:\n        if self.model_name not in SUPPORTED_MODELS:\n            raise ValueError(\"Unsupported model_name\")\n\n        start_image_token, special, end_image_token = SUPPORTED_MODELS[self.model_name]\n\n        image_start_tokens, image_end_tokens, image_tokens, last_line_image_tokens = (\n            [],\n            [],\n            [],\n            [],\n        )\n        text_tokens, special_tokens = [], []\n\n        in_image_sequence = False\n\n        for i, token in enumerate(string_tokens):\n            if token == start_image_token and not in_image_sequence:\n                in_image_sequence = True\n                image_start_tokens.append(i)\n            elif in_image_sequence and token == end_image_token:\n                in_image_sequence = False\n                image_end_tokens.append(i)\n                last_line_image_tokens.append(i - 1)\n            elif in_image_sequence and special and token == special:\n                special_tokens.append(i)\n            elif in_image_sequence:\n                image_tokens.append(i)\n            else:\n                text_tokens.append(i)\n\n        tokens_group, positions_group = self.group_tokens(string_tokens)\n\n        position_dict = {\n            f\"inputs-partition-{i}\": positions_group[i] for i in positions_group\n        }\n\n        return {\n            \"image_start\": image_start_tokens,\n            \"image_end\": image_end_tokens,\n            \"image\": image_tokens,\n            \"last_line_image\": last_line_image_tokens,\n            \"text\": text_tokens,\n            \"special\": special_tokens,\n            \"all\": list(range(len(string_tokens))),\n            **position_dict,\n        }\n\n    def group_tokens(\n        self, string_tokens: List[str]\n    ) -&gt; Tuple[Dict[int, List[str]], Dict[int, List[int]]]:\n        if self.pivot_tokens:\n            return self.group_tokens_by_pivot_tokens(string_tokens)\n        elif self.pivot_positions:\n            return self.group_tokens_by_positions(string_tokens)\n        else:\n            return {0: string_tokens}, {0: list(range(len(string_tokens)))}\n\n    def group_tokens_by_positions(\n        self, string_tokens: List[str]\n    ) -&gt; Tuple[Dict[int, List[str]], Dict[int, List[int]]]:\n        tokens_group, positions_group = {}, {}\n        for i, pos in enumerate(self.pivot_positions):\n            if i == 0:\n                positions_group[i] = [0, pos]\n            else:\n                positions_group[i] = [self.pivot_positions[i - 1], pos]\n        positions_group[len(self.pivot_positions)] = [\n            self.pivot_positions[-1],\n            len(string_tokens),\n        ]\n\n        # modify the positions_group to include all the indexes and not just the start and end\n        for i in range(len(positions_group)):\n            positions_group[i] = list(\n                range(positions_group[i][0], positions_group[i][1])\n            )\n\n        for i, group in positions_group.items():\n            tokens_group[i] = string_tokens[group[0] : group[1]]\n\n        return tokens_group, positions_group\n\n    def group_tokens_by_pivot_tokens(\n        self, string_tokens: List[str]\n    ) -&gt; Tuple[Dict[int, List[str]], Dict[int, List[int]]]:\n        tokens_group, positions_group = {}, {}\n        current_group = 0\n        start_pos = 0\n\n        for i, token in enumerate(string_tokens):\n            if isinstance(self.pivot_tokens, list) and token in self.pivot_tokens:\n                positions_group[current_group] = [start_pos, i]\n                tokens_group[current_group] = string_tokens[start_pos:i]\n                current_group += 1\n                start_pos = i + 1\n\n        positions_group[current_group] = [start_pos, len(string_tokens)]\n        tokens_group[current_group] = string_tokens[start_pos:]\n\n        return tokens_group, positions_group\n\n    def get_token_index(\n        self,\n        tokens: List[str],\n        string_tokens: List[str],\n        return_type: Literal[\"list\", \"dict\", \"all\"] = \"list\",\n    ) -&gt; Union[List[int], Dict, Tuple[List[int], Dict]]:\n        r\"\"\"\n        Main interface to get the indexes of the tokens in the input string tokens.\n        Args:\n            tokens: List[str] (required): a list of strings that represent the tokens we are interested in.\n            string_tokens: List[str] (required): a list of strings that represent the input tokens.\n            return_type: Literal[\"list\", \"int\", \"dict\"] (optional): the type of the return value.\n                If \"list\" it returns a list of integers, if \"int\" it returns an integer, if \"dict\" it returns a dictionary.\n\n        Returns:\n            tokens_positions: Union[List[int], int, Dict]: the indexes of the tokens in the input string tokens in the format specified by return_type.\n\n        Supported tokens:\n            - `last`: the last token of the input sequence\n            - `last-2`: the second last token of the input sequence\n            - `last-4`: the fourth last token of the input sequence\n            - `last-image`: the last token of the image sequence\n            - `end-image`: the end token of the image sequence\n            - `all-text`: all the tokens of the text sequence\n            - `all`: all the tokens of the input sequence\n            - `all-image`: all the tokens of the image sequence\n            - `special`: special list of tokens based on the model\n            - `random-text`: a random token from the text sequence\n            - `random-image`: a random token from the image sequence\n            - `random-text-n`: n random tokens from the text sequence\n            - `random-image-n`: n random tokens from the image sequence\n            - `inputs-partition-i`: the i-th group of tokens based on the pivot_positions or pivot_tokens\n            - `random-inputs-partition-i`: a random token from the i-th group of tokens based on the pivot_positions or pivot_tokens\n\n        Examples:\n            &gt;&gt;&gt; string_tokens = [\"start-image\", \"img1\", \"img2\", \"end-image\", I\", \"love\", \"cats\", \"and\", \"dogs\", \"What\", \"about\", \"you?\"]\n            &gt;&gt;&gt; tokens = [\"end-image\", \"all-text\", \"last\", \"inputs-partition-1\", \"inputs-partition-2\"]\n            &gt;&gt;&gt; TokenIndex(\"facebook/Chameleon-7b\", pivot_tokens = [\"cats\", \"dogs\"]).get_token_index(tokens, string_tokens, return_type=\"dict\")\n            {'end-image': [3], 'all-text': [4, 5, 6, 7, 8, 9, 10, 11], 'last': [-1], \"inputs-partition-1\": [7,8], \"inputs-partition-2\": [9, 10, 11]}\n        \"\"\"\n        if not all(\n            token in SUPPORTED_TOKENS\n            or token.startswith(\"inputs-partition-\")\n            or token.startswith(\"random-inputs-partition-\")\n            for token in tokens\n        ):\n            raise ValueError(\n                f\"Unsupported token type: {tokens}. Supported tokens are: {SUPPORTED_TOKENS} and inputs-partition-0, inputs-partition-1, etc or random-inputs-partition-0, random-inputs-partition-1, etc\"\n            )\n\n        # Check if pivot_positions is required but not provided\n        if self.pivot_positions is None and any(\n            token.startswith(\"inputs-partition-\")\n            or token.startswith(\"random-inputs-partition-\")\n            for token in tokens\n        ):\n            raise ValueError(\n                \"pivot_positions cannot be None when a group position token is requested\"\n            )\n\n        token_indexes = self.categorize_tokens(string_tokens)\n        tokens_positions = self.get_tokens_positions(tokens, token_indexes)\n\n        # if return_type == \"int\":\n        #     if len(tokens_positions) &gt; 1:\n        #         raise ValueError(\n        #             \"More than one token requested: return_type should be list, got int\"\n        #         )\n        #     return tokens_positions[0]\n        if return_type == \"dict\":\n            return self.get_token_dict(token_indexes)\n        if return_type == \"all\":\n            return tokens_positions, self.get_token_dict(token_indexes)\n        return tokens_positions\n\n    def get_tokens_positions(\n        self, tokens: List[str], token_indexes: Dict[str, List[int]]\n    ) -&gt; List[int]:\n        tokens_positions = []\n        position_dict = {\n            k: v for k, v in token_indexes.items() if k.startswith(\"inputs-partition-\")\n        }\n        random_position_dict = {\n            f\"random-{k}\": random.sample(v, 1) for k, v in position_dict.items()\n        }\n\n        for token in tokens:\n            if token.startswith(\"random-inputs-partition-\"):\n                group, n = self.parse_random_group_token(token)\n                random_position_dict[token] = random.sample(\n                    position_dict[f\"inputs-partition-{group}\"], int(n)\n                )\n            elif token.startswith(\"random-image\"):\n                n = token.split(\"-\")[-1]\n                random_position_dict[token] = random.sample(\n                    token_indexes[\"image\"], int(n) if n else 1\n                )\n\n        token_dict = self.get_token_dict(token_indexes, random_position_dict)\n\n        for token in tokens:\n            if token_dict[token] is not None:\n                tokens_positions.extend(token_dict[token])  # type: ignore\n\n        return tokens_positions\n\n    def parse_random_group_token(self, token: str) -&gt; Tuple[str, int]:\n        group_and_n = token.split(\"-\")[2:]\n        if len(group_and_n) &gt; 1:\n            group, n = group_and_n\n        else:\n            group = group_and_n[0]\n            n = 1\n        return group, int(n)\n\n    def get_token_dict(\n        self,\n        token_indexes: Dict[str, List[int]],\n        random_position_dict: Dict[str, List[int]] = {},\n    ) -&gt; Dict[str, Optional[List[int]]]:\n        return {\n            \"last\": [-1],\n            \"last-2\": [-2],\n            \"last-4\": [-4],\n            \"last-image\": token_indexes[\"last_line_image\"],\n            \"end-image\": token_indexes[\"image_end\"],\n            \"all-text\": token_indexes[\"text\"],\n            \"all\": token_indexes[\"all\"],\n            \"all-image\": token_indexes[\"image\"],\n            \"special\": token_indexes[\"special\"],\n            \"random-text\": None\n            if len(token_indexes[\"text\"]) == 0\n            else [random.choice(token_indexes[\"text\"])],\n            \"random-image\": None\n            if len(token_indexes[\"image\"]) == 0\n            else [random.choice(token_indexes[\"image\"])],\n            \"special-pixtral\": [1052, 1051, 1038, 991, 1037, 1047],\n            **{\n                k: v\n                for k, v in token_indexes.items()\n                if k.startswith(\"inputs-partition-\")\n            },\n            **random_position_dict,\n        }\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.__init__","title":"<code>__init__(model_name, pivot_positions=None, pivot_tokens=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>str (required): the name of the model</p> required <code>pivot_positions</code> <code>Optional[List[int]]</code> <p>List[int] (optional): a list of integers that represent the positions where to split the tokens.</p> <code>None</code> <code>pivot_tokens</code> <code>Optional[List[str]]</code> <p>List[str] (optional): a list of strings that represent the tokens where to split the tokens.</p> <code>None</code> <p>The pivot_positions and pivot_tokens are mutually exclusive. The idea of the split is the following. Immagine to have an input string of tokens like this: [\"I\", \"love\", \"cats\", \"and\", \"dogs\". \"What\", \"about\", \"you?\"] Then, i want to extract/ablate/intervene on the second sentence. I can do it by specifying the pivot_positions=[5] or pivot_tokens=[\"What\"]. In this way, the tokens will be split in two groups: [\"I\", \"love\", \"cats\", \"and\"] and [\"dogs\", \"What\", \"about\", \"you?\"] with names \"inputs-partition-0\" and \"inputs-partition-1\".</p> Source code in <code>easyroutine/interpretability/token_index.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pivot_positions: Optional[List[int]] = None,\n    pivot_tokens: Optional[List[str]] = None,\n):\n    r\"\"\"\n    Args:\n        model_name: str (required): the name of the model\n        pivot_positions: List[int] (optional): a list of integers that represent the positions where to split the tokens.\n        pivot_tokens: List[str] (optional): a list of strings that represent the tokens where to split the tokens.\n\n\n    The pivot_positions and pivot_tokens are mutually exclusive.\n    The idea of the split is the following. Immagine to have an input string of tokens like this: [\"I\", \"love\", \"cats\", \"and\", \"dogs\". \"What\", \"about\", \"you?\"]\n    Then, i want to extract/ablate/intervene on the second sentence. I can do it by specifying the pivot_positions=[5] or pivot_tokens=[\"What\"].\n    In this way, the tokens will be split in two groups: [\"I\", \"love\", \"cats\", \"and\"] and [\"dogs\", \"What\", \"about\", \"you?\"] with names \"inputs-partition-0\" and \"inputs-partition-1\".\n    \"\"\"\n    self.model_name = model_name\n    self.pivot_tokens = pivot_tokens\n    self.pivot_positions = sorted(pivot_positions) if pivot_positions else []\n</code></pre>"},{"location":"api/interpretability/token_index/#easyroutine.interpretability.token_index.TokenIndex.get_token_index","title":"<code>get_token_index(tokens, string_tokens, return_type='list')</code>","text":"<p>Main interface to get the indexes of the tokens in the input string tokens. Args:     tokens: List[str] (required): a list of strings that represent the tokens we are interested in.     string_tokens: List[str] (required): a list of strings that represent the input tokens.     return_type: Literal[\"list\", \"int\", \"dict\"] (optional): the type of the return value.         If \"list\" it returns a list of integers, if \"int\" it returns an integer, if \"dict\" it returns a dictionary.</p> <p>Returns:</p> Name Type Description <code>tokens_positions</code> <code>Union[List[int], Dict, Tuple[List[int], Dict]]</code> <p>Union[List[int], int, Dict]: the indexes of the tokens in the input string tokens in the format specified by return_type.</p> Supported tokens <ul> <li><code>last</code>: the last token of the input sequence</li> <li><code>last-2</code>: the second last token of the input sequence</li> <li><code>last-4</code>: the fourth last token of the input sequence</li> <li><code>last-image</code>: the last token of the image sequence</li> <li><code>end-image</code>: the end token of the image sequence</li> <li><code>all-text</code>: all the tokens of the text sequence</li> <li><code>all</code>: all the tokens of the input sequence</li> <li><code>all-image</code>: all the tokens of the image sequence</li> <li><code>special</code>: special list of tokens based on the model</li> <li><code>random-text</code>: a random token from the text sequence</li> <li><code>random-image</code>: a random token from the image sequence</li> <li><code>random-text-n</code>: n random tokens from the text sequence</li> <li><code>random-image-n</code>: n random tokens from the image sequence</li> <li><code>inputs-partition-i</code>: the i-th group of tokens based on the pivot_positions or pivot_tokens</li> <li><code>random-inputs-partition-i</code>: a random token from the i-th group of tokens based on the pivot_positions or pivot_tokens</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; string_tokens = [\"start-image\", \"img1\", \"img2\", \"end-image\", I\", \"love\", \"cats\", \"and\", \"dogs\", \"What\", \"about\", \"you?\"]\n&gt;&gt;&gt; tokens = [\"end-image\", \"all-text\", \"last\", \"inputs-partition-1\", \"inputs-partition-2\"]\n&gt;&gt;&gt; TokenIndex(\"facebook/Chameleon-7b\", pivot_tokens = [\"cats\", \"dogs\"]).get_token_index(tokens, string_tokens, return_type=\"dict\")\n{'end-image': [3], 'all-text': [4, 5, 6, 7, 8, 9, 10, 11], 'last': [-1], \"inputs-partition-1\": [7,8], \"inputs-partition-2\": [9, 10, 11]}\n</code></pre> Source code in <code>easyroutine/interpretability/token_index.py</code> <pre><code>def get_token_index(\n    self,\n    tokens: List[str],\n    string_tokens: List[str],\n    return_type: Literal[\"list\", \"dict\", \"all\"] = \"list\",\n) -&gt; Union[List[int], Dict, Tuple[List[int], Dict]]:\n    r\"\"\"\n    Main interface to get the indexes of the tokens in the input string tokens.\n    Args:\n        tokens: List[str] (required): a list of strings that represent the tokens we are interested in.\n        string_tokens: List[str] (required): a list of strings that represent the input tokens.\n        return_type: Literal[\"list\", \"int\", \"dict\"] (optional): the type of the return value.\n            If \"list\" it returns a list of integers, if \"int\" it returns an integer, if \"dict\" it returns a dictionary.\n\n    Returns:\n        tokens_positions: Union[List[int], int, Dict]: the indexes of the tokens in the input string tokens in the format specified by return_type.\n\n    Supported tokens:\n        - `last`: the last token of the input sequence\n        - `last-2`: the second last token of the input sequence\n        - `last-4`: the fourth last token of the input sequence\n        - `last-image`: the last token of the image sequence\n        - `end-image`: the end token of the image sequence\n        - `all-text`: all the tokens of the text sequence\n        - `all`: all the tokens of the input sequence\n        - `all-image`: all the tokens of the image sequence\n        - `special`: special list of tokens based on the model\n        - `random-text`: a random token from the text sequence\n        - `random-image`: a random token from the image sequence\n        - `random-text-n`: n random tokens from the text sequence\n        - `random-image-n`: n random tokens from the image sequence\n        - `inputs-partition-i`: the i-th group of tokens based on the pivot_positions or pivot_tokens\n        - `random-inputs-partition-i`: a random token from the i-th group of tokens based on the pivot_positions or pivot_tokens\n\n    Examples:\n        &gt;&gt;&gt; string_tokens = [\"start-image\", \"img1\", \"img2\", \"end-image\", I\", \"love\", \"cats\", \"and\", \"dogs\", \"What\", \"about\", \"you?\"]\n        &gt;&gt;&gt; tokens = [\"end-image\", \"all-text\", \"last\", \"inputs-partition-1\", \"inputs-partition-2\"]\n        &gt;&gt;&gt; TokenIndex(\"facebook/Chameleon-7b\", pivot_tokens = [\"cats\", \"dogs\"]).get_token_index(tokens, string_tokens, return_type=\"dict\")\n        {'end-image': [3], 'all-text': [4, 5, 6, 7, 8, 9, 10, 11], 'last': [-1], \"inputs-partition-1\": [7,8], \"inputs-partition-2\": [9, 10, 11]}\n    \"\"\"\n    if not all(\n        token in SUPPORTED_TOKENS\n        or token.startswith(\"inputs-partition-\")\n        or token.startswith(\"random-inputs-partition-\")\n        for token in tokens\n    ):\n        raise ValueError(\n            f\"Unsupported token type: {tokens}. Supported tokens are: {SUPPORTED_TOKENS} and inputs-partition-0, inputs-partition-1, etc or random-inputs-partition-0, random-inputs-partition-1, etc\"\n        )\n\n    # Check if pivot_positions is required but not provided\n    if self.pivot_positions is None and any(\n        token.startswith(\"inputs-partition-\")\n        or token.startswith(\"random-inputs-partition-\")\n        for token in tokens\n    ):\n        raise ValueError(\n            \"pivot_positions cannot be None when a group position token is requested\"\n        )\n\n    token_indexes = self.categorize_tokens(string_tokens)\n    tokens_positions = self.get_tokens_positions(tokens, token_indexes)\n\n    # if return_type == \"int\":\n    #     if len(tokens_positions) &gt; 1:\n    #         raise ValueError(\n    #             \"More than one token requested: return_type should be list, got int\"\n    #         )\n    #     return tokens_positions[0]\n    if return_type == \"dict\":\n        return self.get_token_dict(token_indexes)\n    if return_type == \"all\":\n        return tokens_positions, self.get_token_dict(token_indexes)\n    return tokens_positions\n</code></pre>"}]}

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://easyroutine.github.io/api/interpretability/hooks/">
      
      
        <link rel="prev" href="../hooked_model/">
      
      
        <link rel="next" href="../models/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Hooks - EasyRoutine</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#easyroutine.interpretability.hooks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="EasyRoutine" class="md-header__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EasyRoutine
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Hooks
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="EasyRoutine" class="md-nav__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    EasyRoutine
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Api
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Api
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Interpretability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Interpretability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activation_cache/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Activation cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hooked_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hooked model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Hooks
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Hooks
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks" class="md-nav__link">
    <span class="md-ellipsis">
      hooks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_attn_mat_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_attn_mat_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_heads_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_heads_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_pos_keep_self_attn_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_tokens_hook_flash_attn" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_tokens_hook_flash_attn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      avg_attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_hook" class="md-nav__link">
    <span class="md-ellipsis">
      avg_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.create_dynamic_hook" class="md-nav__link">
    <span class="md-ellipsis">
      create_dynamic_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.embed_hook" class="md-nav__link">
    <span class="md-ellipsis">
      embed_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_value_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_value_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.query_key_value_hook" class="md-nav__link">
    <span class="md-ellipsis">
      query_key_value_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.save_resid_hook" class="md-nav__link">
    <span class="md-ellipsis">
      save_resid_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.zero_ablation" class="md-nav__link">
    <span class="md-ellipsis">
      zero_ablation
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module_wrapper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Module Wrapper
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../token_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Token index
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks" class="md-nav__link">
    <span class="md-ellipsis">
      hooks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_attn_mat_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_attn_mat_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_heads_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_heads_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_pos_keep_self_attn_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.ablate_tokens_hook_flash_attn" class="md-nav__link">
    <span class="md-ellipsis">
      ablate_tokens_hook_flash_attn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_attention_pattern_head" class="md-nav__link">
    <span class="md-ellipsis">
      avg_attention_pattern_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.avg_hook" class="md-nav__link">
    <span class="md-ellipsis">
      avg_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.create_dynamic_hook" class="md-nav__link">
    <span class="md-ellipsis">
      create_dynamic_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.embed_hook" class="md-nav__link">
    <span class="md-ellipsis">
      embed_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.projected_value_vectors_head" class="md-nav__link">
    <span class="md-ellipsis">
      projected_value_vectors_head
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.query_key_value_hook" class="md-nav__link">
    <span class="md-ellipsis">
      query_key_value_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.save_resid_hook" class="md-nav__link">
    <span class="md-ellipsis">
      save_resid_hook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooks.zero_ablation" class="md-nav__link">
    <span class="md-ellipsis">
      zero_ablation
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Hooks</h1>

<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.hooks"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.ablate_attn_mat_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ablate_attn_mat_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the tokens in the attention
mask. It will set to 0 the value vector of the
tokens to ablate</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ablate_attn_mat_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the tokens in the attention</span>
<span class="sd">    mask. It will set to 0 the value vector of the</span>
<span class="sd">    tokens to ablate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the shape of the attention matrix</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len_q</span><span class="p">,</span> <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">q_positions</span> <span class="o">=</span> <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;queries&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Used during generation</span>
    <span class="k">if</span> <span class="n">seq_len_q</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">q_positions</span><span class="p">):</span>
        <span class="n">q_positions</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">k_positions</span> <span class="o">=</span> <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;keys&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Create boolean masks for queries and keys</span>
    <span class="n">q_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len_q</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">q_mask</span><span class="p">[</span><span class="n">q_positions</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Set positions to True</span>

    <span class="n">k_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len_k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">k_mask</span><span class="p">[</span><span class="n">k_positions</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Set positions to TrueW</span>

    <span class="c1"># Create a 2D mask using outer product</span>
    <span class="n">head_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">q_mask</span><span class="p">,</span> <span class="n">k_mask</span><span class="p">)</span>  <span class="c1"># Shape: (seq_len_q, seq_len_k)</span>

    <span class="c1"># Expand mask to match the dimensions of the attention matrix</span>
    <span class="c1"># Shape after expand: (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    <span class="n">head_mask</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">head_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="c1"># Apply the ablation function directly to the attention matrix</span>
    <span class="n">b</span><span class="p">[</span><span class="n">head_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">zero_ablation</span><span class="p">(</span><span class="n">b</span><span class="p">[</span><span class="n">head_mask</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.ablate_heads_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ablate_heads_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the heads in the attention
mask. It will set to 0 the output of the heads to
ablate</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ablate_heads_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the heads in the attention</span>
<span class="sd">    mask. It will set to 0 the output of the heads to</span>
<span class="sd">    ablate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>

    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">]:</span>
        <span class="n">attention_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">b</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.ablate_pos_keep_self_attn_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ablate_pos_keep_self_attn_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to ablate the tokens in the attention
mask but keeping the self attn weigths.
It will set to 0 the row of tokens to ablate except for
the las position</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ablate_pos_keep_self_attn_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to ablate the tokens in the attention</span>
<span class="sd">    mask but keeping the self attn weigths.</span>
<span class="sd">    It will set to 0 the row of tokens to ablate except for</span>
<span class="sd">    the las position</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="ne">Warning</span><span class="p">(</span><span class="s2">&quot;This function is deprecated. Use ablate_attn_mat_hook instead&quot;</span><span class="p">)</span>
    <span class="n">attn_matrix</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
    <span class="c1"># initial_shape = attn_matrix.shape</span>

    <span class="k">for</span> <span class="n">head</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
        <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">],</span> <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;pos_token_to_ablate&quot;</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="n">attn_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">b</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">attn_matrix</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.ablate_tokens_hook_flash_attn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ablate_tokens_hook_flash_attn</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>same of ablate_tokens_hook but for flash attention. This apply the ablation on the values vectors instead of the attention mask</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ablate_tokens_hook_flash_attn</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    same of ablate_tokens_hook but for flash attention. This apply the ablation on the values vectors instead of the attention mask</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">seq</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">b</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">device</span>

    <span class="n">ablation_queries</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span>
        <span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>  <span class="c1"># Reset index to avoid problems with casting to tensor</span>
    <span class="n">head_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="n">pos_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">ablation_queries</span><span class="p">[</span><span class="s2">&quot;keys&quot;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>
    <span class="c1"># if num_layers != len(head_indices) or not torch.all(pos_indices == pos_indices[0]) :</span>
    <span class="c1">#     raise ValueError(&quot;Flash attention ablation should be done on all heads at the same layer and at the same token position&quot;)</span>
    <span class="c1"># if seq &lt; pos_indices[0]:</span>
    <span class="c1">#     # during generation the desired value vector has already been ablated</span>
    <span class="c1">#     return b</span>
    <span class="n">pos_indices</span> <span class="o">=</span> <span class="n">pos_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Use advanced indexing to set the specified slices to zero</span>
    <span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">pos_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">b</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

    <span class="c1">#!!dirty fix</span>

    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.attention_pattern_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">attention_pattern_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the attention pattern of the heads. It will extract the attention pattern.
As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>b</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function. It's the output of the attention pattern of the heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>s</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the state of the hook function. It's the state of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>layer</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>head</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>expand_head</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like "pattern_L0H0", "pattern_L0H1", ...
            while if False, we will have only one key for each layer, like "pattern_L0" and the dimension of the head will be taken into account in the tensor.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">attention_pattern_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">token_index</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the attention pattern of the heads. It will extract the attention pattern.</span>
<span class="sd">    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</span>

<span class="sd">    Args:</span>
<span class="sd">        - b: the input of the hook function. It&#39;s the output of the attention pattern of the heads</span>
<span class="sd">        - s: the state of the hook function. It&#39;s the state of the model</span>
<span class="sd">        - layer: the layer of the model</span>
<span class="sd">        - head: the head of the model</span>
<span class="sd">        - expand_head: bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like &quot;pattern_L0H0&quot;, &quot;pattern_L0H1&quot;, ...</span>
<span class="sd">                        while if False, we will have only one key for each layer, like &quot;pattern_L0&quot; and the dimension of the head will be taken into account in the tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># first get the attention pattern</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">attn_pattern</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads,seq_len, seq_len)</span>

    <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attn_pattern</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;pattern_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head_idx</span><span class="p">,</span> <span class="n">token_index</span><span class="p">][:,</span> <span class="p">:,</span> <span class="n">token_index</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pattern_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">token_index</span><span class="p">][</span>
            <span class="p">:,</span> <span class="p">:,</span> <span class="n">token_index</span>
        <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.avg_attention_pattern_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">avg_attention_pattern_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">attn_pattern_current_avg</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">extract_avg_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it.
As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>b</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function. It's the output of the attention pattern of the heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>s</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the state of the hook function. It's the state of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>layer</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>head</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>-</code>
            </td>
            <td>
                  <code>attn_pattern_current_avg</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the current average attention pattern</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">avg_attention_pattern_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">token_index</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">attn_pattern_current_avg</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">extract_avg_value</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the average attention pattern of the heads. It will extract the attention pattern and then average it.</span>
<span class="sd">    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</span>

<span class="sd">    Args:</span>
<span class="sd">        - b: the input of the hook function. It&#39;s the output of the attention pattern of the heads</span>
<span class="sd">        - s: the state of the hook function. It&#39;s the state of the model</span>
<span class="sd">        - layer: the layer of the model</span>
<span class="sd">        - head: the head of the model</span>
<span class="sd">        - attn_pattern_current_avg: the current average attention pattern</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># first get the attention pattern</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">attn_pattern</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads,seq_len, seq_len)</span>
    <span class="c1"># attn_pattern = attn_pattern.to(torch.float32)</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;avg_pattern_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">attn_pattern_current_avg</span><span class="p">:</span>
            <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">token_index</span><span class="p">][</span>
                <span class="p">:,</span> <span class="p">:,</span> <span class="n">token_index</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">,</span> <span class="n">token_index</span><span class="p">][:,</span> <span class="p">:,</span> <span class="n">token_index</span><span class="p">]</span>
                <span class="o">-</span> <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="c1"># var_key = f&quot;M2_pattern_L{layer}H{head}&quot;</span>
        <span class="c1"># if var_key not in attn_pattern_current_avg:</span>
        <span class="c1">#     attn_pattern_current_avg[var_key] = torch.zeros_like(attn_pattern[:, head])</span>
        <span class="c1"># attn_pattern_current_avg[var_key] = attn_pattern_current_avg[var_key] + (attn_pattern[:, head] - attn_pattern_current_avg[key]) * (attn_pattern[:, head] - attn_pattern_current_avg[var_key])</span>

        <span class="k">if</span> <span class="n">extract_avg_value</span><span class="p">:</span>
            <span class="n">value_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;projected_value_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">values</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Values not found for </span><span class="si">{</span><span class="n">value_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">return</span>
            <span class="c1"># get the attention pattern for the values</span>
            <span class="n">value_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">norm_matrix</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">value_norm</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="n">norm_matrix</span> <span class="o">=</span> <span class="n">norm_matrix</span> <span class="o">*</span> <span class="n">attn_pattern</span><span class="p">[:,</span> <span class="n">head</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">value_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">attn_pattern_current_avg</span><span class="p">:</span>
                <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm_matrix</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="n">norm_matrix</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="p">:]</span>
                    <span class="o">-</span> <span class="n">attn_pattern_current_avg</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span>
                <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># remove values from cache</span>
            <span class="k">del</span> <span class="n">cache</span><span class="p">[</span><span class="n">value_key</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.avg_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">avg_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">last_image_idx</span><span class="p">,</span> <span class="n">end_image_idx</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">avg_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">last_image_idx</span><span class="p">,</span>
    <span class="n">end_image_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">img_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">last_image_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">text_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="n">end_image_idx</span><span class="p">:,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">all_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;avg_</span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">img_avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">text_avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">all_avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.create_dynamic_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_dynamic_hook</span><span class="p">(</span><span class="n">pyvene_hook</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>DEPRECATED: pyvene is not used anymore.
This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_dynamic_hook</span><span class="p">(</span><span class="n">pyvene_hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DEPRECATED: pyvene is not used anymore.</span>
<span class="sd">    This function is used to create a dynamic hook. It is a wrapper around the pyvene_hook function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">partial_hook</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">pyvene_hook</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">wrap</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">partial_hook</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrap</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.embed_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">embed_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the embeddings of the tokens. It will save the embeddings in the cache (a global variable out the scope of the function)</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">embed_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the embeddings of the tokens. It will save the embeddings in the cache (a global variable out the scope of the function)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">output</span>
    <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.projected_value_vectors_head" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">projected_value_vectors_head</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">expand_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection
As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>b</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the input of the hook function. It's the output of the values vectors of the heads</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>s</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the state of the hook function. It's the state of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>layer</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the layer of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>head</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[str, int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the head of the model. If "all" is passed, it will extract all the heads of the layer</p>
              </div>
            </td>
            <td>
                  <code>&#39;all&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>expand_head</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like "value_L0H0", "value_L0H1", ...
            while if False, we will have only one key for each layer, like "value_L0" and the dimension of the head will be taken into account in the tensor.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">projected_value_vectors_head</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">,</span>
    <span class="n">token_index</span><span class="p">,</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_key_value_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_proj_weight</span><span class="p">,</span>
    <span class="n">out_proj_bias</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">act_on_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">expand_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hook function to extract the values vectors of the heads. It will extract the values vectors and then project them with the final W_O projection</span>
<span class="sd">    As the other hooks, it will save the activations in the cache (a global variable out the scope of the function)</span>

<span class="sd">    Args:</span>
<span class="sd">        b: the input of the hook function. It&#39;s the output of the values vectors of the heads</span>
<span class="sd">        s: the state of the hook function. It&#39;s the state of the model</span>
<span class="sd">        layer: the layer of the model</span>
<span class="sd">        head: the head of the model. If &quot;all&quot; is passed, it will extract all the heads of the layer</span>
<span class="sd">        expand_head: bool to expand the head dimension when extracting the values vectors and the attention pattern. If true, in the cache we will have a key for each head, like &quot;value_L0H0&quot;, &quot;value_L0H1&quot;, ...</span>
<span class="sd">                        while if False, we will have only one key for each layer, like &quot;value_L0&quot; and the dimension of the head will be taken into account in the tensor.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># first get the values vectors</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>  <span class="c1"># (batch, num_heads,seq_len, head_dim)</span>

    <span class="c1"># reshape the values vectors to have a separate dimension for the different heads</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">values</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len (num_key_value_heads d_heads) -&gt; batch num_key_value_heads seq_len d_heads&quot;</span><span class="p">,</span>
        <span class="n">num_key_value_heads</span><span class="o">=</span><span class="n">num_key_value_heads</span><span class="p">,</span>
        <span class="n">d_heads</span><span class="o">=</span><span class="n">d_head</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1">#        &quot;batch seq_len (num_key_value_heads d_heads) -&gt; batch seq_len num_key_value_heads d_heads&quot;,</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">num_attention_heads</span> <span class="o">//</span> <span class="n">num_key_value_heads</span><span class="p">)</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">values</span><span class="p">,</span>
        <span class="s2">&quot;batch num_head seq_len d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># reshape in order to get the blocks for each head</span>
    <span class="n">out_proj_weight</span> <span class="o">=</span> <span class="n">out_proj_weight</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">num_attention_heads</span><span class="p">,</span>
        <span class="n">d_head</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># apply bias if present (No in Chameleon)</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_proj_bias</span> <span class="o">=</span> <span class="n">out_proj_bias</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="c1"># apply the projection for each head</span>
    <span class="n">projected_values</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span>
        <span class="n">values</span><span class="p">,</span>
        <span class="n">out_proj_weight</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_head, num_head d_head d_model -&gt; batch seq_len num_head d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">out_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">projected_values</span> <span class="o">=</span> <span class="n">projected_values</span> <span class="o">+</span> <span class="n">out_proj_bias</span>

    <span class="c1"># rearrange the tensor to have dimension that we like more</span>
    <span class="n">projected_values</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
        <span class="n">projected_values</span><span class="p">,</span>
        <span class="s2">&quot;batch seq_len num_head d_model -&gt; batch num_head seq_len d_model&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># slice for token index</span>
    <span class="n">projected_values</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">token_index</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="n">projected_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">projected_values</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># post-process the values vectors</span>
    <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_attention_heads</span><span class="p">):</span>
            <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_value_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[</span>
                <span class="p">:,</span> <span class="n">head_idx</span>
            <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;projected_value_L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projected_values</span><span class="p">[:,</span> <span class="nb">int</span><span class="p">(</span><span class="n">head</span><span class="p">)]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.query_key_value_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">query_key_value_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">query_key_value_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">:</span> <span class="n">ActivationCache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">token_index</span><span class="p">,</span>
    <span class="n">layer</span><span class="p">,</span>
    <span class="n">head_dim</span><span class="p">,</span>
    <span class="n">num_key_value_groups</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
    <span class="n">head</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as save_resid_hook but for the query, key and value vectors, it just have a reshape to have the head dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">hidden_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># cache[cache_key] = b.data.detach().clone()[..., token_index, :]</span>

    <span class="n">info_string</span> <span class="o">=</span> <span class="s2">&quot;Shape: batch seq_len d_head&quot;</span>


    <span class="n">heads</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))]</span> <span class="k">if</span> <span class="n">head</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">head</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">head_idx</span> <span class="ow">in</span> <span class="n">heads</span><span class="p">:</span>
        <span class="c1"># compute group index</span>
        <span class="n">group_idx</span> <span class="o">=</span> <span class="n">head_idx</span> <span class="o">//</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;values_&quot;</span> <span class="ow">in</span> <span class="n">cache_key</span> <span class="ow">or</span> <span class="s2">&quot;keys_&quot;</span> <span class="ow">in</span> <span class="n">cache_key</span><span class="p">:</span>
            <span class="n">cache</span><span class="o">.</span><span class="n">add_with_info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="n">group_idx</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="p">:],</span>
                <span class="n">info_string</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cache</span><span class="o">.</span><span class="n">add_with_info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">L</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">H</span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[:,</span> <span class="n">head_idx</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="p">:],</span>
                <span class="n">info_string</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.save_resid_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">save_resid_hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_key</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="n">avg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">save_resid_hook</span><span class="p">(</span>
    <span class="n">module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">,</span>
    <span class="n">kwargs</span><span class="p">,</span>
    <span class="n">output</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">:</span> <span class="n">ActivationCache</span><span class="p">,</span>
    <span class="n">cache_key</span><span class="p">,</span>
    <span class="n">token_index</span><span class="p">,</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It save the activations of the residual stream in the cache. It will save the activations in the cache (a global variable out the scope of the function)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="c1"># slice the tensor to get the activations of the token we want to extract</span>
    <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()[</span><span class="o">...</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">avg</span><span class="p">:</span>
        <span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="easyroutine.interpretability.hooks.zero_ablation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zero_ablation</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Set the attention values to zero</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooks.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">zero_ablation</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the attention values to zero</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>
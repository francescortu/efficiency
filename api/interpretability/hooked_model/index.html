
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://easyroutine.github.io/api/interpretability/hooked_model/">
      
      
        <link rel="prev" href="../activation_cache/">
      
      
        <link rel="next" href="../hooks/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Hooked model - EasyRoutine</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#easyroutine.interpretability.hooked_model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="EasyRoutine" class="md-header__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EasyRoutine
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Hooked model
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="EasyRoutine" class="md-nav__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    EasyRoutine
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Api
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Api
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Interpretability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Interpretability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activation_cache/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Activation cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Hooked model
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Hooked model
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model" class="md-nav__link">
    <span class="md-ellipsis">
      hooked_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.ExtractionConfig" class="md-nav__link">
    <span class="md-ellipsis">
      ExtractionConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ExtractionConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.ExtractionConfig.is_not_empty" class="md-nav__link">
    <span class="md-ellipsis">
      is_not_empty
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel" class="md-nav__link">
    <span class="md-ellipsis">
      HookedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HookedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.compute_patching" class="md-nav__link">
    <span class="md-ellipsis">
      compute_patching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.create_hooks" class="md-nav__link">
    <span class="md-ellipsis">
      create_hooks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.device" class="md-nav__link">
    <span class="md-ellipsis">
      device
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.eval" class="md-nav__link">
    <span class="md-ellipsis">
      eval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.extract_cache" class="md-nav__link">
    <span class="md-ellipsis">
      extract_cache
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.get_module_from_string" class="md-nav__link">
    <span class="md-ellipsis">
      get_module_from_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.get_processor" class="md-nav__link">
    <span class="md-ellipsis">
      get_processor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.get_text_tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      get_text_tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.register_forward_hook" class="md-nav__link">
    <span class="md-ellipsis">
      register_forward_hook
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.remove_hooks" class="md-nav__link">
    <span class="md-ellipsis">
      remove_hooks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.set_hooks" class="md-nav__link">
    <span class="md-ellipsis">
      set_hooks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.to_string_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      to_string_tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModelConfig" class="md-nav__link">
    <span class="md-ellipsis">
      HookedModelConfig
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hooks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hooks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../module_wrapper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Module Wrapper
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../token_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Token index
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model" class="md-nav__link">
    <span class="md-ellipsis">
      hooked_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.ExtractionConfig" class="md-nav__link">
    <span class="md-ellipsis">
      ExtractionConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ExtractionConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.ExtractionConfig.is_not_empty" class="md-nav__link">
    <span class="md-ellipsis">
      is_not_empty
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel" class="md-nav__link">
    <span class="md-ellipsis">
      HookedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HookedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.compute_patching" class="md-nav__link">
    <span class="md-ellipsis">
      compute_patching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.create_hooks" class="md-nav__link">
    <span class="md-ellipsis">
      create_hooks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.device" class="md-nav__link">
    <span class="md-ellipsis">
      device
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.eval" class="md-nav__link">
    <span class="md-ellipsis">
      eval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.extract_cache" class="md-nav__link">
    <span class="md-ellipsis">
      extract_cache
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.generate" class="md-nav__link">
    <span class="md-ellipsis">
      generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.get_module_from_string" class="md-nav__link">
    <span class="md-ellipsis">
      get_module_from_string
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.get_processor" class="md-nav__link">
    <span class="md-ellipsis">
      get_processor
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.get_text_tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      get_text_tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.register_forward_hook" class="md-nav__link">
    <span class="md-ellipsis">
      register_forward_hook
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.remove_hooks" class="md-nav__link">
    <span class="md-ellipsis">
      remove_hooks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.set_hooks" class="md-nav__link">
    <span class="md-ellipsis">
      set_hooks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModel.to_string_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      to_string_tokens
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#easyroutine.interpretability.hooked_model.HookedModelConfig" class="md-nav__link">
    <span class="md-ellipsis">
      HookedModelConfig
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Hooked model</h1>

<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.hooked_model"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="easyroutine.interpretability.hooked_model.ExtractionConfig" class="doc doc-heading">
            <code>ExtractionConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


        <p>Configuration of the extraction of the activations of the model. It store what activations you want to extract from the model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>extract_resid_in</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the input of the residual stream</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_resid_mid</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the output of the intermediate stream</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_resid_out</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the output of the residual stream</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_resid_in_post_layernorm(bool)</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the input of the residual stream after the layernorm</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_attn_pattern</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the attention pattern of the attn</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_head_values_projected</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the values vectors projected of the model</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_head_values</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the values of the attention</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_head_out</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the output of the heads [DEPRECATED]</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_attn_out</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the output of the attention of the attn_heads passed</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_attn_in</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the input of the attention of the attn_heads passed</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extract_mlp_out</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the output of the mlp of the attn</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>save_input_ids</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, save the input_ids in the cache</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>avg</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the average of the activations over the target positions</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>avg_over_example</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, extract the average of the activations over the examples (it required a external cache to save the running avg)</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_heads</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[list[dict], <span title="typing.Literal">Literal</span>[&#39;all&#39;]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of dictionaries with the layer and head to extract the attention pattern or 'all' to</p>
              </div>
            </td>
            <td>
                  <code>&#39;all&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ExtractionConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration of the extraction of the activations of the model. It store what activations you want to extract from the model.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        extract_resid_in (bool): if True, extract the input of the residual stream</span>
<span class="sd">        extract_resid_mid (bool): if True, extract the output of the intermediate stream</span>
<span class="sd">        extract_resid_out (bool): if True, extract the output of the residual stream</span>
<span class="sd">        extract_resid_in_post_layernorm(bool): if True, extract the input of the residual stream after the layernorm</span>
<span class="sd">        extract_attn_pattern (bool): if True, extract the attention pattern of the attn</span>
<span class="sd">        extract_head_values_projected (bool): if True, extract the values vectors projected of the model</span>
<span class="sd">        extract_head_values (bool): if True, extract the values of the attention</span>
<span class="sd">        extract_head_out (bool): if True, extract the output of the heads [DEPRECATED]</span>
<span class="sd">        extract_attn_out (bool): if True, extract the output of the attention of the attn_heads passed</span>
<span class="sd">        extract_attn_in (bool): if True, extract the input of the attention of the attn_heads passed</span>
<span class="sd">        extract_mlp_out (bool): if True, extract the output of the mlp of the attn</span>
<span class="sd">        save_input_ids (bool): if True, save the input_ids in the cache</span>
<span class="sd">        avg (bool): if True, extract the average of the activations over the target positions</span>
<span class="sd">        avg_over_example (bool): if True, extract the average of the activations over the examples (it required a external cache to save the running avg)</span>
<span class="sd">        attn_heads (Union[list[dict], Literal[&quot;all&quot;]]): list of dictionaries with the layer and head to extract the attention pattern or &#39;all&#39; to</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">extract_resid_in</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_resid_mid</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_resid_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_resid_in_post_layernorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_attn_pattern</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_head_values_projected</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># TODO: add extract_head_queries_projected</span>
    <span class="c1"># TODO: add extract_head_keys_projected</span>
    <span class="n">extract_head_keys</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_head_values</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_head_queries</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_head_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_attn_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_attn_in</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">extract_mlp_out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">save_input_ids</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">avg_over_example</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">attn_heads</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span>

    <span class="k">def</span> <span class="nf">is_not_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return True if at least one of the attributes is True, False otherwise, i.e. if the model should extract something!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">any</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_resid_in</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_resid_mid</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_resid_out</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_attn_pattern</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_values_projected</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_keys</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_values</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_queries</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_out</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_attn_out</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_attn_in</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">extract_mlp_out</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_input_ids</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">avg_over_example</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.ExtractionConfig.is_not_empty" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">is_not_empty</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Return True if at least one of the attributes is True, False otherwise, i.e. if the model should extract something!</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">is_not_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return True if at least one of the attributes is True, False otherwise, i.e. if the model should extract something!</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_resid_in</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_resid_mid</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_resid_out</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_attn_pattern</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_values_projected</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_keys</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_values</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_queries</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_head_out</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_attn_out</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_attn_in</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extract_mlp_out</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_input_ids</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">avg_over_example</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="easyroutine.interpretability.hooked_model.HookedModel" class="doc doc-heading">
            <code>HookedModel</code>


</h2>


    <div class="doc doc-contents ">


        <p>This class is a wrapper around the huggingface model that allows to extract the activations of the model. It is support
advanced mechanistic intepretability methods like ablation, patching, etc.</p>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 145</span>
<span class="normal"> 146</span>
<span class="normal"> 147</span>
<span class="normal"> 148</span>
<span class="normal"> 149</span>
<span class="normal"> 150</span>
<span class="normal"> 151</span>
<span class="normal"> 152</span>
<span class="normal"> 153</span>
<span class="normal"> 154</span>
<span class="normal"> 155</span>
<span class="normal"> 156</span>
<span class="normal"> 157</span>
<span class="normal"> 158</span>
<span class="normal"> 159</span>
<span class="normal"> 160</span>
<span class="normal"> 161</span>
<span class="normal"> 162</span>
<span class="normal"> 163</span>
<span class="normal"> 164</span>
<span class="normal"> 165</span>
<span class="normal"> 166</span>
<span class="normal"> 167</span>
<span class="normal"> 168</span>
<span class="normal"> 169</span>
<span class="normal"> 170</span>
<span class="normal"> 171</span>
<span class="normal"> 172</span>
<span class="normal"> 173</span>
<span class="normal"> 174</span>
<span class="normal"> 175</span>
<span class="normal"> 176</span>
<span class="normal"> 177</span>
<span class="normal"> 178</span>
<span class="normal"> 179</span>
<span class="normal"> 180</span>
<span class="normal"> 181</span>
<span class="normal"> 182</span>
<span class="normal"> 183</span>
<span class="normal"> 184</span>
<span class="normal"> 185</span>
<span class="normal"> 186</span>
<span class="normal"> 187</span>
<span class="normal"> 188</span>
<span class="normal"> 189</span>
<span class="normal"> 190</span>
<span class="normal"> 191</span>
<span class="normal"> 192</span>
<span class="normal"> 193</span>
<span class="normal"> 194</span>
<span class="normal"> 195</span>
<span class="normal"> 196</span>
<span class="normal"> 197</span>
<span class="normal"> 198</span>
<span class="normal"> 199</span>
<span class="normal"> 200</span>
<span class="normal"> 201</span>
<span class="normal"> 202</span>
<span class="normal"> 203</span>
<span class="normal"> 204</span>
<span class="normal"> 205</span>
<span class="normal"> 206</span>
<span class="normal"> 207</span>
<span class="normal"> 208</span>
<span class="normal"> 209</span>
<span class="normal"> 210</span>
<span class="normal"> 211</span>
<span class="normal"> 212</span>
<span class="normal"> 213</span>
<span class="normal"> 214</span>
<span class="normal"> 215</span>
<span class="normal"> 216</span>
<span class="normal"> 217</span>
<span class="normal"> 218</span>
<span class="normal"> 219</span>
<span class="normal"> 220</span>
<span class="normal"> 221</span>
<span class="normal"> 222</span>
<span class="normal"> 223</span>
<span class="normal"> 224</span>
<span class="normal"> 225</span>
<span class="normal"> 226</span>
<span class="normal"> 227</span>
<span class="normal"> 228</span>
<span class="normal"> 229</span>
<span class="normal"> 230</span>
<span class="normal"> 231</span>
<span class="normal"> 232</span>
<span class="normal"> 233</span>
<span class="normal"> 234</span>
<span class="normal"> 235</span>
<span class="normal"> 236</span>
<span class="normal"> 237</span>
<span class="normal"> 238</span>
<span class="normal"> 239</span>
<span class="normal"> 240</span>
<span class="normal"> 241</span>
<span class="normal"> 242</span>
<span class="normal"> 243</span>
<span class="normal"> 244</span>
<span class="normal"> 245</span>
<span class="normal"> 246</span>
<span class="normal"> 247</span>
<span class="normal"> 248</span>
<span class="normal"> 249</span>
<span class="normal"> 250</span>
<span class="normal"> 251</span>
<span class="normal"> 252</span>
<span class="normal"> 253</span>
<span class="normal"> 254</span>
<span class="normal"> 255</span>
<span class="normal"> 256</span>
<span class="normal"> 257</span>
<span class="normal"> 258</span>
<span class="normal"> 259</span>
<span class="normal"> 260</span>
<span class="normal"> 261</span>
<span class="normal"> 262</span>
<span class="normal"> 263</span>
<span class="normal"> 264</span>
<span class="normal"> 265</span>
<span class="normal"> 266</span>
<span class="normal"> 267</span>
<span class="normal"> 268</span>
<span class="normal"> 269</span>
<span class="normal"> 270</span>
<span class="normal"> 271</span>
<span class="normal"> 272</span>
<span class="normal"> 273</span>
<span class="normal"> 274</span>
<span class="normal"> 275</span>
<span class="normal"> 276</span>
<span class="normal"> 277</span>
<span class="normal"> 278</span>
<span class="normal"> 279</span>
<span class="normal"> 280</span>
<span class="normal"> 281</span>
<span class="normal"> 282</span>
<span class="normal"> 283</span>
<span class="normal"> 284</span>
<span class="normal"> 285</span>
<span class="normal"> 286</span>
<span class="normal"> 287</span>
<span class="normal"> 288</span>
<span class="normal"> 289</span>
<span class="normal"> 290</span>
<span class="normal"> 291</span>
<span class="normal"> 292</span>
<span class="normal"> 293</span>
<span class="normal"> 294</span>
<span class="normal"> 295</span>
<span class="normal"> 296</span>
<span class="normal"> 297</span>
<span class="normal"> 298</span>
<span class="normal"> 299</span>
<span class="normal"> 300</span>
<span class="normal"> 301</span>
<span class="normal"> 302</span>
<span class="normal"> 303</span>
<span class="normal"> 304</span>
<span class="normal"> 305</span>
<span class="normal"> 306</span>
<span class="normal"> 307</span>
<span class="normal"> 308</span>
<span class="normal"> 309</span>
<span class="normal"> 310</span>
<span class="normal"> 311</span>
<span class="normal"> 312</span>
<span class="normal"> 313</span>
<span class="normal"> 314</span>
<span class="normal"> 315</span>
<span class="normal"> 316</span>
<span class="normal"> 317</span>
<span class="normal"> 318</span>
<span class="normal"> 319</span>
<span class="normal"> 320</span>
<span class="normal"> 321</span>
<span class="normal"> 322</span>
<span class="normal"> 323</span>
<span class="normal"> 324</span>
<span class="normal"> 325</span>
<span class="normal"> 326</span>
<span class="normal"> 327</span>
<span class="normal"> 328</span>
<span class="normal"> 329</span>
<span class="normal"> 330</span>
<span class="normal"> 331</span>
<span class="normal"> 332</span>
<span class="normal"> 333</span>
<span class="normal"> 334</span>
<span class="normal"> 335</span>
<span class="normal"> 336</span>
<span class="normal"> 337</span>
<span class="normal"> 338</span>
<span class="normal"> 339</span>
<span class="normal"> 340</span>
<span class="normal"> 341</span>
<span class="normal"> 342</span>
<span class="normal"> 343</span>
<span class="normal"> 344</span>
<span class="normal"> 345</span>
<span class="normal"> 346</span>
<span class="normal"> 347</span>
<span class="normal"> 348</span>
<span class="normal"> 349</span>
<span class="normal"> 350</span>
<span class="normal"> 351</span>
<span class="normal"> 352</span>
<span class="normal"> 353</span>
<span class="normal"> 354</span>
<span class="normal"> 355</span>
<span class="normal"> 356</span>
<span class="normal"> 357</span>
<span class="normal"> 358</span>
<span class="normal"> 359</span>
<span class="normal"> 360</span>
<span class="normal"> 361</span>
<span class="normal"> 362</span>
<span class="normal"> 363</span>
<span class="normal"> 364</span>
<span class="normal"> 365</span>
<span class="normal"> 366</span>
<span class="normal"> 367</span>
<span class="normal"> 368</span>
<span class="normal"> 369</span>
<span class="normal"> 370</span>
<span class="normal"> 371</span>
<span class="normal"> 372</span>
<span class="normal"> 373</span>
<span class="normal"> 374</span>
<span class="normal"> 375</span>
<span class="normal"> 376</span>
<span class="normal"> 377</span>
<span class="normal"> 378</span>
<span class="normal"> 379</span>
<span class="normal"> 380</span>
<span class="normal"> 381</span>
<span class="normal"> 382</span>
<span class="normal"> 383</span>
<span class="normal"> 384</span>
<span class="normal"> 385</span>
<span class="normal"> 386</span>
<span class="normal"> 387</span>
<span class="normal"> 388</span>
<span class="normal"> 389</span>
<span class="normal"> 390</span>
<span class="normal"> 391</span>
<span class="normal"> 392</span>
<span class="normal"> 393</span>
<span class="normal"> 394</span>
<span class="normal"> 395</span>
<span class="normal"> 396</span>
<span class="normal"> 397</span>
<span class="normal"> 398</span>
<span class="normal"> 399</span>
<span class="normal"> 400</span>
<span class="normal"> 401</span>
<span class="normal"> 402</span>
<span class="normal"> 403</span>
<span class="normal"> 404</span>
<span class="normal"> 405</span>
<span class="normal"> 406</span>
<span class="normal"> 407</span>
<span class="normal"> 408</span>
<span class="normal"> 409</span>
<span class="normal"> 410</span>
<span class="normal"> 411</span>
<span class="normal"> 412</span>
<span class="normal"> 413</span>
<span class="normal"> 414</span>
<span class="normal"> 415</span>
<span class="normal"> 416</span>
<span class="normal"> 417</span>
<span class="normal"> 418</span>
<span class="normal"> 419</span>
<span class="normal"> 420</span>
<span class="normal"> 421</span>
<span class="normal"> 422</span>
<span class="normal"> 423</span>
<span class="normal"> 424</span>
<span class="normal"> 425</span>
<span class="normal"> 426</span>
<span class="normal"> 427</span>
<span class="normal"> 428</span>
<span class="normal"> 429</span>
<span class="normal"> 430</span>
<span class="normal"> 431</span>
<span class="normal"> 432</span>
<span class="normal"> 433</span>
<span class="normal"> 434</span>
<span class="normal"> 435</span>
<span class="normal"> 436</span>
<span class="normal"> 437</span>
<span class="normal"> 438</span>
<span class="normal"> 439</span>
<span class="normal"> 440</span>
<span class="normal"> 441</span>
<span class="normal"> 442</span>
<span class="normal"> 443</span>
<span class="normal"> 444</span>
<span class="normal"> 445</span>
<span class="normal"> 446</span>
<span class="normal"> 447</span>
<span class="normal"> 448</span>
<span class="normal"> 449</span>
<span class="normal"> 450</span>
<span class="normal"> 451</span>
<span class="normal"> 452</span>
<span class="normal"> 453</span>
<span class="normal"> 454</span>
<span class="normal"> 455</span>
<span class="normal"> 456</span>
<span class="normal"> 457</span>
<span class="normal"> 458</span>
<span class="normal"> 459</span>
<span class="normal"> 460</span>
<span class="normal"> 461</span>
<span class="normal"> 462</span>
<span class="normal"> 463</span>
<span class="normal"> 464</span>
<span class="normal"> 465</span>
<span class="normal"> 466</span>
<span class="normal"> 467</span>
<span class="normal"> 468</span>
<span class="normal"> 469</span>
<span class="normal"> 470</span>
<span class="normal"> 471</span>
<span class="normal"> 472</span>
<span class="normal"> 473</span>
<span class="normal"> 474</span>
<span class="normal"> 475</span>
<span class="normal"> 476</span>
<span class="normal"> 477</span>
<span class="normal"> 478</span>
<span class="normal"> 479</span>
<span class="normal"> 480</span>
<span class="normal"> 481</span>
<span class="normal"> 482</span>
<span class="normal"> 483</span>
<span class="normal"> 484</span>
<span class="normal"> 485</span>
<span class="normal"> 486</span>
<span class="normal"> 487</span>
<span class="normal"> 488</span>
<span class="normal"> 489</span>
<span class="normal"> 490</span>
<span class="normal"> 491</span>
<span class="normal"> 492</span>
<span class="normal"> 493</span>
<span class="normal"> 494</span>
<span class="normal"> 495</span>
<span class="normal"> 496</span>
<span class="normal"> 497</span>
<span class="normal"> 498</span>
<span class="normal"> 499</span>
<span class="normal"> 500</span>
<span class="normal"> 501</span>
<span class="normal"> 502</span>
<span class="normal"> 503</span>
<span class="normal"> 504</span>
<span class="normal"> 505</span>
<span class="normal"> 506</span>
<span class="normal"> 507</span>
<span class="normal"> 508</span>
<span class="normal"> 509</span>
<span class="normal"> 510</span>
<span class="normal"> 511</span>
<span class="normal"> 512</span>
<span class="normal"> 513</span>
<span class="normal"> 514</span>
<span class="normal"> 515</span>
<span class="normal"> 516</span>
<span class="normal"> 517</span>
<span class="normal"> 518</span>
<span class="normal"> 519</span>
<span class="normal"> 520</span>
<span class="normal"> 521</span>
<span class="normal"> 522</span>
<span class="normal"> 523</span>
<span class="normal"> 524</span>
<span class="normal"> 525</span>
<span class="normal"> 526</span>
<span class="normal"> 527</span>
<span class="normal"> 528</span>
<span class="normal"> 529</span>
<span class="normal"> 530</span>
<span class="normal"> 531</span>
<span class="normal"> 532</span>
<span class="normal"> 533</span>
<span class="normal"> 534</span>
<span class="normal"> 535</span>
<span class="normal"> 536</span>
<span class="normal"> 537</span>
<span class="normal"> 538</span>
<span class="normal"> 539</span>
<span class="normal"> 540</span>
<span class="normal"> 541</span>
<span class="normal"> 542</span>
<span class="normal"> 543</span>
<span class="normal"> 544</span>
<span class="normal"> 545</span>
<span class="normal"> 546</span>
<span class="normal"> 547</span>
<span class="normal"> 548</span>
<span class="normal"> 549</span>
<span class="normal"> 550</span>
<span class="normal"> 551</span>
<span class="normal"> 552</span>
<span class="normal"> 553</span>
<span class="normal"> 554</span>
<span class="normal"> 555</span>
<span class="normal"> 556</span>
<span class="normal"> 557</span>
<span class="normal"> 558</span>
<span class="normal"> 559</span>
<span class="normal"> 560</span>
<span class="normal"> 561</span>
<span class="normal"> 562</span>
<span class="normal"> 563</span>
<span class="normal"> 564</span>
<span class="normal"> 565</span>
<span class="normal"> 566</span>
<span class="normal"> 567</span>
<span class="normal"> 568</span>
<span class="normal"> 569</span>
<span class="normal"> 570</span>
<span class="normal"> 571</span>
<span class="normal"> 572</span>
<span class="normal"> 573</span>
<span class="normal"> 574</span>
<span class="normal"> 575</span>
<span class="normal"> 576</span>
<span class="normal"> 577</span>
<span class="normal"> 578</span>
<span class="normal"> 579</span>
<span class="normal"> 580</span>
<span class="normal"> 581</span>
<span class="normal"> 582</span>
<span class="normal"> 583</span>
<span class="normal"> 584</span>
<span class="normal"> 585</span>
<span class="normal"> 586</span>
<span class="normal"> 587</span>
<span class="normal"> 588</span>
<span class="normal"> 589</span>
<span class="normal"> 590</span>
<span class="normal"> 591</span>
<span class="normal"> 592</span>
<span class="normal"> 593</span>
<span class="normal"> 594</span>
<span class="normal"> 595</span>
<span class="normal"> 596</span>
<span class="normal"> 597</span>
<span class="normal"> 598</span>
<span class="normal"> 599</span>
<span class="normal"> 600</span>
<span class="normal"> 601</span>
<span class="normal"> 602</span>
<span class="normal"> 603</span>
<span class="normal"> 604</span>
<span class="normal"> 605</span>
<span class="normal"> 606</span>
<span class="normal"> 607</span>
<span class="normal"> 608</span>
<span class="normal"> 609</span>
<span class="normal"> 610</span>
<span class="normal"> 611</span>
<span class="normal"> 612</span>
<span class="normal"> 613</span>
<span class="normal"> 614</span>
<span class="normal"> 615</span>
<span class="normal"> 616</span>
<span class="normal"> 617</span>
<span class="normal"> 618</span>
<span class="normal"> 619</span>
<span class="normal"> 620</span>
<span class="normal"> 621</span>
<span class="normal"> 622</span>
<span class="normal"> 623</span>
<span class="normal"> 624</span>
<span class="normal"> 625</span>
<span class="normal"> 626</span>
<span class="normal"> 627</span>
<span class="normal"> 628</span>
<span class="normal"> 629</span>
<span class="normal"> 630</span>
<span class="normal"> 631</span>
<span class="normal"> 632</span>
<span class="normal"> 633</span>
<span class="normal"> 634</span>
<span class="normal"> 635</span>
<span class="normal"> 636</span>
<span class="normal"> 637</span>
<span class="normal"> 638</span>
<span class="normal"> 639</span>
<span class="normal"> 640</span>
<span class="normal"> 641</span>
<span class="normal"> 642</span>
<span class="normal"> 643</span>
<span class="normal"> 644</span>
<span class="normal"> 645</span>
<span class="normal"> 646</span>
<span class="normal"> 647</span>
<span class="normal"> 648</span>
<span class="normal"> 649</span>
<span class="normal"> 650</span>
<span class="normal"> 651</span>
<span class="normal"> 652</span>
<span class="normal"> 653</span>
<span class="normal"> 654</span>
<span class="normal"> 655</span>
<span class="normal"> 656</span>
<span class="normal"> 657</span>
<span class="normal"> 658</span>
<span class="normal"> 659</span>
<span class="normal"> 660</span>
<span class="normal"> 661</span>
<span class="normal"> 662</span>
<span class="normal"> 663</span>
<span class="normal"> 664</span>
<span class="normal"> 665</span>
<span class="normal"> 666</span>
<span class="normal"> 667</span>
<span class="normal"> 668</span>
<span class="normal"> 669</span>
<span class="normal"> 670</span>
<span class="normal"> 671</span>
<span class="normal"> 672</span>
<span class="normal"> 673</span>
<span class="normal"> 674</span>
<span class="normal"> 675</span>
<span class="normal"> 676</span>
<span class="normal"> 677</span>
<span class="normal"> 678</span>
<span class="normal"> 679</span>
<span class="normal"> 680</span>
<span class="normal"> 681</span>
<span class="normal"> 682</span>
<span class="normal"> 683</span>
<span class="normal"> 684</span>
<span class="normal"> 685</span>
<span class="normal"> 686</span>
<span class="normal"> 687</span>
<span class="normal"> 688</span>
<span class="normal"> 689</span>
<span class="normal"> 690</span>
<span class="normal"> 691</span>
<span class="normal"> 692</span>
<span class="normal"> 693</span>
<span class="normal"> 694</span>
<span class="normal"> 695</span>
<span class="normal"> 696</span>
<span class="normal"> 697</span>
<span class="normal"> 698</span>
<span class="normal"> 699</span>
<span class="normal"> 700</span>
<span class="normal"> 701</span>
<span class="normal"> 702</span>
<span class="normal"> 703</span>
<span class="normal"> 704</span>
<span class="normal"> 705</span>
<span class="normal"> 706</span>
<span class="normal"> 707</span>
<span class="normal"> 708</span>
<span class="normal"> 709</span>
<span class="normal"> 710</span>
<span class="normal"> 711</span>
<span class="normal"> 712</span>
<span class="normal"> 713</span>
<span class="normal"> 714</span>
<span class="normal"> 715</span>
<span class="normal"> 716</span>
<span class="normal"> 717</span>
<span class="normal"> 718</span>
<span class="normal"> 719</span>
<span class="normal"> 720</span>
<span class="normal"> 721</span>
<span class="normal"> 722</span>
<span class="normal"> 723</span>
<span class="normal"> 724</span>
<span class="normal"> 725</span>
<span class="normal"> 726</span>
<span class="normal"> 727</span>
<span class="normal"> 728</span>
<span class="normal"> 729</span>
<span class="normal"> 730</span>
<span class="normal"> 731</span>
<span class="normal"> 732</span>
<span class="normal"> 733</span>
<span class="normal"> 734</span>
<span class="normal"> 735</span>
<span class="normal"> 736</span>
<span class="normal"> 737</span>
<span class="normal"> 738</span>
<span class="normal"> 739</span>
<span class="normal"> 740</span>
<span class="normal"> 741</span>
<span class="normal"> 742</span>
<span class="normal"> 743</span>
<span class="normal"> 744</span>
<span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">HookedModel</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is a wrapper around the huggingface model that allows to extract the activations of the model. It is support</span>
<span class="sd">    advanced mechanistic intepretability methods like ablation, patching, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">HookedModelConfig</span><span class="p">,</span> <span class="n">log_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span>
            <span class="n">logname</span><span class="o">=</span><span class="s2">&quot;HookedModel&quot;</span><span class="p">,</span>
            <span class="n">level</span><span class="o">=</span><span class="s2">&quot;info&quot;</span><span class="p">,</span>
            <span class="n">log_file_path</span><span class="o">=</span><span class="n">log_file_path</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_language_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ModelFactory</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
                <span class="n">model_name</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                <span class="n">device_map</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_map</span><span class="p">,</span>
                <span class="n">torch_dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
                <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;eager&quot;</span>
                <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_implementation</span> <span class="o">==</span> <span class="s2">&quot;custom_eager&quot;</span>
                <span class="k">else</span> <span class="n">config</span><span class="o">.</span><span class="n">attn_implementation</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_wrapper_manager</span> <span class="o">=</span> <span class="n">ModuleWrapperManager</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">)</span>

        <span class="n">tokenizer</span><span class="p">,</span> <span class="n">processor</span> <span class="o">=</span> <span class="n">TokenizerFactory</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">torch_dtype</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
            <span class="n">device_map</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_map</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span> <span class="o">=</span> <span class="n">InputHandler</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">processor</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="o">=</span> <span class="n">tokenizer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">text_tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">text_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="n">device_num</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Model loaded in </span><span class="si">{</span><span class="n">device_num</span><span class="si">}</span><span class="s2"> devices. First device: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_type_to_hook_name</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;resid_in&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_input_hook_name</span><span class="p">,</span>
            <span class="s2">&quot;resid_out&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_hook_name</span><span class="p">,</span>
            <span class="s2">&quot;resid_mid&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">intermediate_stream_hook_name</span><span class="p">,</span>
            <span class="s2">&quot;attn_out&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_hook_name</span><span class="p">,</span>
            <span class="s2">&quot;attn_in&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_in_hook_name</span><span class="p">,</span>
            <span class="s2">&quot;values&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_value_hook_name</span><span class="p">,</span>
            <span class="c1"># Add other act_types if needed</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assert_all_modules_exist</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_implementation</span> <span class="o">==</span> <span class="s2">&quot;custom_eager&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">                            The model is using the custom eager attention implementation that support attention matrix hooks because I get config.attn_impelemntation == &#39;custom_eager&#39;. If you don&#39;t want this, you can call HookedModel.restore_original_modules. </span>
<span class="sd">                            However, we reccomend using this implementation since the base one do not contains attention matrix hook resulting in unexpected behaviours. </span>
<span class="sd">                            &quot;&quot;&quot;</span><span class="p">,</span>
                <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_custom_modules</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;HookedModel(model_name=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">):</span>
<span class="s2">        </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span><span class="si">}</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">HookedModelConfig</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">assert_module_exists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="c1"># Remove &#39;.input&#39; or &#39;.output&#39; from the component</span>
        <span class="n">component</span> <span class="o">=</span> <span class="n">component</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.input&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.output&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># Check if &#39;{}&#39; is in the component, indicating layer indexing</span>
        <span class="k">if</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span> <span class="ow">in</span> <span class="n">component</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">):</span>
                <span class="n">attr_name</span> <span class="o">=</span> <span class="n">component</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

                <span class="k">try</span><span class="p">:</span>
                    <span class="n">get_attribute_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_wrapper_manager</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">set_custom_modules</span><span class="p">()</span>
                            <span class="n">get_attribute_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">restore_original_modules</span><span class="p">()</span>
                    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Component &#39;</span><span class="si">{</span><span class="n">attr_name</span><span class="si">}</span><span class="s2">&#39; does not exist in the model. Please check the model configuration.&quot;</span>
                        <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">get_attribute_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Component &#39;</span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">&#39; does not exist in the model. Please check the model configuration.&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="nf">assert_all_modules_exist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># get the list of all attributes of model_config</span>
        <span class="n">all_attributes</span> <span class="o">=</span> <span class="p">[</span><span class="n">attr_name</span> <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
        <span class="c1"># save just the attributes that have &quot;hook&quot; in the name</span>
        <span class="n">hook_attributes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">attr_name</span> <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">all_attributes</span> <span class="k">if</span> <span class="s2">&quot;hook&quot;</span> <span class="ow">in</span> <span class="n">attr_name</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">hook_attribute</span> <span class="ow">in</span> <span class="n">hook_attributes</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assert_module_exists</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span> <span class="n">hook_attribute</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_custom_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Setting custom modules.&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_wrapper_manager</span><span class="o">.</span><span class="n">substitute_attention_module</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">restore_original_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Restoring original modules.&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_wrapper_manager</span><span class="o">.</span><span class="n">restore_original_attention_module</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">use_full_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using full model capabilities&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using full text only model capabilities&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">use_language_model_only</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_language_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The model does not have a separate language model that can be used&quot;</span><span class="p">,</span>
                <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_language_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using only language model capabilities&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span>

    <span class="k">def</span> <span class="nf">get_text_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer</span>

<span class="sd">        Args:</span>
<span class="sd">            None</span>

<span class="sd">        Returns:</span>
<span class="sd">            tokenizer: the tokenizer of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The processor does not have a tokenizer&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span>  <span class="c1"># type: ignore</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span>

    <span class="k">def</span> <span class="nf">get_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the processor of the model (None if the model does not have a processor, i.e. text only model)</span>

<span class="sd">        Args:</span>
<span class="sd">            None</span>

<span class="sd">        Returns:</span>
<span class="sd">            processor: the processor of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The model does not have a processor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span>

    <span class="k">def</span> <span class="nf">get_lm_head</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">get_attribute_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">unembed_matrix</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_last_layernorm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">get_attribute_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">last_layernorm</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the model in evaluation mode</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the device of the model. If the model is in multiple devices, it will return the first device</span>

<span class="sd">        Args:</span>
<span class="sd">            None</span>

<span class="sd">        Returns:</span>
<span class="sd">            device: the device of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span>

    <span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a new hook to the model. The hook will be called in the forward pass of the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            component (str): the component of the model where the hook will be added.</span>
<span class="sd">            hook_function (Callable): the function that will be called in the forward pass of the model. The function must have the following signature:</span>
<span class="sd">                def hook_function(module, input, output):</span>
<span class="sd">                    pass</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; def hook_function(module, input, output):</span>
<span class="sd">            &gt;&gt;&gt;     # your code here</span>
<span class="sd">            &gt;&gt;&gt;     pass</span>
<span class="sd">            &gt;&gt;&gt; model.register_forward_hook(&quot;model.layers[0].self_attn&quot;, hook_function)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="n">component</span><span class="p">,</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">hook_function</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_string_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform a list or a tensor of tokens in a list of string tokens.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokens (Union[list, torch.Tensor]): the tokens to transform in string tokens</span>

<span class="sd">        Returns:</span>
<span class="sd">            string_tokens (list): the list of string tokens</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; tokens = [101, 1234, 1235, 102]</span>
<span class="sd">            &gt;&gt;&gt; model.to_string_tokens(tokens)</span>
<span class="sd">            [&#39;[CLS]&#39;, &#39;hello&#39;, &#39;world&#39;, &#39;[SEP]&#39;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">tokens</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">string_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="n">string_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
        <span class="k">return</span> <span class="n">string_tokens</span>

    <span class="k">def</span> <span class="nf">create_hooks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">:</span> <span class="n">ActivationCache</span><span class="p">,</span>
        <span class="n">token_index</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
        <span class="n">token_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
        <span class="c1"># string_tokens: List[str],</span>
        <span class="n">extraction_config</span><span class="p">:</span> <span class="n">ExtractionConfig</span> <span class="o">=</span> <span class="n">ExtractionConfig</span><span class="p">(),</span>
        <span class="n">patching_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">external_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ActivationCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</span>
<span class="sd">            cache (ActivationCache): dictionary where the activations of the model will be saved</span>
<span class="sd">            extracted_token_position (list[str]): list of tokens to extract the activations from ([&quot;last&quot;, &quot;end-image&quot;, &quot;start-image&quot;, &quot;first&quot;])</span>
<span class="sd">            string_tokens (list[str]): list of string tokens</span>
<span class="sd">            pivot_positions (Optional[list[int]]): list of split positions of the tokens</span>
<span class="sd">            extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model (default = ExtractionConfig())</span>
<span class="sd">            ablation_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the ablation queries to perform during forward pass</span>
<span class="sd">            patching_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the patching queries to perform during forward pass</span>
<span class="sd">            batch_idx (Optional[int]): index of the batch in the dataloader</span>
<span class="sd">            external_cache (Optional[ActivationCache]): external cache to use in the forward pass</span>

<span class="sd">        Returns:</span>
<span class="sd">            hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">hooks</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># compute layer and head indexes</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span>
        <span class="p">):</span>
            <span class="n">layer_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)]</span>
            <span class="n">head_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">layer_head_indexes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">el</span><span class="p">[</span><span class="s2">&quot;layer&quot;</span><span class="p">],</span> <span class="n">el</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span>
            <span class="p">]</span>
            <span class="n">layer_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">el</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">layer_head_indexes</span><span class="p">]</span>
            <span class="n">head_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">el</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">layer_head_indexes</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;attn_heads must be &#39;all&#39; or a list of dictionaries as [{&#39;layer&#39;: 0, &#39;head&#39;: 0}]&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_out</span><span class="p">:</span>
            <span class="c1"># assert that the component exists in the model</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_out_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_in</span><span class="p">:</span>
            <span class="c1"># assert that the component exists in the model</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_input_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">i</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_in_post_layernorm</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_input_post_layernorm_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">i</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_in_post_layernorm_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">save_input_ids</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">embed_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_queries</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_query_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">query_key_value_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;queries_&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                        <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                        <span class="n">num_key_value_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_values</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_value_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">query_key_value_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;values_&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                        <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                        <span class="n">num_key_value_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_keys</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_key_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">query_key_value_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;keys_&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                        <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                        <span class="n">num_key_value_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_out</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_o_proj_input_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">i</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">head_out_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;head_out_&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                        <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                        <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                        <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                        <span class="n">o_proj_weight</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_weight</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                        <span class="p">),</span>
                        <span class="n">o_proj_bias</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_bias</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                        <span class="p">),</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_attn_in</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_in_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;attn_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_attn_out</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;attn_out_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="c1"># if extraction_config.extract_avg:</span>
        <span class="c1">#     # Define a hook that saves the activations of the residual stream</span>
        <span class="c1">#     raise NotImplementedError(</span>
        <span class="c1">#         &quot;The hook for the average is not working with token_index as a list&quot;</span>
        <span class="c1">#     )</span>

        <span class="c1">#     # hooks.extend(</span>
        <span class="c1">#     #     [</span>
        <span class="c1">#     #         {</span>
        <span class="c1">#     #             &quot;component&quot;: self.model_config.residual_stream_hook_name.format(</span>
        <span class="c1">#     #                 i</span>
        <span class="c1">#     #             ),</span>
        <span class="c1">#     #             &quot;intervention&quot;: partial(</span>
        <span class="c1">#     #                 avg_hook,</span>
        <span class="c1">#     #                 cache=cache,</span>
        <span class="c1">#     #                 cache_key=&quot;resid_avg_{}&quot;.format(i),</span>
        <span class="c1">#     #                 last_image_idx=last_image_idxs, #type</span>
        <span class="c1">#     #                 end_image_idx=end_image_idxs,</span>
        <span class="c1">#     #             ),</span>
        <span class="c1">#     #         }</span>
        <span class="c1">#     #         for i in range(0, self.model_config.num_hidden_layers)</span>
        <span class="c1">#     #     ]</span>
        <span class="c1">#     # )</span>
        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_mid</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">intermediate_stream_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">i</span>
                    <span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_mid_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>

            <span class="c1"># if we want to extract the output of the heads</span>
        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_mlp_out</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">mlp_out_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">save_resid_hook</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;mlp_out_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="c1"># PATCHING</span>
        <span class="k">if</span> <span class="n">patching_queries</span><span class="p">:</span>
            <span class="n">token_to_pos</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">map_token_to_pos</span><span class="p">,</span>
                <span class="n">_get_token_index</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
                <span class="c1"># string_tokens=string_tokens,</span>
                <span class="n">hf_tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">patching_queries</span> <span class="o">=</span> <span class="n">preprocess_patching_queries</span><span class="p">(</span>
                <span class="n">patching_queries</span><span class="o">=</span><span class="n">patching_queries</span><span class="p">,</span>
                <span class="n">map_token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span><span class="p">,</span>
                <span class="n">model_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">def</span> <span class="nf">make_patch_tokens_hook</span><span class="p">(</span><span class="n">patching_queries_subset</span><span class="p">):</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">                Creates a hook function to patch the activations in the</span>
<span class="sd">                current forward pass.</span>
<span class="sd">                &quot;&quot;&quot;</span>

                <span class="k">def</span> <span class="nf">patch_tokens_hook</span><span class="p">(</span>
                    <span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span>
                <span class="p">):</span>  <span class="c1"># TODO: Move to hook.py</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
                    <span class="c1"># Modify the tensor without affecting the computation graph</span>
                    <span class="n">act_to_patch</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">patch</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                        <span class="n">patching_queries_subset</span><span class="p">[</span><span class="s2">&quot;pos_token_to_patch&quot;</span><span class="p">],</span>
                        <span class="n">patching_queries_subset</span><span class="p">[</span><span class="s2">&quot;patching_activations&quot;</span><span class="p">],</span>
                    <span class="p">):</span>
                        <span class="n">act_to_patch</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">patching_queries_subset</span><span class="p">[</span>
                            <span class="s2">&quot;patching_activations&quot;</span>
                        <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                    <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                            <span class="k">return</span> <span class="p">(</span><span class="n">act_to_patch</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                        <span class="k">elif</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">return</span> <span class="n">act_to_patch</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                            <span class="k">return</span> <span class="p">(</span><span class="n">act_to_patch</span><span class="p">,</span> <span class="o">*</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                        <span class="k">elif</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="k">return</span> <span class="n">act_to_patch</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No output or input found&quot;</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">patch_tokens_hook</span>

            <span class="c1"># Group the patching queries by &#39;layer&#39; and &#39;act_type&#39;</span>
            <span class="n">grouped_queries</span> <span class="o">=</span> <span class="n">patching_queries</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;layer&quot;</span><span class="p">,</span> <span class="s2">&quot;activation_type&quot;</span><span class="p">])</span>

            <span class="k">for</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">act_type</span><span class="p">),</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">grouped_queries</span><span class="p">:</span>
                <span class="n">hook_name_template</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_type_to_hook_name</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                    <span class="n">act_type</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
                <span class="p">)</span>  <span class="c1"># -3 because we remove {}</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">hook_name_template</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown activation type: </span><span class="si">{</span><span class="n">act_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="c1"># continue  # Skip unknown activation types</span>

                <span class="n">hook_name</span> <span class="o">=</span> <span class="n">hook_name_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
                <span class="n">hook_function</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">make_patch_tokens_hook</span><span class="p">(</span><span class="n">group</span><span class="p">))</span>

                <span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="n">hook_name</span><span class="p">,</span>
                        <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">hook_function</span><span class="p">,</span>
                    <span class="p">}</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="n">ablation_queries</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: debug and test the ablation. Check with ale</span>
            <span class="n">token_to_pos</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">map_token_to_pos</span><span class="p">,</span>
                <span class="n">_get_token_index</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
                <span class="c1"># string_tokens=string_tokens,</span>
                <span class="n">hf_tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Ablation is not supported with batch size &gt; 1&quot;</span><span class="p">)</span>
            <span class="n">ablation_manager</span> <span class="o">=</span> <span class="n">AblationManager</span><span class="p">(</span>
                <span class="n">model_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
                <span class="n">token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">model_attn_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_implementation</span><span class="p">,</span>
                <span class="n">ablation_queries</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ablation_queries</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ablation_queries</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="k">else</span> <span class="n">ablation_queries</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">hooks</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ablation_manager</span><span class="o">.</span><span class="n">main</span><span class="p">())</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_values_projected</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_value_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">projected_value_vectors_head</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                        <span class="n">num_attention_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                        <span class="n">num_key_value_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span>
                        <span class="n">hidden_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                        <span class="n">d_head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                        <span class="n">out_proj_weight</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_weight</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">out_proj_bias</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_bias</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                        <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_attn_pattern</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">external_cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                        </span><span class="sd">&quot;&quot;&quot;The external_cache is None. The average could not be computed since missing an external cache where store the iterations.</span>
<span class="sd">                        &quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="n">batch_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                        </span><span class="sd">&quot;&quot;&quot;The batch_idx is None. The average could not be computed since missing the batch index.</span>

<span class="sd">                        &quot;&quot;&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># move the cache to the same device of the model</span>
                    <span class="n">external_cache</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>
                    <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_matrix_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="n">i</span>
                            <span class="p">),</span>
                            <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                                <span class="n">avg_attention_pattern_head</span><span class="p">,</span>
                                <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                                <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                                <span class="n">attn_pattern_current_avg</span><span class="o">=</span><span class="n">external_cache</span><span class="p">,</span>
                                <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
                                <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                                <span class="c1"># avg=extraction_config.avg,</span>
                                <span class="n">extract_avg_value</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_values_projected</span><span class="p">,</span>
                            <span class="p">),</span>
                        <span class="p">}</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
                    <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_matrix_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                        <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                            <span class="n">attention_pattern_head</span><span class="p">,</span>
                            <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                            <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                            <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                            <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                        <span class="p">),</span>
                    <span class="p">}</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
                <span class="p">]</span>

            <span class="c1"># if additional hooks are not empty, add them to the hooks list</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span>
        <span class="k">return</span> <span class="n">hooks</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;last&quot;</span><span class="p">],</span>
        <span class="n">pivot_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">extraction_config</span><span class="p">:</span> <span class="n">ExtractionConfig</span> <span class="o">=</span> <span class="n">ExtractionConfig</span><span class="p">(),</span>
        <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">patching_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">external_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ActivationCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attn_heads</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">move_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</span>
<span class="sd">            target_token_positions (list[str]): list of tokens to extract the activations from ([&quot;last&quot;, &quot;end-image&quot;, &quot;start-image&quot;, &quot;first&quot;])</span>
<span class="sd">            pivot_positions (Optional[list[int]]): list of split positions of the tokens</span>
<span class="sd">            extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model</span>
<span class="sd">            ablation_queries (Optional[pd.DataFrame | None]): dataframe with the ablation queries to perform during forward pass</span>
<span class="sd">            patching_queries (Optional[pd.DataFrame | None]): dataframe with the patching queries to perform during forward pass</span>
<span class="sd">            external_cache (Optional[ActivationCache]): external cache to use in the forward pass</span>
<span class="sd">            attn_heads (Union[list[dict], Literal[&quot;all&quot;]]): list of dictionaries with the layer and head to extract the attention pattern or &#39;all&#39; to</span>
<span class="sd">            batch_idx (Optional[int]): index of the batch in the dataloader</span>
<span class="sd">            move_to_cpu (bool): if True, move the activations to the cpu</span>

<span class="sd">        Returns:</span>
<span class="sd">            cache (ActivationCache): dictionary with the activations of the model</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; inputs = {&quot;input_ids&quot;: torch.tensor([[101, 1234, 1235, 102]]), &quot;attention_mask&quot;: torch.tensor([[1, 1, 1, 1]])}</span>
<span class="sd">            &gt;&gt;&gt; model.forward(inputs, target_token_positions=[&quot;last&quot;], extract_resid_out=True)</span>
<span class="sd">            {&#39;resid_out_0&#39;: tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), &#39;input_ids&#39;: tensor([[101, 1234, 1235, 102]]), &#39;mapping_index&#39;: {&#39;last&#39;: [0]}}</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">target_token_positions</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">is_not_empty</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;target_token_positions must be passed if we want to extract the activations of the model&quot;</span>
            <span class="p">)</span>
        <span class="n">cache</span> <span class="o">=</span> <span class="n">ActivationCache</span><span class="p">()</span>
        <span class="n">string_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_string_tokens</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">get_input_ids</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">token_index</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">TokenIndex</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">pivot_positions</span><span class="o">=</span><span class="n">pivot_positions</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get_token_index</span><span class="p">(</span>
            <span class="n">tokens</span><span class="o">=</span><span class="n">target_token_positions</span><span class="p">,</span>
            <span class="n">string_tokens</span><span class="o">=</span><span class="n">string_tokens</span><span class="p">,</span>
            <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_index</span><span class="p">,</span> <span class="nb">list</span><span class="p">),</span> <span class="s2">&quot;Token index must be a list&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span> <span class="s2">&quot;Token dict must be a dict&quot;</span>

        <span class="n">hooks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_hooks</span><span class="p">(</span>  <span class="c1"># TODO: add **kwargs</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">token_dict</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
            <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
            <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
            <span class="n">extraction_config</span><span class="o">=</span><span class="n">extraction_config</span><span class="p">,</span>
            <span class="n">ablation_queries</span><span class="o">=</span><span class="n">ablation_queries</span><span class="p">,</span>
            <span class="n">patching_queries</span><span class="o">=</span><span class="n">patching_queries</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
            <span class="n">external_cache</span><span class="o">=</span><span class="n">external_cache</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hook_handlers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hooks</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span>
        <span class="p">)</span>
        <span class="c1"># forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">(</span>
            <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
            <span class="c1"># output_original_output=True,</span>
            <span class="c1"># output_attentions=extract_attn_pattern,</span>
        <span class="p">)</span>

        <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># since attention_patterns are returned in the output, we need to adapt to the cache structure</span>
        <span class="k">if</span> <span class="n">move_to_cpu</span><span class="p">:</span>
            <span class="n">cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">external_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">external_cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="n">mapping_index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">target_token_positions</span><span class="p">:</span>
            <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
                <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_dict</span><span class="p">[</span><span class="n">token</span><span class="p">])):</span>
                    <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
                    <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_dict</span><span class="p">)):</span>
                    <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
                    <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Token dict must be an int, a dict or a list&quot;</span><span class="p">)</span>
        <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;mapping_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_index</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">remove_hooks</span><span class="p">(</span><span class="n">hook_handlers</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cache</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Call the forward method of the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">topk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="c1"># return a dictionary with the topk tokens and their probabilities</span>
        <span class="n">string_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_string_tokens</span><span class="p">(</span><span class="n">topk</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
        <span class="n">token_probs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">string_tokens</span><span class="p">,</span> <span class="n">topk</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_probs</span><span class="p">:</span>
                <span class="n">token_probs</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">token_probs</span>
        <span class="c1"># return {</span>
        <span class="c1">#     token: prob.item() for token, prob in zip(string_tokens, topk.values)</span>
        <span class="c1"># }</span>

    <span class="k">def</span> <span class="nf">get_module_from_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a module from the model given the string of the module.</span>

<span class="sd">        Args:</span>
<span class="sd">            component (str): the string of the module</span>

<span class="sd">        Returns:</span>
<span class="sd">            module (torch.nn.Module): the module of the model</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; model.get_module_from_string(&quot;model.layers[0].self_attn&quot;)</span>
<span class="sd">            BertAttention(...)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">retrieve_modules_from_names</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hooks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the hooks in the model</span>

<span class="sd">        Args:</span>
<span class="sd">            hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model</span>

<span class="sd">        Returns:</span>
<span class="sd">            hook_handlers (list): list of hook handlers</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

        <span class="n">hook_handlers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="n">hooks</span><span class="p">:</span>
            <span class="n">component</span> <span class="o">=</span> <span class="n">hook</span><span class="p">[</span><span class="s2">&quot;component&quot;</span><span class="p">]</span>
            <span class="n">hook_function</span> <span class="o">=</span> <span class="n">hook</span><span class="p">[</span><span class="s2">&quot;intervention&quot;</span><span class="p">]</span>

            <span class="c1"># get the last module string (.input or .output) and remove it from the component string</span>
            <span class="n">last_module</span> <span class="o">=</span> <span class="n">component</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># now remove the last module from the component string</span>
            <span class="n">component</span> <span class="o">=</span> <span class="n">component</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">last_module</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="c1"># check if the component exists in the model</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">assert_module_exists</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Probably the module </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2"> do not exists in the model. If the module is the attention_matrix_hook, try callig HookedModel.set_custom_hooks() or setting attn_implementation == &#39;custom_eager&#39;.  Now we will skip the hook for the component </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">last_module</span> <span class="o">==</span> <span class="s2">&quot;input&quot;</span><span class="p">:</span>
                <span class="n">hook_handlers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">get_module_by_path</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">component</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span>
                        <span class="n">partial</span><span class="p">(</span><span class="n">hook_function</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">last_module</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span>
                <span class="n">hook_handlers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">get_module_by_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
                        <span class="n">hook_function</span><span class="p">,</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">)</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">hook_handlers</span>

    <span class="k">def</span> <span class="nf">remove_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook_handlers</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove all the hooks from the model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">hook_handler</span> <span class="ow">in</span> <span class="n">hook_handlers</span><span class="p">:</span>
            <span class="n">hook_handler</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_text</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        __WARNING__: This method could be buggy in the return dict of the output. Pay attention!</span>

<span class="sd">        Generate new tokens using the model and the inputs passed as argument</span>
<span class="sd">        Args:</span>
<span class="sd">            inputs (dict): dictionary with the inputs of the model {&quot;input_ids&quot;: ..., &quot;attention_mask&quot;: ..., &quot;pixel_values&quot;: ...}</span>
<span class="sd">            generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration</span>
<span class="sd">            **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)</span>
<span class="sd">        Returns:</span>
<span class="sd">            output (ActivationCache): dictionary with the output of the model</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; inputs = {&quot;input_ids&quot;: torch.tensor([[101, 1234, 1235, 102]]), &quot;attention_mask&quot;: torch.tensor([[1, 1, 1, 1]])}</span>
<span class="sd">            &gt;&gt;&gt; model.generate(inputs)</span>
<span class="sd">            {&#39;sequences&#39;: tensor([[101, 1234, 1235, 102]])}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize cache for logits</span>
        <span class="c1"># TODO FIX THIS. IT is not general and it is not working</span>
        <span class="c1"># raise NotImplementedError(&quot;This method is not working. It needs to be fixed&quot;)</span>
        <span class="n">hook_handlers</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">target_token_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">string_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_string_tokens</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">get_input_ids</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">token_index</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">TokenIndex</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">pivot_positions</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_token_index</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="p">[],</span> <span class="n">string_tokens</span><span class="o">=</span><span class="n">string_tokens</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_index</span><span class="p">,</span> <span class="nb">list</span><span class="p">),</span> <span class="s2">&quot;Token index must be a list&quot;</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span> <span class="s2">&quot;Token dict must be a dict&quot;</span>
            <span class="n">hooks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_hooks</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">token_dict</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
                <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                <span class="n">cache</span><span class="o">=</span><span class="n">ActivationCache</span><span class="p">(),</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">hook_handlers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hooks</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">output_scores</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">hook_handlers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">remove_hooks</span><span class="p">(</span><span class="n">hook_handlers</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_text</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">return</span> <span class="n">output</span>  <span class="c1"># type: ignore</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">extract_cache</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataloader</span><span class="p">,</span>
        <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">batch_saver</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">move_to_cpu_after_forward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># save_other_batch_elements: bool = False,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataloader (iterable): dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)</span>
<span class="sd">            extracted_token_position (list[str]): list of tokens to extract the activations from ([&quot;last&quot;, &quot;end-image&quot;, &quot;start-image&quot;, &quot;first&quot;])</span>
<span class="sd">            batch_saver (Callable): function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)</span>
<span class="sd">            move_to_cpu_after_forward (bool): if True, move the activations to the cpu right after the any forward pass of the model</span>
<span class="sd">            **kwargs: additional arguments to control hooks generation, basically accept any argument handled by the `.forward` method (i.e. ablation_queries, patching_queries, extract_resid_in)</span>

<span class="sd">        Returns:</span>
<span class="sd">            final_cache: dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; dataloader = [{&quot;input_ids&quot;: torch.tensor([[101, 1234, 1235, 102]]), &quot;attention_mask&quot;: torch.tensor([[1, 1, 1, 1]]), &quot;labels&quot;: torch.tensor([1])}, ...]</span>
<span class="sd">            &gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[&quot;last&quot;], batch_saver=lambda x: {&quot;labels&quot;: x[&quot;labels&quot;]})</span>
<span class="sd">            {&#39;resid_out_0&#39;: tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), &#39;labels&#39;: tensor([1]), &#39;mapping_index&#39;: {&#39;last&#39;: [0]}}</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Extracting cache&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># get the function to save in the cache the additional element from the batch sime</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Forward pass started&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">all_cache</span> <span class="o">=</span> <span class="n">ActivationCache</span><span class="p">()</span>  <span class="c1"># a list of dictoionaries, each dictionary contains the activations of the model for a batch (so a dict of tensors)</span>
        <span class="n">attn_pattern</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ActivationCache</span><span class="p">()</span>
        <span class="p">)</span>  <span class="c1"># Initialize the dictionary to hold running averages</span>

        <span class="n">example_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">n_batches</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Initialize batch counter</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Extracting cache:&quot;</span><span class="p">):</span>
            <span class="c1"># log_memory_usage(&quot;Extract cache - Before batch&quot;)</span>
            <span class="c1"># tokens, others = batch</span>
            <span class="c1"># inputs = {k: v.to(self.first_device) for k, v in tokens.items()}</span>

            <span class="c1"># get input_ids, attention_mask, and if available, pixel_values from batch (that is a dictionary)</span>
            <span class="c1"># then move them to the first device</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>
            <span class="n">others</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">}</span>

            <span class="n">cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">target_token_positions</span><span class="o">=</span><span class="n">target_token_positions</span><span class="p">,</span>
                <span class="n">pivot_positions</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">external_cache</span><span class="o">=</span><span class="n">attn_pattern</span><span class="p">,</span>
                <span class="n">batch_idx</span><span class="o">=</span><span class="n">n_batches</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># possible memory leak from here -___---------------&gt;</span>
            <span class="n">additional_dict</span> <span class="o">=</span> <span class="n">batch_saver</span><span class="p">(</span><span class="n">others</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">additional_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># cache = {**cache, **additional_dict}</span>
                <span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">additional_dict</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">move_to_cpu_after_forward</span><span class="p">:</span>
                <span class="n">cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

            <span class="n">n_batches</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Increment batch counter# Process and remove &quot;pattern_&quot; keys from cache</span>
            <span class="n">all_cache</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>

            <span class="k">del</span> <span class="n">cache</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">inputs</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Forward pass finished - started to aggregate different batch&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">all_cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">attn_pattern</span><span class="p">)</span>
        <span class="n">all_cache</span><span class="p">[</span><span class="s2">&quot;example_dict&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">example_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Aggregation finished&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">all_cache</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">compute_patching</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="c1"># counterfactual_dataset,</span>
        <span class="n">base_dataloader</span><span class="p">,</span>
        <span class="n">target_dataloader</span><span class="p">,</span>
        <span class="n">patching_query</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;patching_elem&quot;</span><span class="p">:</span> <span class="s2">&quot;@end-image&quot;</span><span class="p">,</span>
                <span class="s2">&quot;layers_to_patch&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
                <span class="s2">&quot;activation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;resid_in_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">],</span>
        <span class="n">base_dictonary_idxs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">target_dictonary_idxs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_logit_diff</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">batch_saver</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method for activation patching. This substitutes the activations of the model</span>
<span class="sd">        with the activations of the counterfactual dataset.</span>

<span class="sd">        It performs three forward passes:</span>
<span class="sd">        1. Forward pass on the base dataset to extract the activations of the model (cat).</span>
<span class="sd">        2. Forward pass on the target dataset to extract clean logits (dog)</span>
<span class="sd">        [to compare against the patched logits].</span>
<span class="sd">        3. Forward pass on the target dataset to patch (cat) into (dog)</span>
<span class="sd">        and extract the patched logits.</span>

<span class="sd">        Args:</span>
<span class="sd">            target_token_positions (list[str]): List of tokens to extract the activations from.</span>
<span class="sd">            base_dataloader (torch.utils.data.DataLoader): Dataloader with the base dataset. (dataset where we sample the activations from)</span>
<span class="sd">            target_dataloader (torch.utils.data.DataLoader): Dataloader with the target dataset. (dataset where we patch the activations)</span>
<span class="sd">            patching_query (list[dict]): List of dictionaries with the patching queries. Each dictionary must have the keys &quot;patching_elem&quot;, &quot;layers_to_patch&quot; and &quot;activation_type&quot;. The &quot;patching_elem&quot; is the token to patch, the &quot;layers_to_patch&quot; is the list of layers to patch and the &quot;activation_type&quot; is the type of the activation to patch. The activation type must be one of the following: &quot;resid_in_{}&quot;, &quot;resid_out_{}&quot;, &quot;resid_mid_{}&quot;, &quot;attn_in_{}&quot;, &quot;attn_out_{}&quot;, &quot;values_{}&quot;. The &quot;{}&quot; will be replaced with the layer index.</span>
<span class="sd">            base_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It&#39;s useful to extract the logit difference between the clean logits and the patched logits.</span>
<span class="sd">            target_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It&#39;s useful to extract the logit difference between the clean logits and the patched logits.</span>
<span class="sd">            return_logit_diff (bool): If True, it will return the logit difference between the clean logits and the patched logits.</span>


<span class="sd">        Returns:</span>
<span class="sd">            final_cache (ActivationCache): dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; model.compute_patching(</span>
<span class="sd">            &gt;&gt;&gt;     target_token_positions=[&quot;end-image&quot;, &quot; last&quot;],</span>
<span class="sd">            &gt;&gt;&gt;     base_dataloader=base_dataloader,</span>
<span class="sd">            &gt;&gt;&gt;     target_dataloader=target_dataloader,</span>
<span class="sd">            &gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,</span>
<span class="sd">            &gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,</span>
<span class="sd">            &gt;&gt;&gt;     patching_query=[</span>
<span class="sd">            &gt;&gt;&gt;         {</span>
<span class="sd">            &gt;&gt;&gt;             &quot;patching_elem&quot;: &quot;@end-image&quot;,</span>
<span class="sd">            &gt;&gt;&gt;             &quot;layers_to_patch&quot;: [1, 2, 3, 4],</span>
<span class="sd">            &gt;&gt;&gt;             &quot;activation_type&quot;: &quot;resid_in_{}&quot;,</span>
<span class="sd">            &gt;&gt;&gt;         }</span>
<span class="sd">            &gt;&gt;&gt;     ],</span>
<span class="sd">            &gt;&gt;&gt;     return_logit_diff=False,</span>
<span class="sd">            &gt;&gt;&gt;     batch_saver=lambda x: None,</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; print(final_cache)</span>
<span class="sd">            {</span>
<span class="sd">                &quot;resid_out_0&quot;: tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0</span>
<span class="sd">                &quot;resid_mid_0&quot;: tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0</span>
<span class="sd">                ....</span>
<span class="sd">                &quot;logit_diff_variation&quot;: tensor of shape [batch] with the logit difference variation</span>
<span class="sd">                &quot;logit_diff_in_clean&quot;: tensor of shape [batch] with the logit difference in the clean logits</span>
<span class="sd">                &quot;logit_diff_in_patched&quot;: tensor of shape [batch] with the logit difference in the patched logits</span>
<span class="sd">            }</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Computing patching&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Forward pass started&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Patching elements: </span><span class="si">{</span><span class="p">[</span><span class="n">q</span><span class="p">[</span><span class="s1">&#39;patching_elem&#39;</span><span class="p">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">q</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">patching_query</span><span class="p">]</span><span class="si">}</span><span class="s2"> at </span><span class="si">{</span><span class="p">[</span><span class="n">query</span><span class="p">[</span><span class="s1">&#39;activation_type&#39;</span><span class="p">][:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">patching_query</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># get a random number in the range of the dataset to save a random batch</span>
        <span class="n">all_cache</span> <span class="o">=</span> <span class="n">ActivationCache</span><span class="p">()</span>
        <span class="c1"># for each batch in the dataset</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">base_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">base_dataloader</span><span class="p">,</span> <span class="n">target_dataloader</span><span class="p">)),</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Computing patching on the dataset:&quot;</span><span class="p">,</span>
            <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">base_dataloader</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">base_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>

            <span class="c1"># set the right arguments for extract the patching activations</span>
            <span class="n">activ_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">query</span><span class="p">[</span><span class="s2">&quot;activation_type&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">patching_query</span><span class="p">]</span>

            <span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;extract_resid_out&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s2">&quot;extract_resid_in&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_resid_mid&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_attn_in&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_attn_out&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_head_values&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_head_out&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_avg_attn_pattern&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_avg_values_vectors_projected&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_head_values_projected&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;extract_avg&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">&quot;ablation_queries&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;patching_queries&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;external_cache&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;attn_heads&quot;</span><span class="p">:</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
                <span class="s2">&quot;batch_idx&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;move_to_cpu&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="p">}</span>

            <span class="k">if</span> <span class="s2">&quot;resid_in&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_resid_in&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="s2">&quot;resid_out&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_resid_out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="s2">&quot;resid_mid&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_intermediate_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="s2">&quot;attn_in&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_attn_in&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="s2">&quot;attn_out&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_attn_out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="s2">&quot;values&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
                <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_head_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># other cases</span>

            <span class="c1"># first forward pass to extract the base activations</span>
            <span class="n">base_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">target_token_positions</span><span class="o">=</span><span class="n">target_token_positions</span><span class="p">,</span>
                <span class="n">pivot_positions</span><span class="o">=</span><span class="n">base_batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">extraction_config</span><span class="o">=</span><span class="n">ExtractionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">),</span>
                <span class="n">ablation_queries</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;ablation_queries&quot;</span><span class="p">],</span>
                <span class="n">patching_queries</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;patching_queries&quot;</span><span class="p">],</span>
                <span class="n">external_cache</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;external_cache&quot;</span><span class="p">],</span>
                <span class="n">batch_idx</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;batch_idx&quot;</span><span class="p">],</span>
                <span class="n">move_to_cpu</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;move_to_cpu&quot;</span><span class="p">],</span>
            <span class="p">)</span>

            <span class="c1"># extract the target activations</span>
            <span class="n">target_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
                <span class="n">target_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span>
            <span class="p">)</span>

            <span class="n">requested_position_to_extract</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">patching_query</span><span class="p">:</span>
                <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_activations&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_cache</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_elem&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="ow">not</span> <span class="ow">in</span> <span class="n">requested_position_to_extract</span>
                <span class="p">):</span>
                    <span class="n">requested_position_to_extract</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_elem&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="n">query</span><span class="p">[</span><span class="s2">&quot;base_activation_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_cache</span><span class="p">[</span><span class="s2">&quot;mapping_index&quot;</span><span class="p">][</span>
                    <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_elem&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">]</span>

            <span class="c1"># second forward pass to extract the clean logits</span>
            <span class="n">target_clean_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">target_inputs</span><span class="p">,</span>
                <span class="n">target_token_positions</span><span class="o">=</span><span class="n">requested_position_to_extract</span><span class="p">,</span>
                <span class="n">pivot_positions</span><span class="o">=</span><span class="n">target_batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="c1"># move_to_cpu=True,</span>
            <span class="p">)</span>

            <span class="c1"># merge requested_position_to_extract with extracted_token_positio</span>
            <span class="c1"># third forward pass to patch the activations</span>
            <span class="n">target_patched_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
                <span class="n">target_inputs</span><span class="p">,</span>
                <span class="n">target_token_positions</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span>
                    <span class="nb">set</span><span class="p">(</span><span class="n">target_token_positions</span> <span class="o">+</span> <span class="n">requested_position_to_extract</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="n">pivot_positions</span><span class="o">=</span><span class="n">target_batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">patching_queries</span><span class="o">=</span><span class="n">patching_query</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">return_logit_diff</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">base_dictonary_idxs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">target_dictonary_idxs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;To compute the logit difference, you need to pass the base_dictonary_idxs and the target_dictonary_idxs&quot;</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Computing logit difference&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="c1"># get the target tokens (&quot; cat&quot; and &quot; dog&quot;)</span>
                <span class="n">base_targets</span> <span class="o">=</span> <span class="n">base_dictonary_idxs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                <span class="n">target_targets</span> <span class="o">=</span> <span class="n">target_dictonary_idxs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

                <span class="c1"># compute the logit difference</span>
                <span class="n">result_diff</span> <span class="o">=</span> <span class="n">logit_diff</span><span class="p">(</span>
                    <span class="n">base_label_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">base_targets</span><span class="p">],</span>
                    <span class="n">target_label_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">target_targets</span><span class="p">],</span>
                    <span class="n">target_clean_logits</span><span class="o">=</span><span class="n">target_clean_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
                    <span class="n">target_patched_logits</span><span class="o">=</span><span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logit_diff_variation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_diff</span><span class="p">[</span>
                    <span class="s2">&quot;diff_variation&quot;</span>
                <span class="p">]</span>
                <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logit_diff_in_clean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_diff</span><span class="p">[</span>
                    <span class="s2">&quot;diff_in_clean&quot;</span>
                <span class="p">]</span>
                <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logit_diff_in_patched&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_diff</span><span class="p">[</span>
                    <span class="s2">&quot;diff_in_patched&quot;</span>
                <span class="p">]</span>

            <span class="c1"># compute the KL divergence</span>
            <span class="n">result_kl</span> <span class="o">=</span> <span class="n">kl_divergence_diff</span><span class="p">(</span>
                <span class="n">base_logits</span><span class="o">=</span><span class="n">base_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
                <span class="n">target_clean_logits</span><span class="o">=</span><span class="n">target_clean_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
                <span class="n">target_patched_logits</span><span class="o">=</span><span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">result_kl</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">target_patched_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

            <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;base_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
            <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;target_clean_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_clean_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
            <span class="c1"># rename logits to target_patched_logits</span>
            <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;target_patched_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_patched_cache</span><span class="p">[</span>
                <span class="s2">&quot;logits&quot;</span>
            <span class="p">]</span>
            <span class="k">del</span> <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>

            <span class="n">target_patched_cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

            <span class="c1"># all_cache.append(target_patched_cache)</span>
            <span class="n">all_cache</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">target_patched_cache</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Forward pass finished - started to aggregate different batch&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="c1"># final_cache = aggregate_cache_efficient(all_cache)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Aggregation finished&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_cache</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Call the forward method of the model</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">968</span>
<span class="normal">969</span>
<span class="normal">970</span>
<span class="normal">971</span>
<span class="normal">972</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Call the forward method of the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.compute_patching" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_patching</span><span class="p">(</span><span class="n">target_token_positions</span><span class="p">,</span> <span class="n">base_dataloader</span><span class="p">,</span> <span class="n">target_dataloader</span><span class="p">,</span> <span class="n">patching_query</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;patching_elem&#39;</span><span class="p">:</span> <span class="s1">&#39;@end-image&#39;</span><span class="p">,</span> <span class="s1">&#39;layers_to_patch&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="s1">&#39;activation_type&#39;</span><span class="p">:</span> <span class="s1">&#39;resid_in_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">}],</span> <span class="n">base_dictonary_idxs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_dictonary_idxs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_logit_diff</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_saver</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Method for activation patching. This substitutes the activations of the model
with the activations of the counterfactual dataset.</p>
<p>It performs three forward passes:
1. Forward pass on the base dataset to extract the activations of the model (cat).
2. Forward pass on the target dataset to extract clean logits (dog)
[to compare against the patched logits].
3. Forward pass on the target dataset to patch (cat) into (dog)
and extract the patched logits.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>target_token_positions</code>
            </td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of tokens to extract the activations from.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>base_dataloader</code>
            </td>
            <td>
                  <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dataloader with the base dataset. (dataset where we sample the activations from)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>target_dataloader</code>
            </td>
            <td>
                  <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dataloader with the target dataset. (dataset where we patch the activations)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patching_query</code>
            </td>
            <td>
                  <code>list[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of dictionaries with the patching queries. Each dictionary must have the keys "patching_elem", "layers_to_patch" and "activation_type". The "patching_elem" is the token to patch, the "layers_to_patch" is the list of layers to patch and the "activation_type" is the type of the activation to patch. The activation type must be one of the following: "resid_in_{}", "resid_out_{}", "resid_mid_{}", "attn_in_{}", "attn_out_{}", "values_{}". The "{}" will be replaced with the layer index.</p>
              </div>
            </td>
            <td>
                  <code>[{&#39;patching_elem&#39;: &#39;@end-image&#39;, &#39;layers_to_patch&#39;: [1, 2, 3, 4], &#39;activation_type&#39;: &#39;resid_in_{}&#39;}]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>base_dictonary_idxs</code>
            </td>
            <td>
                  <code>list[list[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>target_dictonary_idxs</code>
            </td>
            <td>
                  <code>list[list[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It's useful to extract the logit difference between the clean logits and the patched logits.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_logit_diff</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, it will return the logit difference between the clean logits and the patched logits.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>final_cache</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.activation_cache.ActivationCache" href="../activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache">ActivationCache</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">compute_patching</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">target_token_positions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;end-image&quot;</span><span class="p">,</span> <span class="s2">&quot; last&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">base_dataloader</span><span class="o">=</span><span class="n">base_dataloader</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">target_dataloader</span><span class="o">=</span><span class="n">target_dataloader</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">base_dictonary_idxs</span><span class="o">=</span><span class="n">base_dictonary_idxs</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">target_dictonary_idxs</span><span class="o">=</span><span class="n">target_dictonary_idxs</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">patching_query</span><span class="o">=</span><span class="p">[</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="p">{</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="s2">&quot;patching_elem&quot;</span><span class="p">:</span> <span class="s2">&quot;@end-image&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="s2">&quot;layers_to_patch&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="s2">&quot;activation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;resid_in_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_logit_diff</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">batch_saver</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">final_cache</span><span class="p">)</span>
<span class="go">{</span>
<span class="go">    &quot;resid_out_0&quot;: tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0</span>
<span class="go">    &quot;resid_mid_0&quot;: tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0</span>
<span class="go">    ....</span>
<span class="go">    &quot;logit_diff_variation&quot;: tensor of shape [batch] with the logit difference variation</span>
<span class="go">    &quot;logit_diff_in_clean&quot;: tensor of shape [batch] with the logit difference in the clean logits</span>
<span class="go">    &quot;logit_diff_in_patched&quot;: tensor of shape [batch] with the logit difference in the patched logits</span>
<span class="go">}</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">compute_patching</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="c1"># counterfactual_dataset,</span>
    <span class="n">base_dataloader</span><span class="p">,</span>
    <span class="n">target_dataloader</span><span class="p">,</span>
    <span class="n">patching_query</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;patching_elem&quot;</span><span class="p">:</span> <span class="s2">&quot;@end-image&quot;</span><span class="p">,</span>
            <span class="s2">&quot;layers_to_patch&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
            <span class="s2">&quot;activation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;resid_in_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="n">base_dictonary_idxs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_dictonary_idxs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_logit_diff</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_saver</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method for activation patching. This substitutes the activations of the model</span>
<span class="sd">    with the activations of the counterfactual dataset.</span>

<span class="sd">    It performs three forward passes:</span>
<span class="sd">    1. Forward pass on the base dataset to extract the activations of the model (cat).</span>
<span class="sd">    2. Forward pass on the target dataset to extract clean logits (dog)</span>
<span class="sd">    [to compare against the patched logits].</span>
<span class="sd">    3. Forward pass on the target dataset to patch (cat) into (dog)</span>
<span class="sd">    and extract the patched logits.</span>

<span class="sd">    Args:</span>
<span class="sd">        target_token_positions (list[str]): List of tokens to extract the activations from.</span>
<span class="sd">        base_dataloader (torch.utils.data.DataLoader): Dataloader with the base dataset. (dataset where we sample the activations from)</span>
<span class="sd">        target_dataloader (torch.utils.data.DataLoader): Dataloader with the target dataset. (dataset where we patch the activations)</span>
<span class="sd">        patching_query (list[dict]): List of dictionaries with the patching queries. Each dictionary must have the keys &quot;patching_elem&quot;, &quot;layers_to_patch&quot; and &quot;activation_type&quot;. The &quot;patching_elem&quot; is the token to patch, the &quot;layers_to_patch&quot; is the list of layers to patch and the &quot;activation_type&quot; is the type of the activation to patch. The activation type must be one of the following: &quot;resid_in_{}&quot;, &quot;resid_out_{}&quot;, &quot;resid_mid_{}&quot;, &quot;attn_in_{}&quot;, &quot;attn_out_{}&quot;, &quot;values_{}&quot;. The &quot;{}&quot; will be replaced with the layer index.</span>
<span class="sd">        base_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It&#39;s useful to extract the logit difference between the clean logits and the patched logits.</span>
<span class="sd">        target_dictonary_idxs (list[list[int]]): List of list of integers with the indexes of the tokens in the dictonary that we are interested in. It&#39;s useful to extract the logit difference between the clean logits and the patched logits.</span>
<span class="sd">        return_logit_diff (bool): If True, it will return the logit difference between the clean logits and the patched logits.</span>


<span class="sd">    Returns:</span>
<span class="sd">        final_cache (ActivationCache): dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; model.compute_patching(</span>
<span class="sd">        &gt;&gt;&gt;     target_token_positions=[&quot;end-image&quot;, &quot; last&quot;],</span>
<span class="sd">        &gt;&gt;&gt;     base_dataloader=base_dataloader,</span>
<span class="sd">        &gt;&gt;&gt;     target_dataloader=target_dataloader,</span>
<span class="sd">        &gt;&gt;&gt;     base_dictonary_idxs=base_dictonary_idxs,</span>
<span class="sd">        &gt;&gt;&gt;     target_dictonary_idxs=target_dictonary_idxs,</span>
<span class="sd">        &gt;&gt;&gt;     patching_query=[</span>
<span class="sd">        &gt;&gt;&gt;         {</span>
<span class="sd">        &gt;&gt;&gt;             &quot;patching_elem&quot;: &quot;@end-image&quot;,</span>
<span class="sd">        &gt;&gt;&gt;             &quot;layers_to_patch&quot;: [1, 2, 3, 4],</span>
<span class="sd">        &gt;&gt;&gt;             &quot;activation_type&quot;: &quot;resid_in_{}&quot;,</span>
<span class="sd">        &gt;&gt;&gt;         }</span>
<span class="sd">        &gt;&gt;&gt;     ],</span>
<span class="sd">        &gt;&gt;&gt;     return_logit_diff=False,</span>
<span class="sd">        &gt;&gt;&gt;     batch_saver=lambda x: None,</span>
<span class="sd">        &gt;&gt;&gt; )</span>
<span class="sd">        &gt;&gt;&gt; print(final_cache)</span>
<span class="sd">        {</span>
<span class="sd">            &quot;resid_out_0&quot;: tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0</span>
<span class="sd">            &quot;resid_mid_0&quot;: tensor of shape [batch, seq_len, hidden_size] with the activations of the residual stream of layer 0</span>
<span class="sd">            ....</span>
<span class="sd">            &quot;logit_diff_variation&quot;: tensor of shape [batch] with the logit difference variation</span>
<span class="sd">            &quot;logit_diff_in_clean&quot;: tensor of shape [batch] with the logit difference in the clean logits</span>
<span class="sd">            &quot;logit_diff_in_patched&quot;: tensor of shape [batch] with the logit difference in the patched logits</span>
<span class="sd">        }</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Computing patching&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Forward pass started&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Patching elements: </span><span class="si">{</span><span class="p">[</span><span class="n">q</span><span class="p">[</span><span class="s1">&#39;patching_elem&#39;</span><span class="p">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">q</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">patching_query</span><span class="p">]</span><span class="si">}</span><span class="s2"> at </span><span class="si">{</span><span class="p">[</span><span class="n">query</span><span class="p">[</span><span class="s1">&#39;activation_type&#39;</span><span class="p">][:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">query</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">patching_query</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># get a random number in the range of the dataset to save a random batch</span>
    <span class="n">all_cache</span> <span class="o">=</span> <span class="n">ActivationCache</span><span class="p">()</span>
    <span class="c1"># for each batch in the dataset</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">base_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
        <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">base_dataloader</span><span class="p">,</span> <span class="n">target_dataloader</span><span class="p">)),</span>
        <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Computing patching on the dataset:&quot;</span><span class="p">,</span>
        <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">base_dataloader</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">base_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>

        <span class="c1"># set the right arguments for extract the patching activations</span>
        <span class="n">activ_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">query</span><span class="p">[</span><span class="s2">&quot;activation_type&quot;</span><span class="p">][:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">patching_query</span><span class="p">]</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;extract_resid_out&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;extract_resid_in&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_resid_mid&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_attn_in&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_attn_out&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_head_values&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_head_out&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_avg_attn_pattern&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_avg_values_vectors_projected&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_head_values_projected&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;extract_avg&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;ablation_queries&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;patching_queries&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;external_cache&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;attn_heads&quot;</span><span class="p">:</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
            <span class="s2">&quot;batch_idx&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;move_to_cpu&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="s2">&quot;resid_in&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_resid_in&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="s2">&quot;resid_out&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_resid_out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="s2">&quot;resid_mid&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_intermediate_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="s2">&quot;attn_in&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_attn_in&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="s2">&quot;attn_out&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_attn_out&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="s2">&quot;values&quot;</span> <span class="ow">in</span> <span class="n">activ_type</span><span class="p">:</span>
            <span class="n">args</span><span class="p">[</span><span class="s2">&quot;extract_head_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># other cases</span>

        <span class="c1"># first forward pass to extract the base activations</span>
        <span class="n">base_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">target_token_positions</span><span class="o">=</span><span class="n">target_token_positions</span><span class="p">,</span>
            <span class="n">pivot_positions</span><span class="o">=</span><span class="n">base_batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">extraction_config</span><span class="o">=</span><span class="n">ExtractionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">),</span>
            <span class="n">ablation_queries</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;ablation_queries&quot;</span><span class="p">],</span>
            <span class="n">patching_queries</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;patching_queries&quot;</span><span class="p">],</span>
            <span class="n">external_cache</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;external_cache&quot;</span><span class="p">],</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;batch_idx&quot;</span><span class="p">],</span>
            <span class="n">move_to_cpu</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;move_to_cpu&quot;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="c1"># extract the target activations</span>
        <span class="n">target_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
            <span class="n">target_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span>
        <span class="p">)</span>

        <span class="n">requested_position_to_extract</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">query</span> <span class="ow">in</span> <span class="n">patching_query</span><span class="p">:</span>
            <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_activations&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_cache</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_elem&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                <span class="ow">not</span> <span class="ow">in</span> <span class="n">requested_position_to_extract</span>
            <span class="p">):</span>
                <span class="n">requested_position_to_extract</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_elem&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="n">query</span><span class="p">[</span><span class="s2">&quot;base_activation_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_cache</span><span class="p">[</span><span class="s2">&quot;mapping_index&quot;</span><span class="p">][</span>
                <span class="n">query</span><span class="p">[</span><span class="s2">&quot;patching_elem&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;@&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">]</span>

        <span class="c1"># second forward pass to extract the clean logits</span>
        <span class="n">target_clean_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">target_inputs</span><span class="p">,</span>
            <span class="n">target_token_positions</span><span class="o">=</span><span class="n">requested_position_to_extract</span><span class="p">,</span>
            <span class="n">pivot_positions</span><span class="o">=</span><span class="n">target_batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="c1"># move_to_cpu=True,</span>
        <span class="p">)</span>

        <span class="c1"># merge requested_position_to_extract with extracted_token_positio</span>
        <span class="c1"># third forward pass to patch the activations</span>
        <span class="n">target_patched_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">target_inputs</span><span class="p">,</span>
            <span class="n">target_token_positions</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span>
                <span class="nb">set</span><span class="p">(</span><span class="n">target_token_positions</span> <span class="o">+</span> <span class="n">requested_position_to_extract</span><span class="p">)</span>
            <span class="p">),</span>
            <span class="n">pivot_positions</span><span class="o">=</span><span class="n">target_batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">patching_queries</span><span class="o">=</span><span class="n">patching_query</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_logit_diff</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">base_dictonary_idxs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">target_dictonary_idxs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;To compute the logit difference, you need to pass the base_dictonary_idxs and the target_dictonary_idxs&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Computing logit difference&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># get the target tokens (&quot; cat&quot; and &quot; dog&quot;)</span>
            <span class="n">base_targets</span> <span class="o">=</span> <span class="n">base_dictonary_idxs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">target_targets</span> <span class="o">=</span> <span class="n">target_dictonary_idxs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

            <span class="c1"># compute the logit difference</span>
            <span class="n">result_diff</span> <span class="o">=</span> <span class="n">logit_diff</span><span class="p">(</span>
                <span class="n">base_label_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">base_targets</span><span class="p">],</span>
                <span class="n">target_label_tokens</span><span class="o">=</span><span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">target_targets</span><span class="p">],</span>
                <span class="n">target_clean_logits</span><span class="o">=</span><span class="n">target_clean_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
                <span class="n">target_patched_logits</span><span class="o">=</span><span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logit_diff_variation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_diff</span><span class="p">[</span>
                <span class="s2">&quot;diff_variation&quot;</span>
            <span class="p">]</span>
            <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logit_diff_in_clean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_diff</span><span class="p">[</span>
                <span class="s2">&quot;diff_in_clean&quot;</span>
            <span class="p">]</span>
            <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logit_diff_in_patched&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_diff</span><span class="p">[</span>
                <span class="s2">&quot;diff_in_patched&quot;</span>
            <span class="p">]</span>

        <span class="c1"># compute the KL divergence</span>
        <span class="n">result_kl</span> <span class="o">=</span> <span class="n">kl_divergence_diff</span><span class="p">(</span>
            <span class="n">base_logits</span><span class="o">=</span><span class="n">base_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
            <span class="n">target_clean_logits</span><span class="o">=</span><span class="n">target_clean_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
            <span class="n">target_patched_logits</span><span class="o">=</span><span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">result_kl</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">target_patched_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;base_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;target_clean_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_clean_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="c1"># rename logits to target_patched_logits</span>
        <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;target_patched_logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_patched_cache</span><span class="p">[</span>
            <span class="s2">&quot;logits&quot;</span>
        <span class="p">]</span>
        <span class="k">del</span> <span class="n">target_patched_cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>

        <span class="n">target_patched_cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="c1"># all_cache.append(target_patched_cache)</span>
        <span class="n">all_cache</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">target_patched_cache</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s2">&quot;Forward pass finished - started to aggregate different batch&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># final_cache = aggregate_cache_efficient(all_cache)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Aggregation finished&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_cache</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.create_hooks" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_hooks</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">token_index</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">extraction_config</span><span class="o">=</span><span class="n">ExtractionConfig</span><span class="p">(),</span> <span class="n">patching_queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ablation_queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">external_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs</code>
            </td>
            <td>
                  <code>dict</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cache</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.activation_cache.ActivationCache" href="../activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache">ActivationCache</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary where the activations of the model will be saved</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extracted_token_position</code>
            </td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of tokens to extract the activations from (["last", "end-image", "start-image", "first"])</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>string_tokens</code>
            </td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of string tokens</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pivot_positions</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[list[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of split positions of the tokens</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extraction_config</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.hooked_model.ExtractionConfig" href="#easyroutine.interpretability.hooked_model.ExtractionConfig">ExtractionConfig</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>configuration of the extraction of the activations of the model (default = ExtractionConfig())</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.hooked_model.ExtractionConfig" href="#easyroutine.interpretability.hooked_model.ExtractionConfig">ExtractionConfig</a>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ablation_queries</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[dict, <span title="pandas.DataFrame">DataFrame</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary or dataframe with the ablation queries to perform during forward pass</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patching_queries</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[dict, <span title="pandas.DataFrame">DataFrame</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary or dataframe with the patching queries to perform during forward pass</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_idx</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>index of the batch in the dataloader</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>external_cache</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="easyroutine.interpretability.activation_cache.ActivationCache" href="../activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache">ActivationCache</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>external cache to use in the forward pass</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>hooks</code></td>            <td>
                  <code>list[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of dictionaries with the component and the intervention to perform in the forward pass of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">create_hooks</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">cache</span><span class="p">:</span> <span class="n">ActivationCache</span><span class="p">,</span>
    <span class="n">token_index</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
    <span class="n">token_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="c1"># string_tokens: List[str],</span>
    <span class="n">extraction_config</span><span class="p">:</span> <span class="n">ExtractionConfig</span> <span class="o">=</span> <span class="n">ExtractionConfig</span><span class="p">(),</span>
    <span class="n">patching_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">external_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ActivationCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create the hooks to extract the activations of the model. The hooks will be added to the model and will be called in the forward pass of the model.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</span>
<span class="sd">        cache (ActivationCache): dictionary where the activations of the model will be saved</span>
<span class="sd">        extracted_token_position (list[str]): list of tokens to extract the activations from ([&quot;last&quot;, &quot;end-image&quot;, &quot;start-image&quot;, &quot;first&quot;])</span>
<span class="sd">        string_tokens (list[str]): list of string tokens</span>
<span class="sd">        pivot_positions (Optional[list[int]]): list of split positions of the tokens</span>
<span class="sd">        extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model (default = ExtractionConfig())</span>
<span class="sd">        ablation_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the ablation queries to perform during forward pass</span>
<span class="sd">        patching_queries (Optional[Union[dict, pd.DataFrame]]): dictionary or dataframe with the patching queries to perform during forward pass</span>
<span class="sd">        batch_idx (Optional[int]): index of the batch in the dataloader</span>
<span class="sd">        external_cache (Optional[ActivationCache]): external cache to use in the forward pass</span>

<span class="sd">    Returns:</span>
<span class="sd">        hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hooks</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># compute layer and head indexes</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="ow">and</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span>
    <span class="p">):</span>
        <span class="n">layer_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)]</span>
        <span class="n">head_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">layer_head_indexes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">el</span><span class="p">[</span><span class="s2">&quot;layer&quot;</span><span class="p">],</span> <span class="n">el</span><span class="p">[</span><span class="s2">&quot;head&quot;</span><span class="p">])</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">attn_heads</span>
        <span class="p">]</span>
        <span class="n">layer_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">el</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">layer_head_indexes</span><span class="p">]</span>
        <span class="n">head_indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">el</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">layer_head_indexes</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;attn_heads must be &#39;all&#39; or a list of dictionaries as [{&#39;layer&#39;: 0, &#39;head&#39;: 0}]&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_out</span><span class="p">:</span>
        <span class="c1"># assert that the component exists in the model</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_out_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_in</span><span class="p">:</span>
        <span class="c1"># assert that the component exists in the model</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_input_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">i</span>
                <span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_in_post_layernorm</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">residual_stream_input_post_layernorm_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">i</span>
                <span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_in_post_layernorm_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">save_input_ids</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">,</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">embed_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_queries</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_query_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">query_key_value_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;queries_&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                    <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                    <span class="n">num_key_value_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_values</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_value_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">query_key_value_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;values_&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                    <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                    <span class="n">num_key_value_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_keys</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_key_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">query_key_value_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;keys_&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                    <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                    <span class="n">num_key_value_groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_out</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_o_proj_input_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">i</span>
                <span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">head_out_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="s2">&quot;head_out_&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                    <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                    <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                    <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">o_proj_weight</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_weight</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="p">),</span>
                    <span class="n">o_proj_bias</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_bias</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="p">),</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_attn_in</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_in_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;attn_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_attn_out</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;attn_out_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="c1"># if extraction_config.extract_avg:</span>
    <span class="c1">#     # Define a hook that saves the activations of the residual stream</span>
    <span class="c1">#     raise NotImplementedError(</span>
    <span class="c1">#         &quot;The hook for the average is not working with token_index as a list&quot;</span>
    <span class="c1">#     )</span>

    <span class="c1">#     # hooks.extend(</span>
    <span class="c1">#     #     [</span>
    <span class="c1">#     #         {</span>
    <span class="c1">#     #             &quot;component&quot;: self.model_config.residual_stream_hook_name.format(</span>
    <span class="c1">#     #                 i</span>
    <span class="c1">#     #             ),</span>
    <span class="c1">#     #             &quot;intervention&quot;: partial(</span>
    <span class="c1">#     #                 avg_hook,</span>
    <span class="c1">#     #                 cache=cache,</span>
    <span class="c1">#     #                 cache_key=&quot;resid_avg_{}&quot;.format(i),</span>
    <span class="c1">#     #                 last_image_idx=last_image_idxs, #type</span>
    <span class="c1">#     #                 end_image_idx=end_image_idxs,</span>
    <span class="c1">#     #             ),</span>
    <span class="c1">#     #         }</span>
    <span class="c1">#     #         for i in range(0, self.model_config.num_hidden_layers)</span>
    <span class="c1">#     #     ]</span>
    <span class="c1">#     # )</span>
    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_resid_mid</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">intermediate_stream_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">i</span>
                <span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;resid_mid_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># if we want to extract the output of the heads</span>
    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_mlp_out</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">mlp_out_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">save_resid_hook</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">cache_key</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;mlp_out_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="c1"># PATCHING</span>
    <span class="k">if</span> <span class="n">patching_queries</span><span class="p">:</span>
        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">map_token_to_pos</span><span class="p">,</span>
            <span class="n">_get_token_index</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
            <span class="c1"># string_tokens=string_tokens,</span>
            <span class="n">hf_tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">patching_queries</span> <span class="o">=</span> <span class="n">preprocess_patching_queries</span><span class="p">(</span>
            <span class="n">patching_queries</span><span class="o">=</span><span class="n">patching_queries</span><span class="p">,</span>
            <span class="n">map_token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span><span class="p">,</span>
            <span class="n">model_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">make_patch_tokens_hook</span><span class="p">(</span><span class="n">patching_queries_subset</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Creates a hook function to patch the activations in the</span>
<span class="sd">            current forward pass.</span>
<span class="sd">            &quot;&quot;&quot;</span>

            <span class="k">def</span> <span class="nf">patch_tokens_hook</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span>
            <span class="p">):</span>  <span class="c1"># TODO: Move to hook.py</span>
                <span class="n">b</span> <span class="o">=</span> <span class="n">process_args_kwargs_output</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
                <span class="c1"># Modify the tensor without affecting the computation graph</span>
                <span class="n">act_to_patch</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">patch</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">patching_queries_subset</span><span class="p">[</span><span class="s2">&quot;pos_token_to_patch&quot;</span><span class="p">],</span>
                    <span class="n">patching_queries_subset</span><span class="p">[</span><span class="s2">&quot;patching_activations&quot;</span><span class="p">],</span>
                <span class="p">):</span>
                    <span class="n">act_to_patch</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">patching_queries_subset</span><span class="p">[</span>
                        <span class="s2">&quot;patching_activations&quot;</span>
                    <span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                        <span class="k">return</span> <span class="p">(</span><span class="n">act_to_patch</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">elif</span> <span class="nb">input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">act_to_patch</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                        <span class="k">return</span> <span class="p">(</span><span class="n">act_to_patch</span><span class="p">,</span> <span class="o">*</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">elif</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">act_to_patch</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No output or input found&quot;</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">patch_tokens_hook</span>

        <span class="c1"># Group the patching queries by &#39;layer&#39; and &#39;act_type&#39;</span>
        <span class="n">grouped_queries</span> <span class="o">=</span> <span class="n">patching_queries</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;layer&quot;</span><span class="p">,</span> <span class="s2">&quot;activation_type&quot;</span><span class="p">])</span>

        <span class="k">for</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">act_type</span><span class="p">),</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">grouped_queries</span><span class="p">:</span>
            <span class="n">hook_name_template</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act_type_to_hook_name</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">act_type</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
            <span class="p">)</span>  <span class="c1"># -3 because we remove {}</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">hook_name_template</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown activation type: </span><span class="si">{</span><span class="n">act_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="c1"># continue  # Skip unknown activation types</span>

            <span class="n">hook_name</span> <span class="o">=</span> <span class="n">hook_name_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
            <span class="n">hook_function</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">make_patch_tokens_hook</span><span class="p">(</span><span class="n">group</span><span class="p">))</span>

            <span class="n">hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="n">hook_name</span><span class="p">,</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">hook_function</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">ablation_queries</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># TODO: debug and test the ablation. Check with ale</span>
        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">map_token_to_pos</span><span class="p">,</span>
            <span class="n">_get_token_index</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
            <span class="c1"># string_tokens=string_tokens,</span>
            <span class="n">hf_tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Ablation is not supported with batch size &gt; 1&quot;</span><span class="p">)</span>
        <span class="n">ablation_manager</span> <span class="o">=</span> <span class="n">AblationManager</span><span class="p">(</span>
            <span class="n">model_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
            <span class="n">token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">model_attn_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_implementation</span><span class="p">,</span>
            <span class="n">ablation_queries</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ablation_queries</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ablation_queries</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">ablation_queries</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hooks</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ablation_manager</span><span class="o">.</span><span class="n">main</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_values_projected</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_value_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                    <span class="n">projected_value_vectors_head</span><span class="p">,</span>
                    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                    <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                    <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                    <span class="n">num_attention_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
                    <span class="n">num_key_value_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span>
                    <span class="n">hidden_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
                    <span class="n">d_head</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
                    <span class="n">out_proj_weight</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_weight</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">out_proj_bias</span><span class="o">=</span><span class="n">get_attribute_from_name</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_out_proj_bias</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                    <span class="n">avg</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_attn_pattern</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">avg</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">external_cache</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;The external_cache is None. The average could not be computed since missing an external cache where store the iterations.</span>
<span class="sd">                    &quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">batch_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;The batch_idx is None. The average could not be computed since missing the batch index.</span>

<span class="sd">                    &quot;&quot;&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># move the cache to the same device of the model</span>
                <span class="n">external_cache</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>
                <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_matrix_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">i</span>
                        <span class="p">),</span>
                        <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                            <span class="n">avg_attention_pattern_head</span><span class="p">,</span>
                            <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                            <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                            <span class="n">attn_pattern_current_avg</span><span class="o">=</span><span class="n">external_cache</span><span class="p">,</span>
                            <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
                            <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                            <span class="c1"># avg=extraction_config.avg,</span>
                            <span class="n">extract_avg_value</span><span class="o">=</span><span class="n">extraction_config</span><span class="o">.</span><span class="n">extract_head_values_projected</span><span class="p">,</span>
                        <span class="p">),</span>
                    <span class="p">}</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
                <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hooks</span> <span class="o">+=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">attn_matrix_hook_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                    <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span>
                        <span class="n">attention_pattern_head</span><span class="p">,</span>
                        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
                        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
                        <span class="n">layer</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
                        <span class="n">head</span><span class="o">=</span><span class="n">head</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_indexes</span><span class="p">,</span> <span class="n">head_indexes</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="c1"># if additional hooks are not empty, add them to the hooks list</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span><span class="p">:</span>
        <span class="n">hooks</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span>
    <span class="k">return</span> <span class="n">hooks</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.device" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">device</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Return the device of the model. If the model is in multiple devices, it will return the first device</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>device</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the device of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the device of the model. If the model is in multiple devices, it will return the first device</span>

<span class="sd">    Args:</span>
<span class="sd">        None</span>

<span class="sd">    Returns:</span>
<span class="sd">        device: the device of the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.eval" class="doc doc-heading">
            <code class="highlight language-python"><span class="nb">eval</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Set the model in evaluation mode</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the model in evaluation mode</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.extract_cache" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">extract_cache</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">target_token_positions</span><span class="p">,</span> <span class="n">batch_saver</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="n">move_to_cpu_after_forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>dataloader</code>
            </td>
            <td>
                  <code>iterable</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extracted_token_position</code>
            </td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of tokens to extract the activations from (["last", "end-image", "start-image", "first"])</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_saver</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)</p>
              </div>
            </td>
            <td>
                  <code>lambda x: None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>move_to_cpu_after_forward</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, move the activations to the cpu right after the any forward pass of the model</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>**kwargs</code>
            </td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>additional arguments to control hooks generation, basically accept any argument handled by the <code>.forward</code> method (i.e. ablation_queries, patching_queries, extract_resid_in)</p>
              </div>
            </td>
            <td>
                  <code>{}</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>final_cache</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">1235</span><span class="p">,</span> <span class="mi">102</span><span class="p">]]),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])},</span> <span class="o">...</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">extract_cache</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">extracted_token_position</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;last&quot;</span><span class="p">],</span> <span class="n">batch_saver</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]})</span>
<span class="go">{&#39;resid_out_0&#39;: tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), &#39;labels&#39;: tensor([1]), &#39;mapping_index&#39;: {&#39;last&#39;: [0]}}</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">extract_cache</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataloader</span><span class="p">,</span>
    <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">batch_saver</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">move_to_cpu_after_forward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="c1"># save_other_batch_elements: bool = False,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Method to extract the activations of the model from a specific dataset. Compute a forward pass for each batch of the dataloader and save the activations in the cache.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (iterable): dataloader with the dataset. Each element of the dataloader must be a dictionary that contains the inputs that the model expects (input_ids, attention_mask, pixel_values ...)</span>
<span class="sd">        extracted_token_position (list[str]): list of tokens to extract the activations from ([&quot;last&quot;, &quot;end-image&quot;, &quot;start-image&quot;, &quot;first&quot;])</span>
<span class="sd">        batch_saver (Callable): function to save in the cache the additional element from each elemtn of the batch (For example, the labels of the dataset)</span>
<span class="sd">        move_to_cpu_after_forward (bool): if True, move the activations to the cpu right after the any forward pass of the model</span>
<span class="sd">        **kwargs: additional arguments to control hooks generation, basically accept any argument handled by the `.forward` method (i.e. ablation_queries, patching_queries, extract_resid_in)</span>

<span class="sd">    Returns:</span>
<span class="sd">        final_cache: dictionary with the activations of the model. The keys are the names of the activations and the values are the activations themselve</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = [{&quot;input_ids&quot;: torch.tensor([[101, 1234, 1235, 102]]), &quot;attention_mask&quot;: torch.tensor([[1, 1, 1, 1]]), &quot;labels&quot;: torch.tensor([1])}, ...]</span>
<span class="sd">        &gt;&gt;&gt; model.extract_cache(dataloader, extracted_token_position=[&quot;last&quot;], batch_saver=lambda x: {&quot;labels&quot;: x[&quot;labels&quot;]})</span>
<span class="sd">        {&#39;resid_out_0&#39;: tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), &#39;labels&#39;: tensor([1]), &#39;mapping_index&#39;: {&#39;last&#39;: [0]}}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Extracting cache&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># get the function to save in the cache the additional element from the batch sime</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Forward pass started&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">all_cache</span> <span class="o">=</span> <span class="n">ActivationCache</span><span class="p">()</span>  <span class="c1"># a list of dictoionaries, each dictionary contains the activations of the model for a batch (so a dict of tensors)</span>
    <span class="n">attn_pattern</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">ActivationCache</span><span class="p">()</span>
    <span class="p">)</span>  <span class="c1"># Initialize the dictionary to hold running averages</span>

    <span class="n">example_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Initialize batch counter</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Extracting cache:&quot;</span><span class="p">):</span>
        <span class="c1"># log_memory_usage(&quot;Extract cache - Before batch&quot;)</span>
        <span class="c1"># tokens, others = batch</span>
        <span class="c1"># inputs = {k: v.to(self.first_device) for k, v in tokens.items()}</span>

        <span class="c1"># get input_ids, attention_mask, and if available, pixel_values from batch (that is a dictionary)</span>
        <span class="c1"># then move them to the first device</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>
        <span class="n">others</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">}</span>

        <span class="n">cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">target_token_positions</span><span class="o">=</span><span class="n">target_token_positions</span><span class="p">,</span>
            <span class="n">pivot_positions</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pivot_positions&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">external_cache</span><span class="o">=</span><span class="n">attn_pattern</span><span class="p">,</span>
            <span class="n">batch_idx</span><span class="o">=</span><span class="n">n_batches</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># possible memory leak from here -___---------------&gt;</span>
        <span class="n">additional_dict</span> <span class="o">=</span> <span class="n">batch_saver</span><span class="p">(</span><span class="n">others</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">additional_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># cache = {**cache, **additional_dict}</span>
            <span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">additional_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">move_to_cpu_after_forward</span><span class="p">:</span>
            <span class="n">cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="n">n_batches</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Increment batch counter# Process and remove &quot;pattern_&quot; keys from cache</span>
        <span class="n">all_cache</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>

        <span class="k">del</span> <span class="n">cache</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">inputs</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s2">&quot;Forward pass finished - started to aggregate different batch&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">all_cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">attn_pattern</span><span class="p">)</span>
    <span class="n">all_cache</span><span class="p">[</span><span class="s2">&quot;example_dict&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">example_dict</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Aggregation finished&quot;</span><span class="p">,</span> <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">all_cache</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_token_positions</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;last&#39;</span><span class="p">],</span> <span class="n">pivot_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">extraction_config</span><span class="o">=</span><span class="n">ExtractionConfig</span><span class="p">(),</span> <span class="n">ablation_queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">patching_queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">external_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attn_heads</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">move_to_cpu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>inputs</code>
            </td>
            <td>
                  <code>dict</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>target_token_positions</code>
            </td>
            <td>
                  <code>list[str]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of tokens to extract the activations from (["last", "end-image", "start-image", "first"])</p>
              </div>
            </td>
            <td>
                  <code>[&#39;last&#39;]</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>pivot_positions</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[list[int]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of split positions of the tokens</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>extraction_config</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.hooked_model.ExtractionConfig" href="#easyroutine.interpretability.hooked_model.ExtractionConfig">ExtractionConfig</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>configuration of the extraction of the activations of the model</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.hooked_model.ExtractionConfig" href="#easyroutine.interpretability.hooked_model.ExtractionConfig">ExtractionConfig</a>()</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ablation_queries</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="pandas.DataFrame">DataFrame</span> | None]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dataframe with the ablation queries to perform during forward pass</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>patching_queries</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="pandas.DataFrame">DataFrame</span> | None]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dataframe with the patching queries to perform during forward pass</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>external_cache</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="easyroutine.interpretability.activation_cache.ActivationCache" href="../activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache">ActivationCache</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>external cache to use in the forward pass</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_heads</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[list[dict], <span title="typing.Literal">Literal</span>[&#39;all&#39;]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of dictionaries with the layer and head to extract the attention pattern or 'all' to</p>
              </div>
            </td>
            <td>
                  <code>&#39;all&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_idx</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[int]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>index of the batch in the dataloader</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>move_to_cpu</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, move the activations to the cpu</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>cache</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.activation_cache.ActivationCache" href="../activation_cache/#easyroutine.interpretability.activation_cache.ActivationCache">ActivationCache</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dictionary with the activations of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">1235</span><span class="p">,</span> <span class="mi">102</span><span class="p">]]),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_token_positions</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;last&quot;</span><span class="p">],</span> <span class="n">extract_resid_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">{&#39;resid_out_0&#39;: tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), &#39;input_ids&#39;: tensor([[101, 1234, 1235, 102]]), &#39;mapping_index&#39;: {&#39;last&#39;: [0]}}</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span>
<span class="normal">964</span>
<span class="normal">965</span>
<span class="normal">966</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;last&quot;</span><span class="p">],</span>
    <span class="n">pivot_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">extraction_config</span><span class="p">:</span> <span class="n">ExtractionConfig</span> <span class="o">=</span> <span class="n">ExtractionConfig</span><span class="p">(),</span>
    <span class="n">ablation_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">patching_queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">external_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ActivationCache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attn_heads</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">move_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of the model. It will extract the activations of the model and save them in the cache. It will also perform ablation and patching if needed.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (dict): dictionary with the inputs of the model (input_ids, attention_mask, pixel_values ...)</span>
<span class="sd">        target_token_positions (list[str]): list of tokens to extract the activations from ([&quot;last&quot;, &quot;end-image&quot;, &quot;start-image&quot;, &quot;first&quot;])</span>
<span class="sd">        pivot_positions (Optional[list[int]]): list of split positions of the tokens</span>
<span class="sd">        extraction_config (ExtractionConfig): configuration of the extraction of the activations of the model</span>
<span class="sd">        ablation_queries (Optional[pd.DataFrame | None]): dataframe with the ablation queries to perform during forward pass</span>
<span class="sd">        patching_queries (Optional[pd.DataFrame | None]): dataframe with the patching queries to perform during forward pass</span>
<span class="sd">        external_cache (Optional[ActivationCache]): external cache to use in the forward pass</span>
<span class="sd">        attn_heads (Union[list[dict], Literal[&quot;all&quot;]]): list of dictionaries with the layer and head to extract the attention pattern or &#39;all&#39; to</span>
<span class="sd">        batch_idx (Optional[int]): index of the batch in the dataloader</span>
<span class="sd">        move_to_cpu (bool): if True, move the activations to the cpu</span>

<span class="sd">    Returns:</span>
<span class="sd">        cache (ActivationCache): dictionary with the activations of the model</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = {&quot;input_ids&quot;: torch.tensor([[101, 1234, 1235, 102]]), &quot;attention_mask&quot;: torch.tensor([[1, 1, 1, 1]])}</span>
<span class="sd">        &gt;&gt;&gt; model.forward(inputs, target_token_positions=[&quot;last&quot;], extract_resid_out=True)</span>
<span class="sd">        {&#39;resid_out_0&#39;: tensor([[[0.1, 0.2, 0.3, 0.4]]], grad_fn=&lt;CopyBackwards&gt;), &#39;input_ids&#39;: tensor([[101, 1234, 1235, 102]]), &#39;mapping_index&#39;: {&#39;last&#39;: [0]}}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">target_token_positions</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">extraction_config</span><span class="o">.</span><span class="n">is_not_empty</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;target_token_positions must be passed if we want to extract the activations of the model&quot;</span>
        <span class="p">)</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="n">ActivationCache</span><span class="p">()</span>
    <span class="n">string_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_string_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">get_input_ids</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">token_index</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">TokenIndex</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">pivot_positions</span><span class="o">=</span><span class="n">pivot_positions</span>
    <span class="p">)</span><span class="o">.</span><span class="n">get_token_index</span><span class="p">(</span>
        <span class="n">tokens</span><span class="o">=</span><span class="n">target_token_positions</span><span class="p">,</span>
        <span class="n">string_tokens</span><span class="o">=</span><span class="n">string_tokens</span><span class="p">,</span>
        <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_index</span><span class="p">,</span> <span class="nb">list</span><span class="p">),</span> <span class="s2">&quot;Token index must be a list&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span> <span class="s2">&quot;Token dict must be a dict&quot;</span>

    <span class="n">hooks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_hooks</span><span class="p">(</span>  <span class="c1"># TODO: add **kwargs</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">token_dict</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
        <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
        <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
        <span class="n">extraction_config</span><span class="o">=</span><span class="n">extraction_config</span><span class="p">,</span>
        <span class="n">ablation_queries</span><span class="o">=</span><span class="n">ablation_queries</span><span class="p">,</span>
        <span class="n">patching_queries</span><span class="o">=</span><span class="n">patching_queries</span><span class="p">,</span>
        <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span>
        <span class="n">external_cache</span><span class="o">=</span><span class="n">external_cache</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">hook_handlers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hooks</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">torch_dtype</span>
    <span class="p">)</span>
    <span class="c1"># forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="c1"># output_original_output=True,</span>
        <span class="c1"># output_attentions=extract_attn_pattern,</span>
    <span class="p">)</span>

    <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># since attention_patterns are returned in the output, we need to adapt to the cache structure</span>
    <span class="k">if</span> <span class="n">move_to_cpu</span><span class="p">:</span>
        <span class="n">cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">external_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">external_cache</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

    <span class="n">mapping_index</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">target_token_positions</span><span class="p">:</span>
        <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
            <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_dict</span><span class="p">[</span><span class="n">token</span><span class="p">])):</span>
                <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
                <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_dict</span><span class="p">)):</span>
                <span class="n">mapping_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
                <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Token dict must be an int, a dict or a list&quot;</span><span class="p">)</span>
    <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;mapping_index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_index</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">remove_hooks</span><span class="p">(</span><span class="n">hook_handlers</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cache</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.generate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_token_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_text</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p><strong>WARNING</strong>: This method could be buggy in the return dict of the output. Pay attention!</p>
<p>Generate new tokens using the model and the inputs passed as argument
Args:
    inputs (dict): dictionary with the inputs of the model {"input_ids": ..., "attention_mask": ..., "pixel_values": ...}
    generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration
    **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)
Returns:
    output (ActivationCache): dictionary with the output of the model</p>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">1235</span><span class="p">,</span> <span class="mi">102</span><span class="p">]]),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="go">{&#39;sequences&#39;: tensor([[101, 1234, 1235, 102]])}</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_token_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_text</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ActivationCache</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    __WARNING__: This method could be buggy in the return dict of the output. Pay attention!</span>

<span class="sd">    Generate new tokens using the model and the inputs passed as argument</span>
<span class="sd">    Args:</span>
<span class="sd">        inputs (dict): dictionary with the inputs of the model {&quot;input_ids&quot;: ..., &quot;attention_mask&quot;: ..., &quot;pixel_values&quot;: ...}</span>
<span class="sd">        generation_config (Optional[GenerationConfig]): original hf dataclass with the generation configuration</span>
<span class="sd">        **kwargs: additional arguments to control hooks generation (i.e. ablation_queries, patching_queries)</span>
<span class="sd">    Returns:</span>
<span class="sd">        output (ActivationCache): dictionary with the output of the model</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = {&quot;input_ids&quot;: torch.tensor([[101, 1234, 1235, 102]]), &quot;attention_mask&quot;: torch.tensor([[1, 1, 1, 1]])}</span>
<span class="sd">        &gt;&gt;&gt; model.generate(inputs)</span>
<span class="sd">        {&#39;sequences&#39;: tensor([[101, 1234, 1235, 102]])}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize cache for logits</span>
    <span class="c1"># TODO FIX THIS. IT is not general and it is not working</span>
    <span class="c1"># raise NotImplementedError(&quot;This method is not working. It needs to be fixed&quot;)</span>
    <span class="n">hook_handlers</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">target_token_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">string_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_string_tokens</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">get_input_ids</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">token_index</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">TokenIndex</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">pivot_positions</span><span class="o">=</span><span class="kc">None</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get_token_index</span><span class="p">(</span><span class="n">tokens</span><span class="o">=</span><span class="p">[],</span> <span class="n">string_tokens</span><span class="o">=</span><span class="n">string_tokens</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_index</span><span class="p">,</span> <span class="nb">list</span><span class="p">),</span> <span class="s2">&quot;Token index must be a list&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">),</span> <span class="s2">&quot;Token dict must be a dict&quot;</span>
        <span class="n">hooks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_hooks</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">token_dict</span><span class="o">=</span><span class="n">token_dict</span><span class="p">,</span>
            <span class="n">token_index</span><span class="o">=</span><span class="n">token_index</span><span class="p">,</span>
            <span class="n">cache</span><span class="o">=</span><span class="n">ActivationCache</span><span class="p">(),</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hook_handlers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_hooks</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_handler</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_device</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
        <span class="n">output_scores</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">hook_handlers</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_hooks</span><span class="p">(</span><span class="n">hook_handlers</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_text</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="n">output</span>  <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.get_module_from_string" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_module_from_string</span><span class="p">(</span><span class="n">component</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Return a module from the model given the string of the module.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>component</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the string of the module</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>module</code></td>            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the module of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">get_module_from_string</span><span class="p">(</span><span class="s2">&quot;model.layers[0].self_attn&quot;</span><span class="p">)</span>
<span class="go">BertAttention(...)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_module_from_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a module from the model given the string of the module.</span>

<span class="sd">    Args:</span>
<span class="sd">        component (str): the string of the module</span>

<span class="sd">    Returns:</span>
<span class="sd">        module (torch.nn.Module): the module of the model</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; model.get_module_from_string(&quot;model.layers[0].self_attn&quot;)</span>
<span class="sd">        BertAttention(...)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="o">.</span><span class="n">retrieve_modules_from_names</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.get_processor" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_processor</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Return the processor of the model (None if the model does not have a processor, i.e. text only model)</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>processor</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the processor of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the processor of the model (None if the model does not have a processor, i.e. text only model)</span>

<span class="sd">    Args:</span>
<span class="sd">        None</span>

<span class="sd">    Returns:</span>
<span class="sd">        processor: the processor of the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The model does not have a processor&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.get_text_tokenizer" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_text_tokenizer</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>tokenizer</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the tokenizer of the model</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_text_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If the tokenizer is a processor, return just the tokenizer. If the tokenizer is a tokenizer, return the tokenizer</span>

<span class="sd">    Args:</span>
<span class="sd">        None</span>

<span class="sd">    Returns:</span>
<span class="sd">        tokenizer: the tokenizer of the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="p">,</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The processor does not have a tokenizer&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.register_forward_hook" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">component</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Add a new hook to the model. The hook will be called in the forward pass of the model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>component</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the component of the model where the hook will be added.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hook_function</code>
            </td>
            <td>
                  <code><span title="typing.Callable">Callable</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the function that will be called in the forward pass of the model. The function must have the following signature:
def hook_function(module, input, output):
    pass</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">hook_function</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># your code here</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">pass</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="s2">&quot;model.layers[0].self_attn&quot;</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">register_forward_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">component</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">hook_function</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a new hook to the model. The hook will be called in the forward pass of the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        component (str): the component of the model where the hook will be added.</span>
<span class="sd">        hook_function (Callable): the function that will be called in the forward pass of the model. The function must have the following signature:</span>
<span class="sd">            def hook_function(module, input, output):</span>
<span class="sd">                pass</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; def hook_function(module, input, output):</span>
<span class="sd">        &gt;&gt;&gt;     # your code here</span>
<span class="sd">        &gt;&gt;&gt;     pass</span>
<span class="sd">        &gt;&gt;&gt; model.register_forward_hook(&quot;model.layers[0].self_attn&quot;, hook_function)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">additional_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;component&quot;</span><span class="p">:</span> <span class="n">component</span><span class="p">,</span>
            <span class="s2">&quot;intervention&quot;</span><span class="p">:</span> <span class="n">hook_function</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.remove_hooks" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">remove_hooks</span><span class="p">(</span><span class="n">hook_handlers</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Remove all the hooks from the model</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">remove_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook_handlers</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove all the hooks from the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">hook_handler</span> <span class="ow">in</span> <span class="n">hook_handlers</span><span class="p">:</span>
        <span class="n">hook_handler</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.set_hooks" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_hooks</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Set the hooks in the model</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>hooks</code>
            </td>
            <td>
                  <code>list[dict]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of dictionaries with the component and the intervention to perform in the forward pass of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>hook_handlers</code></td>            <td>
                  <code>list</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of hook handlers</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hooks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set the hooks in the model</span>

<span class="sd">    Args:</span>
<span class="sd">        hooks (list[dict]): list of dictionaries with the component and the intervention to perform in the forward pass of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">        hook_handlers (list): list of hook handlers</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hooks</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>

    <span class="n">hook_handlers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">hook</span> <span class="ow">in</span> <span class="n">hooks</span><span class="p">:</span>
        <span class="n">component</span> <span class="o">=</span> <span class="n">hook</span><span class="p">[</span><span class="s2">&quot;component&quot;</span><span class="p">]</span>
        <span class="n">hook_function</span> <span class="o">=</span> <span class="n">hook</span><span class="p">[</span><span class="s2">&quot;intervention&quot;</span><span class="p">]</span>

        <span class="c1"># get the last module string (.input or .output) and remove it from the component string</span>
        <span class="n">last_module</span> <span class="o">=</span> <span class="n">component</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># now remove the last module from the component string</span>
        <span class="n">component</span> <span class="o">=</span> <span class="n">component</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">last_module</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># check if the component exists in the model</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assert_module_exists</span><span class="p">(</span><span class="n">component</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Probably the module </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2"> do not exists in the model. If the module is the attention_matrix_hook, try callig HookedModel.set_custom_hooks() or setting attn_implementation == &#39;custom_eager&#39;.  Now we will skip the hook for the component </span><span class="si">{</span><span class="n">component</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">std_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">last_module</span> <span class="o">==</span> <span class="s2">&quot;input&quot;</span><span class="p">:</span>
            <span class="n">hook_handlers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">get_module_by_path</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">component</span>
                <span class="p">)</span><span class="o">.</span><span class="n">register_forward_pre_hook</span><span class="p">(</span>
                    <span class="n">partial</span><span class="p">(</span><span class="n">hook_function</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">last_module</span> <span class="o">==</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span>
            <span class="n">hook_handlers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">get_module_by_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">,</span> <span class="n">component</span><span class="p">)</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span>
                    <span class="n">hook_function</span><span class="p">,</span> <span class="n">with_kwargs</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">hook_handlers</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.hooked_model.HookedModel.to_string_tokens" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">to_string_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Transform a list or a tensor of tokens in a list of string tokens.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>tokens</code>
            </td>
            <td>
                  <code><span title="typing.Union">Union</span>[list, <span title="torch.Tensor">Tensor</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the tokens to transform in string tokens</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>string_tokens</code></td>            <td>
                  <code>list</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the list of string tokens</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Examples:</span></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">1235</span><span class="p">,</span> <span class="mi">102</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">to_string_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="go">[&#39;[CLS]&#39;, &#39;hello&#39;, &#39;world&#39;, &#39;[SEP]&#39;]</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_string_tokens</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform a list or a tensor of tokens in a list of string tokens.</span>

<span class="sd">    Args:</span>
<span class="sd">        tokens (Union[list, torch.Tensor]): the tokens to transform in string tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        string_tokens (list): the list of string tokens</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tokens = [101, 1234, 1235, 102]</span>
<span class="sd">        &gt;&gt;&gt; model.to_string_tokens(tokens)</span>
<span class="sd">        [&#39;[CLS]&#39;, &#39;hello&#39;, &#39;world&#39;, &#39;[SEP]&#39;]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tokens</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">string_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">string_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="n">string_tokens</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="easyroutine.interpretability.hooked_model.HookedModelConfig" class="doc doc-heading">
            <code>HookedModelConfig</code>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


        <p>Configuration of the HookedModel</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model_name</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the name of the model to load</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device_map</code>
            </td>
            <td>
                  <code><span title="typing.Literal">Literal</span>[&#39;balanced&#39;, &#39;cuda&#39;, &#39;cpu&#39;, &#39;auto&#39;]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the device to use for the model</p>
              </div>
            </td>
            <td>
                  <code>&#39;balanced&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>torch_dtype</code>
            </td>
            <td>
                  <code><span title="torch.dtype">dtype</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the dtype of the model</p>
              </div>
            </td>
            <td>
                  <code><span title="torch.bfloat16">bfloat16</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>attn_implementation</code>
            </td>
            <td>
                  <code><span title="typing.Literal">Literal</span>[&#39;eager&#39;, &#39;flash_attention_2&#39;]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the implementation of the attention</p>
              </div>
            </td>
            <td>
                  <code>&#39;custom_eager&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>batch_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>the batch size of the model. FOR NOW, ONLY BATCH SIZE 1 IS SUPPORTED. USE AT YOUR OWN RISK</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/hooked_model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">HookedModelConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Configuration of the HookedModel</span>

<span class="sd">    Arguments:</span>
<span class="sd">        model_name (str): the name of the model to load</span>
<span class="sd">        device_map (Literal[&quot;balanced&quot;, &quot;cuda&quot;, &quot;cpu&quot;, &quot;auto&quot;]): the device to use for the model</span>
<span class="sd">        torch_dtype (torch.dtype): the dtype of the model</span>
<span class="sd">        attn_implementation (Literal[&quot;eager&quot;, &quot;flash_attention_2&quot;]): the implementation of the attention</span>
<span class="sd">        batch_size (int): the batch size of the model. FOR NOW, ONLY BATCH SIZE 1 IS SUPPORTED. USE AT YOUR OWN RISK</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">device_map</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;balanced&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;auto&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;balanced&quot;</span>
    <span class="n">torch_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">attn_implementation</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;eager&quot;</span><span class="p">,</span> <span class="s2">&quot;custom_eager&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;custom_eager&quot;</span>  <span class="c1"># TODO: add flash_attention_2 in custom module to support it</span>
    <span class="p">)</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://easyroutine.github.io/api/interpretability/module_wrapper/">
      
      
        <link rel="prev" href="../models/">
      
      
        <link rel="next" href="../token_index/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Module Wrapper - EasyRoutine</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-wrapper" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="EasyRoutine" class="md-header__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EasyRoutine
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Module Wrapper
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="EasyRoutine" class="md-nav__button md-logo" aria-label="EasyRoutine" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    EasyRoutine
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Api
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Api
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Interpretability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Interpretability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activation_cache/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Activation cache
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hooked_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hooked model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hooks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hooks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Module Wrapper
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Module Wrapper
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#manager-wrappers-and-abstract-base-class" class="md-nav__link">
    <span class="md-ellipsis">
      Manager Wrappers and Abstract Base Class
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Manager Wrappers and Abstract Base Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager" class="md-nav__link">
    <span class="md-ellipsis">
      manager
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionWrapperFactory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AttentionWrapperFactory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory.get_wrapper_class" class="md-nav__link">
    <span class="md-ellipsis">
      get_wrapper_class
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager" class="md-nav__link">
    <span class="md-ellipsis">
      ModuleWrapperManager
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ModuleWrapperManager">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.restore_original_attention_module" class="md-nav__link">
    <span class="md-ellipsis">
      restore_original_attention_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.substitute_attention_module" class="md-nav__link">
    <span class="md-ellipsis">
      substitute_attention_module
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base" class="md-nav__link">
    <span class="md-ellipsis">
      base
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.AttentionMatrixHookModule" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionMatrixHookModule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      BaseAttentionWrapper
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BaseAttentionWrapper">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.__getattr__" class="md-nav__link">
    <span class="md-ellipsis">
      __getattr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.original_name" class="md-nav__link">
    <span class="md-ellipsis">
      original_name
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specific-module-wrappers" class="md-nav__link">
    <span class="md-ellipsis">
      Specific Module Wrappers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Specific Module Wrappers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention" class="md-nav__link">
    <span class="md-ellipsis">
      llama_attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      LlamaAttentionWrapper
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LlamaAttentionWrapper">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention.repeat_kv" class="md-nav__link">
    <span class="md-ellipsis">
      repeat_kv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention" class="md-nav__link">
    <span class="md-ellipsis">
      chameleon_attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.ChameleonAttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      ChameleonAttentionWrapper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.apply_rotary_pos_emb" class="md-nav__link">
    <span class="md-ellipsis">
      apply_rotary_pos_emb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.repeat_kv" class="md-nav__link">
    <span class="md-ellipsis">
      repeat_kv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.rotate_half" class="md-nav__link">
    <span class="md-ellipsis">
      rotate_half
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.T5_attention" class="md-nav__link">
    <span class="md-ellipsis">
      T5_attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      T5AttentionWrapper
    </span>
  </a>
  
    <nav class="md-nav" aria-label="T5AttentionWrapper">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../token_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Token index
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#manager-wrappers-and-abstract-base-class" class="md-nav__link">
    <span class="md-ellipsis">
      Manager Wrappers and Abstract Base Class
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Manager Wrappers and Abstract Base Class">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager" class="md-nav__link">
    <span class="md-ellipsis">
      manager
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionWrapperFactory
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AttentionWrapperFactory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory.get_wrapper_class" class="md-nav__link">
    <span class="md-ellipsis">
      get_wrapper_class
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager" class="md-nav__link">
    <span class="md-ellipsis">
      ModuleWrapperManager
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ModuleWrapperManager">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.restore_original_attention_module" class="md-nav__link">
    <span class="md-ellipsis">
      restore_original_attention_module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.substitute_attention_module" class="md-nav__link">
    <span class="md-ellipsis">
      substitute_attention_module
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base" class="md-nav__link">
    <span class="md-ellipsis">
      base
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.AttentionMatrixHookModule" class="md-nav__link">
    <span class="md-ellipsis">
      AttentionMatrixHookModule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      BaseAttentionWrapper
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BaseAttentionWrapper">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.__getattr__" class="md-nav__link">
    <span class="md-ellipsis">
      __getattr__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.original_name" class="md-nav__link">
    <span class="md-ellipsis">
      original_name
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specific-module-wrappers" class="md-nav__link">
    <span class="md-ellipsis">
      Specific Module Wrappers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Specific Module Wrappers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention" class="md-nav__link">
    <span class="md-ellipsis">
      llama_attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      LlamaAttentionWrapper
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LlamaAttentionWrapper">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.llama_attention.repeat_kv" class="md-nav__link">
    <span class="md-ellipsis">
      repeat_kv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention" class="md-nav__link">
    <span class="md-ellipsis">
      chameleon_attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.ChameleonAttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      ChameleonAttentionWrapper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.apply_rotary_pos_emb" class="md-nav__link">
    <span class="md-ellipsis">
      apply_rotary_pos_emb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.repeat_kv" class="md-nav__link">
    <span class="md-ellipsis">
      repeat_kv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.chameleon_attention.rotate_half" class="md-nav__link">
    <span class="md-ellipsis">
      rotate_half
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.T5_attention" class="md-nav__link">
    <span class="md-ellipsis">
      T5_attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper" class="md-nav__link">
    <span class="md-ellipsis">
      T5AttentionWrapper
    </span>
  </a>
  
    <nav class="md-nav" aria-label="T5AttentionWrapper">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="module-wrapper">Module Wrapper</h1>
<h2 id="introduction">Introduction</h2>
<p>Module Wrapper is the submodule that is responsible for managing the module wrappers. The module wrappers are essential to add custom hook where in the original transfomer codebase the hook is not available. For example, the <code>transformer</code> module does not have a hook to get the attention matrix of a head. The module wrapper is used to add this hook. The <code>module_wrapper</code>  submodel is composed of the following files:
    - <code>manager.py</code>: The manager file is responsible for managing the module wrappers. It is the standard interface to add the wrap around models.
    - <code>base.py</code>: The base file is the base class for the module wrapper. Implement a base form of a Wrapper class.
    - <code>model_name_attention.py</code>: The model name attention file is the module wrapper for the attention matrix of a single model. When add a new model, add a new file with the name <code>model_name_attention.py</code> and implement the <code>ModelNameAttention</code> class. It is basically a copy of the forward pass of the attention module with the addition of the hook to get the attention matrix. </p>
<h2 id="manager-wrappers-and-abstract-base-class">Manager Wrappers and Abstract Base Class</h2>


<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.module_wrappers.manager"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory" class="doc doc-heading">
            <code>AttentionWrapperFactory</code>


</h3>


    <div class="doc doc-contents ">


        <p>Maps a given model name to the correct attention wrapper class.</p>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AttentionWrapperFactory</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Maps a given model name to the correct attention wrapper class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">AVAILABLE_MODULE_WRAPPERS</span><span class="p">:</span><span class="nb">dict</span> <span class="o">=</span> <span class="p">{</span>
                                    <span class="n">ChameleonAttentionWrapper</span><span class="o">.</span><span class="n">original_name</span><span class="p">():</span> <span class="n">ChameleonAttentionWrapper</span><span class="p">,</span> 
                                    <span class="n">LlamaAttentionWrapper</span><span class="o">.</span><span class="n">original_name</span><span class="p">():</span> <span class="n">LlamaAttentionWrapper</span><span class="p">,</span> 
                                    <span class="n">T5AttentionWrapper</span><span class="o">.</span><span class="n">original_name</span><span class="p">():</span> <span class="n">T5AttentionWrapper</span><span class="p">,</span> 
                                    <span class="n">MistralAttentionWrapper</span><span class="o">.</span><span class="n">original_name</span><span class="p">():</span> <span class="n">MistralAttentionWrapper</span> 
    <span class="p">}</span>

    <span class="c1"># MODEL_NAME_TO_WRAPPER = {</span>
    <span class="c1">#     &quot;facebook/chameleon-7b&quot;: ChameleonAttentionWrapper,</span>
    <span class="c1">#     &quot;facebook/chameleon-30b&quot;: ChameleonAttentionWrapper,</span>
    <span class="c1">#     &quot;mistral-community/pixtral-12b&quot;: LlamaAttentionWrapper,</span>
    <span class="c1">#     &quot;llava-hf/llava-v1.6-mistral-7b-hf&quot;: LlamaAttentionWrapper,</span>
    <span class="c1">#     &quot;hf-internal-testing/tiny-random-LlamaForCausalLM&quot;: LlamaAttentionWrapper,</span>
    <span class="c1">#     &quot;ChoereForAI/aya-101&quot;: T5AttentionWrapper,</span>
    <span class="c1"># }</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_wrapper_class</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">Type</span><span class="p">[</span><span class="n">ChameleonAttentionWrapper</span><span class="p">],</span>
        <span class="n">Type</span><span class="p">[</span><span class="n">LlamaAttentionWrapper</span><span class="p">],</span>
        <span class="n">Type</span><span class="p">[</span><span class="n">T5AttentionWrapper</span><span class="p">],</span>
        <span class="n">Type</span><span class="p">[</span><span class="n">MistralAttentionWrapper</span><span class="p">],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the attention wrapper class for the specified model name.</span>
<span class="sd">        Raises a ValueError if the model is not supported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_modules</span> <span class="o">=</span> <span class="n">find_all_modules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">return_only_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">candidate_name</span><span class="p">,</span> <span class="n">candidate_wrappers</span> <span class="ow">in</span> <span class="n">AttentionWrapperFactory</span><span class="o">.</span><span class="n">AVAILABLE_MODULE_WRAPPERS</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">candidate_name</span> <span class="ow">in</span> <span class="n">all_modules</span><span class="p">:</span>
                <span class="n">LambdaLogger</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found a wrapper for </span><span class="si">{</span><span class="n">candidate_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">candidate_wrappers</span>

        <span class="n">LambdaLogger</span><span class="p">()</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Do not have any wrapper for </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.manager.AttentionWrapperFactory.get_wrapper_class" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_wrapper_class</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Returns the attention wrapper class for the specified model name.
Raises a ValueError if the model is not supported.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_wrapper_class</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">Type</span><span class="p">[</span><span class="n">ChameleonAttentionWrapper</span><span class="p">],</span>
    <span class="n">Type</span><span class="p">[</span><span class="n">LlamaAttentionWrapper</span><span class="p">],</span>
    <span class="n">Type</span><span class="p">[</span><span class="n">T5AttentionWrapper</span><span class="p">],</span>
    <span class="n">Type</span><span class="p">[</span><span class="n">MistralAttentionWrapper</span><span class="p">],</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the attention wrapper class for the specified model name.</span>
<span class="sd">    Raises a ValueError if the model is not supported.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_modules</span> <span class="o">=</span> <span class="n">find_all_modules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">return_only_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">candidate_name</span><span class="p">,</span> <span class="n">candidate_wrappers</span> <span class="ow">in</span> <span class="n">AttentionWrapperFactory</span><span class="o">.</span><span class="n">AVAILABLE_MODULE_WRAPPERS</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">candidate_name</span> <span class="ow">in</span> <span class="n">all_modules</span><span class="p">:</span>
            <span class="n">LambdaLogger</span><span class="p">()</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found a wrapper for </span><span class="si">{</span><span class="n">candidate_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">candidate_wrappers</span>

    <span class="n">LambdaLogger</span><span class="p">()</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Do not have any wrapper for </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager" class="doc doc-heading">
            <code>ModuleWrapperManager</code>


</h3>


    <div class="doc doc-contents ">


        <p>Handles the logic of replacing an original attention class within a given model
with a custom attention wrapper, based on user-specified model_name.
Also allows restoring the original modules if needed, using a single
recursive function.</p>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ModuleWrapperManager</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Handles the logic of replacing an original attention class within a given model</span>
<span class="sd">    with a custom attention wrapper, based on user-specified model_name.</span>
<span class="sd">    Also allows restoring the original modules if needed, using a single</span>
<span class="sd">    recursive function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">log_level</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;INFO&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the manager with a given model name.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">logname</span><span class="o">=</span><span class="s2">&quot;ModuleWrapperManager&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log_level</span><span class="p">)</span>

        <span class="c1"># Fetch the appropriate wrapper class for the given model name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_wrapper_class</span> <span class="o">=</span> <span class="n">AttentionWrapperFactory</span><span class="o">.</span><span class="n">get_wrapper_class</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># TODO: extend to support multiple module type for model</span>
        <span class="c1"># The original attention class name is fetched via a class method or attribute in the wrapper</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_module_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_wrapper_class</span><span class="o">.</span><span class="n">original_name</span><span class="p">()</span> <span class="c1"># TODO: extend to support multiple module type for model</span>

        <span class="c1"># Dictionary to store submodule_path -&gt; original attention module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_modules</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_name</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">module_name</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_module_name</span> <span class="c1"># TODO: extend to support multiple module type for model</span>

    <span class="k">def</span> <span class="nf">substitute_attention_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Public method that performs the substitution of attention modules in the model.</span>
<span class="sd">        Logs each replacement. This will replace *all* modules whose class name</span>
<span class="sd">        matches `self.target_module_name`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_and_modify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parent_path</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;substitute&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">restore_original_attention_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Public method that restores the original attention modules in the model.</span>
<span class="sd">        Logs each restoration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_and_modify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parent_path</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;restore&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_traverse_and_modify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">parent_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recursively traverses `module` and either substitutes or restores each matching</span>
<span class="sd">        submodule, depending on `mode`.</span>

<span class="sd">        - mode=&quot;substitute&quot;: Replaces the original module (with class name == self.target_module_name)</span>
<span class="sd">                            with the wrapper, storing the original in self.original_modules.</span>
<span class="sd">        - mode=&quot;restore&quot;: Replaces the wrapper submodule (class name == self.attention_wrapper_class.__name__)</span>
<span class="sd">                        with the original module from self.original_modules.</span>

<span class="sd">        Args:</span>
<span class="sd">            module (nn.Module): The current module to inspect.</span>
<span class="sd">            parent_path (str): A string that tracks the &#39;path&#39; of this submodule in the overall model hierarchy.</span>
<span class="sd">            mode (str): Either &quot;substitute&quot; or &quot;restore&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">()):</span>
            <span class="c1"># Identify the submodule path (e.g. &quot;encoder.layer.0.attention&quot;)</span>
            <span class="n">submodule_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">parent_path</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">parent_path</span> <span class="k">else</span> <span class="n">name</span>

            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;substitute&quot;</span><span class="p">:</span>
                <span class="c1"># Look for the original module class name</span>
                <span class="k">if</span> <span class="n">child</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_module_name</span><span class="p">:</span>
                    <span class="c1"># Store the original</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">original_modules</span><span class="p">[</span><span class="n">submodule_path</span><span class="p">]</span> <span class="o">=</span> <span class="n">child</span>
                    <span class="c1"># Wrap it</span>
                    <span class="n">wrapped_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_wrapper_class</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">wrapped_module</span><span class="p">)</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Substituted &#39;</span><span class="si">{</span><span class="n">submodule_path</span><span class="si">}</span><span class="s2">&#39; with wrapper for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">target_module_name</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Recurse</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_and_modify</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">submodule_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;substitute&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;restore&quot;</span><span class="p">:</span>
                <span class="c1"># Look for the wrapper class name</span>
                <span class="k">if</span> <span class="n">child</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_wrapper_class</span><span class="o">.</span><span class="vm">__name__</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">submodule_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_modules</span><span class="p">:</span>
                        <span class="n">original_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_modules</span><span class="p">[</span><span class="n">submodule_path</span><span class="p">]</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">original_module</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Restored &#39;</span><span class="si">{</span><span class="n">submodule_path</span><span class="si">}</span><span class="s2">&#39; to original </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">target_module_name</span><span class="si">}</span><span class="s2">.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Found a wrapped submodule &#39;</span><span class="si">{</span><span class="n">submodule_path</span><span class="si">}</span><span class="s2">&#39; but no original stored. Skipping.&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Recurse</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_and_modify</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">submodule_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;restore&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">log_level</span><span class="o">=</span><span class="s1">&#39;INFO&#39;</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Initializes the manager with a given model name.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">log_level</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;INFO&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the manager with a given model name.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">Logger</span><span class="p">(</span><span class="n">logname</span><span class="o">=</span><span class="s2">&quot;ModuleWrapperManager&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log_level</span><span class="p">)</span>

    <span class="c1"># Fetch the appropriate wrapper class for the given model name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_wrapper_class</span> <span class="o">=</span> <span class="n">AttentionWrapperFactory</span><span class="o">.</span><span class="n">get_wrapper_class</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># TODO: extend to support multiple module type for model</span>
    <span class="c1"># The original attention class name is fetched via a class method or attribute in the wrapper</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">target_module_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_wrapper_class</span><span class="o">.</span><span class="n">original_name</span><span class="p">()</span> <span class="c1"># TODO: extend to support multiple module type for model</span>

    <span class="c1"># Dictionary to store submodule_path -&gt; original attention module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">original_modules</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.restore_original_attention_module" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">restore_original_attention_module</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Public method that restores the original attention modules in the model.
Logs each restoration.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">restore_original_attention_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Public method that restores the original attention modules in the model.</span>
<span class="sd">    Logs each restoration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_and_modify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parent_path</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;restore&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.manager.ModuleWrapperManager.substitute_attention_module" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">substitute_attention_module</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Public method that performs the substitution of attention modules in the model.
Logs each replacement. This will replace <em>all</em> modules whose class name
matches <code>self.target_module_name</code>.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/manager.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">substitute_attention_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Public method that performs the substitution of attention modules in the model.</span>
<span class="sd">    Logs each replacement. This will replace *all* modules whose class name</span>
<span class="sd">    matches `self.target_module_name`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_traverse_and_modify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">parent_path</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;substitute&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.module_wrappers.base"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.base.AttentionMatrixHookModule" class="doc doc-heading">
            <code>AttentionMatrixHookModule</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Computation of the attention matrix. <em>Note</em>: it has been added just for adding custom hooks.</p>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AttentionMatrixHookModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computation of the attention matrix. *Note*: it has been added just for adding custom hooks.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">attention_matrix</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="n">attention_matrix</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper" class="doc doc-heading">
            <code>BaseAttentionWrapper</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>A base class for wrapping an original attention module.</p>


<details class="provides" open>
  <summary>Provides</summary>
  <p><code>_orig_module</code> to store the real (unwrapped) attention.
A robust <code>__getattr__</code> that checks:
    1) self.<strong>dict</strong>
    2) self._modules
    3) the base class
    4) fallback to <code>_orig_module</code></p>
</details>





              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BaseAttentionWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A base class for wrapping an original attention module.</span>

<span class="sd">    Provides:</span>
<span class="sd">        `_orig_module` to store the real (unwrapped) attention.</span>
<span class="sd">        A robust `__getattr__` that checks:</span>
<span class="sd">            1) self.__dict__</span>
<span class="sd">            2) self._modules</span>
<span class="sd">            3) the base class</span>
<span class="sd">            4) fallback to `_orig_module`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># store the original module in a private attribute</span>
        <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_orig_module&quot;</span><span class="p">,</span> <span class="n">original_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If name is not in this wrapper, fall back to the original module.</span>
<span class="sd">        Also checks `self._modules` for submodules, because PyTorch</span>
<span class="sd">        automatically places them there.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># 1) get this wrapper&#39;s __dict__</span>
        <span class="n">wrapper_dict</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;__dict__&quot;</span><span class="p">)</span>

        <span class="c1"># 2) if name is in our own instance dictionary, return it</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">wrapper_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">wrapper_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

        <span class="c1"># 3) if name is in our submodules, return it</span>
        <span class="n">modules_dict</span> <span class="o">=</span> <span class="n">wrapper_dict</span><span class="p">[</span><span class="s2">&quot;_modules&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">modules_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">modules_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

        <span class="c1"># 4) check if name is in our class (methods, etc.)</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

        <span class="c1"># 5) fallback to _orig_module</span>
        <span class="n">orig</span> <span class="o">=</span> <span class="n">wrapper_dict</span><span class="p">[</span><span class="s2">&quot;_orig_module&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">original_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        By default, you might override this in each derived class if you want</span>
<span class="sd">        your manager code to know which original class name this wrapper replaces.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;BaseAttention&quot;</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.__getattr__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>If name is not in this wrapper, fall back to the original module.
Also checks <code>self._modules</code> for submodules, because PyTorch
automatically places them there.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If name is not in this wrapper, fall back to the original module.</span>
<span class="sd">    Also checks `self._modules` for submodules, because PyTorch</span>
<span class="sd">    automatically places them there.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 1) get this wrapper&#39;s __dict__</span>
    <span class="n">wrapper_dict</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;__dict__&quot;</span><span class="p">)</span>

    <span class="c1"># 2) if name is in our own instance dictionary, return it</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">wrapper_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">wrapper_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

    <span class="c1"># 3) if name is in our submodules, return it</span>
    <span class="n">modules_dict</span> <span class="o">=</span> <span class="n">wrapper_dict</span><span class="p">[</span><span class="s2">&quot;_modules&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">modules_dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">modules_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

    <span class="c1"># 4) check if name is in our class (methods, etc.)</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># 5) fallback to _orig_module</span>
    <span class="n">orig</span> <span class="o">=</span> <span class="n">wrapper_dict</span><span class="p">[</span><span class="s2">&quot;_orig_module&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper.original_name" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">original_name</span><span class="p">()</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>By default, you might override this in each derived class if you want
your manager code to know which original class name this wrapper replaces.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/base.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">original_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By default, you might override this in each derived class if you want</span>
<span class="sd">    your manager code to know which original class name this wrapper replaces.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;BaseAttention&quot;</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div><h2 id="specific-module-wrappers">Specific Module Wrappers</h2>


<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.module_wrappers.llama_attention"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper" class="doc doc-heading">
            <code>LlamaAttentionWrapper</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper" href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper">BaseAttentionWrapper</a></code></p>


        <p>A wrapper around the original LlamaAttention. It has:
- The same named attributes (q_proj, k_proj, etc.), which are references
    to the original module's submodules/parameters.
- A private reference (<code>_orig_attn</code>) to the entire original attention,
    for falling back if something isn't found on the wrapper itself.
- An additional <code>attention_matrix_hook</code> for intercepting attention.</p>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LlamaAttentionWrapper</span><span class="p">(</span><span class="n">BaseAttentionWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper around the original LlamaAttention. It has:</span>
<span class="sd">    - The same named attributes (q_proj, k_proj, etc.), which are references</span>
<span class="sd">        to the original module&#39;s submodules/parameters.</span>
<span class="sd">    - A private reference (`_orig_attn`) to the entire original attention,</span>
<span class="sd">        for falling back if something isn&#39;t found on the wrapper itself.</span>
<span class="sd">    - An additional `attention_matrix_hook` for intercepting attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">original_name</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;LlamaAttention&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_attention</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Store references to all relevant submodules so the wrapper</span>
<span class="sd">        &quot;feels&quot; the same. Also store a reference to the original module</span>
<span class="sd">        in a private attribute for fallback.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">original_attention</span><span class="p">)</span>

        <span class="c1"># This is the private reference to the entire original attention.</span>
        <span class="c1"># We&#39;ll fallback to it for any attribute we haven&#39;t explicitly set.</span>
        <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_orig_attn&quot;</span><span class="p">,</span> <span class="n">original_attention</span><span class="p">)</span>

        <span class="c1"># Now replicate the original attention&#39;s submodules as attributes of *this* wrapper.</span>
        <span class="c1"># These are direct references, not new modules:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">q_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">k_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">v_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">o_proj</span>

        <span class="c1"># Copy over any scalar attributes you need</span>
        <span class="c1"># self.num_heads = original_attention.num_heads</span>
        <span class="c1"># self.num_key_value_heads = original_attention.num_key_value_heads</span>
        <span class="c1"># self.num_key_value_groups = original_attention.num_key_value_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">head_dim</span>
        <span class="c1"># self.hidden_size = original_attention.hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">layer_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">config</span>

        <span class="c1"># Add your custom hook module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span> <span class="o">=</span> <span class="n">AttentionMatrixHookModule</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">position_embeddings</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Cache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Unpack</span><span class="p">[</span><span class="n">FlashAttentionKwargs</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">hidden_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">position_embeddings</span>
        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span> <span class="n">sin</span><span class="p">,</span> <span class="s2">&quot;cos&quot;</span><span class="p">:</span> <span class="n">cos</span><span class="p">,</span> <span class="s2">&quot;cache_position&quot;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">}</span>
            <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">cache_kwargs</span>
            <span class="p">)</span>

        <span class="c1"># Inline eager_attention_forward logic</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">+</span> <span class="n">causal_mask</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">attn_weights</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># End inline</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.llama_attention.LlamaAttentionWrapper.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">original_attention</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Store references to all relevant submodules so the wrapper
"feels" the same. Also store a reference to the original module
in a private attribute for fallback.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_attention</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Store references to all relevant submodules so the wrapper</span>
<span class="sd">    &quot;feels&quot; the same. Also store a reference to the original module</span>
<span class="sd">    in a private attribute for fallback.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">original_attention</span><span class="p">)</span>

    <span class="c1"># This is the private reference to the entire original attention.</span>
    <span class="c1"># We&#39;ll fallback to it for any attribute we haven&#39;t explicitly set.</span>
    <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_orig_attn&quot;</span><span class="p">,</span> <span class="n">original_attention</span><span class="p">)</span>

    <span class="c1"># Now replicate the original attention&#39;s submodules as attributes of *this* wrapper.</span>
    <span class="c1"># These are direct references, not new modules:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">q_proj</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">k_proj</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">v_proj</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">o_proj</span>

    <span class="c1"># Copy over any scalar attributes you need</span>
    <span class="c1"># self.num_heads = original_attention.num_heads</span>
    <span class="c1"># self.num_key_value_heads = original_attention.num_key_value_heads</span>
    <span class="c1"># self.num_key_value_groups = original_attention.num_key_value_groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">head_dim</span>
    <span class="c1"># self.hidden_size = original_attention.hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">attention_dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">layer_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">config</span>

    <span class="c1"># Add your custom hook module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span> <span class="o">=</span> <span class="n">AttentionMatrixHookModule</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.module_wrappers.llama_attention.repeat_kv" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">repeat_kv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>(batch, num_key_value_heads, seq_len, head_dim)
    -&gt; (batch, num_attention_heads, seq_len, head_dim)</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/llama_attention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">repeat_kv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    (batch, num_key_value_heads, seq_len, head_dim)</span>
<span class="sd">        -&gt; (batch, num_attention_heads, seq_len, head_dim)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bsz</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">n_rep</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_kv_heads</span> <span class="o">*</span> <span class="n">n_rep</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.module_wrappers.chameleon_attention"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.chameleon_attention.ChameleonAttentionWrapper" class="doc doc-heading">
            <code>ChameleonAttentionWrapper</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper" href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper">BaseAttentionWrapper</a></code></p>


        <p>Attention wrapper for the Chameleon model.</p>






              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChameleonAttentionWrapper</span><span class="p">(</span><span class="n">BaseAttentionWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention wrapper for the Chameleon model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">original_name</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;ChameleonAttention&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_attention</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">original_attention</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">q_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">k_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">v_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">q_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_norm</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">k_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">o_proj</span>
        <span class="c1"># self.softmax = original_attention.softmax</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">layer_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">num_key_value_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">rotary_emb</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span> <span class="o">=</span> <span class="n">AttentionMatrixHookModule</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">original_attention</span> <span class="o">=</span> <span class="n">original_attention</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Cache</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span><span class="p">(</span><span class="n">query_states</span><span class="p">)</span>

        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_norm</span><span class="p">(</span><span class="n">key_states</span><span class="p">)</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># sin and cos are specific to RoPE models; position_ids needed for the static cache</span>
            <span class="n">cache_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sin&quot;</span><span class="p">:</span> <span class="n">sin</span><span class="p">,</span> <span class="s2">&quot;cos&quot;</span><span class="p">:</span> <span class="n">cos</span><span class="p">,</span> <span class="s2">&quot;cache_position&quot;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">}</span>
            <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">cache_kwargs</span><span class="p">)</span>

        <span class="n">key_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">key_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">repeat_kv</span><span class="p">(</span><span class="n">value_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># no matter the length, we just slice it</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">+</span> <span class="n">causal_mask</span>

        <span class="c1"># upcast attention to fp32</span>
        <span class="c1"># attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`attn_output` should be of size </span><span class="si">{</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="w"> </span><span class="n">q_len</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="si">}</span><span class="s2">, but is&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">q_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_weights</span><span class="p">,</span> <span class="n">past_key_value</span> <span class="c1"># type: ignore</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.module_wrappers.chameleon_attention.apply_rotary_pos_emb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">unsqueeze_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Applies Rotary Position Embedding to the query and key tensors.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>q</code>
            </td>
            <td>
                  <code>`torch.Tensor`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The query tensor.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>k</code>
            </td>
            <td>
                  <code>`torch.Tensor`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The key tensor.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cos</code>
            </td>
            <td>
                  <code>`torch.Tensor`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The cosine part of the rotary embedding.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>sin</code>
            </td>
            <td>
                  <code>`torch.Tensor`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The sine part of the rotary embedding.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>position_ids</code>
            </td>
            <td>
                  <code>`torch.Tensor`, *optional*</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Deprecated and unused.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>unsqueeze_dim</code>
            </td>
            <td>
                  <code>`int`, *optional*, defaults to 1</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:
    <code>tuple(torch.Tensor)</code> comprising of the query and key tensors rotated using the Rotary Position Embedding.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">apply_rotary_pos_emb</span><span class="p">(</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">unsqueeze_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Applies Rotary Position Embedding to the query and key tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        q (`torch.Tensor`): The query tensor.</span>
<span class="sd">        k (`torch.Tensor`): The key tensor.</span>
<span class="sd">        cos (`torch.Tensor`): The cosine part of the rotary embedding.</span>
<span class="sd">        sin (`torch.Tensor`): The sine part of the rotary embedding.</span>
<span class="sd">        position_ids (`torch.Tensor`, *optional*):</span>
<span class="sd">            Deprecated and unused.</span>
<span class="sd">        unsqueeze_dim (`int`, *optional*, defaults to 1):</span>
<span class="sd">            The &#39;unsqueeze_dim&#39; argument specifies the dimension along which to unsqueeze cos[position_ids] and</span>
<span class="sd">            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note</span>
<span class="sd">            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and</span>
<span class="sd">            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes</span>
<span class="sd">            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have</span>
<span class="sd">            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.</span>
<span class="sd">    Returns:</span>
<span class="sd">        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">unsqueeze_dim</span><span class="p">)</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">unsqueeze_dim</span><span class="p">)</span>
    <span class="n">q_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    <span class="n">k_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_embed</span><span class="p">,</span> <span class="n">k_embed</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.module_wrappers.chameleon_attention.repeat_kv" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">repeat_kv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">repeat_kv</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,</span>
<span class="sd">    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">n_rep</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">n_rep</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">n_rep</span><span class="p">,</span> <span class="n">slen</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="easyroutine.interpretability.module_wrappers.chameleon_attention.rotate_half" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Rotates half the hidden dims of the input.</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/chameleon_attention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rotates half the hidden dims of the input.&quot;&quot;&quot;</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="easyroutine.interpretability.module_wrappers.T5_attention"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper" class="doc doc-heading">
            <code>T5AttentionWrapper</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper" href="#easyroutine.interpretability.module_wrappers.base.BaseAttentionWrapper">BaseAttentionWrapper</a></code></p>







              <details class="quote">
                <summary>Source code in <code>easyroutine/interpretability/module_wrappers/T5_attention.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">T5AttentionWrapper</span><span class="p">(</span><span class="n">BaseAttentionWrapper</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">original_name</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;T5Attention&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_attention</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">original_attention</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">v</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">o</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">layer_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">inner_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_attention_bias</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">original_attention</span><span class="o">.</span><span class="n">has_relative_attention_bias</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="n">original_attention</span><span class="o">.</span><span class="n">pruned_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span> <span class="o">=</span> <span class="n">AttentionMatrixHookModule</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_attention</span> <span class="o">=</span> <span class="n">original_attention</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">key_value_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">layer_head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">query_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">cache_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Input is (batch_size, seq_length, dim)</span>
        <span class="c1"># Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># if key_value_states are provided this layer is used as a cross-attention layer for the decoder</span>
        <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">key_value_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
        <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">is_updated</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">is_updated</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="c1"># after the first generated id, we can subsequently re-use all key/value_states from cache</span>
                <span class="n">curr_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">cross_attention_cache</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">curr_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">self_attention_cache</span>

        <span class="n">current_states</span> <span class="o">=</span> <span class="n">key_value_states</span> <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="k">else</span> <span class="n">hidden_states</span>
        <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">is_updated</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># reuse k,v, cross_attentions</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="n">curr_past_key_value</span><span class="o">.</span><span class="n">key_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="n">curr_past_key_value</span><span class="o">.</span><span class="n">value_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">current_states</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">current_states</span><span class="p">)</span>
            <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
            <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># save all key/value_states to cache to be re-used for fast auto-regressive generation</span>
                <span class="n">cache_position</span> <span class="o">=</span> <span class="n">cache_position</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cross_attention</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">curr_past_key_value</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                    <span class="n">key_states</span><span class="p">,</span>
                    <span class="n">value_states</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span>
                    <span class="p">{</span><span class="s2">&quot;cache_position&quot;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">},</span>
                <span class="p">)</span>
                <span class="c1"># set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls</span>
                <span class="k">if</span> <span class="n">is_cross_attention</span><span class="p">:</span>
                    <span class="n">past_key_value</span><span class="o">.</span><span class="n">is_updated</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># compute scores, equivalent of torch.einsum(&quot;bnqd,bnkd-&gt;bnqk&quot;, query_states, key_states), compatible with onnx op&gt;9</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">position_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_length</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
            <span class="c1"># cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)</span>
            <span class="n">real_seq_length</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">query_length</span> <span class="k">if</span> <span class="n">query_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cache_position</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># type: ignore</span>
            <span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_attention_bias</span><span class="p">:</span>
                <span class="n">position_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">key_length</span><span class="p">),</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">position_bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">position_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_bias</span><span class="p">(</span>
                    <span class="n">real_seq_length</span><span class="p">,</span>
                    <span class="n">key_length</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                    <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">position_bias</span> <span class="o">=</span> <span class="n">position_bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">seq_length</span><span class="p">:,</span> <span class="p">:]</span>

            <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
                <span class="n">position_bias</span> <span class="o">=</span> <span class="n">position_bias</span> <span class="o">+</span> <span class="n">causal_mask</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">position_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">mask</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">position_bias_masked</span> <span class="o">=</span> <span class="n">position_bias</span><span class="p">[:,</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">position_bias_masked</span> <span class="o">=</span> <span class="n">position_bias</span>

        <span class="n">scores</span> <span class="o">+=</span> <span class="n">position_bias_masked</span>

        <span class="c1"># (batch_size, n_heads, seq_length, key_length)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span>
        <span class="p">)</span>

        <span class="c1"># Mask heads if we want to</span>
        <span class="k">if</span> <span class="n">layer_head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">*</span> <span class="n">layer_head_mask</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span><span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">past_key_value</span><span class="p">,</span> <span class="n">position_bias</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="easyroutine.interpretability.module_wrappers.T5_attention.T5AttentionWrapper.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_value_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">layer_head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">query_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_position</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).</p>

            <details class="quote">
              <summary>Source code in <code>easyroutine/interpretability/module_wrappers/T5_attention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">key_value_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">position_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">layer_head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">query_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cache_position</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Input is (batch_size, seq_length, dim)</span>
    <span class="c1"># Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># if key_value_states are provided this layer is used as a cross-attention layer for the decoder</span>
    <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">key_value_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">query_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
    <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_updated</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">is_updated</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_cross_attention</span><span class="p">:</span>
            <span class="c1"># after the first generated id, we can subsequently re-use all key/value_states from cache</span>
            <span class="n">curr_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">cross_attention_cache</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">curr_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="o">.</span><span class="n">self_attention_cache</span>

    <span class="n">current_states</span> <span class="o">=</span> <span class="n">key_value_states</span> <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="k">else</span> <span class="n">hidden_states</span>
    <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">is_updated</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
        <span class="c1"># reuse k,v, cross_attentions</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">curr_past_key_value</span><span class="o">.</span><span class="n">key_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">curr_past_key_value</span><span class="o">.</span><span class="n">value_cache</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">current_states</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">current_states</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
        <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_value_proj_dim</span>
        <span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># save all key/value_states to cache to be re-used for fast auto-regressive generation</span>
            <span class="n">cache_position</span> <span class="o">=</span> <span class="n">cache_position</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cross_attention</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">curr_past_key_value</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">key_states</span><span class="p">,</span>
                <span class="n">value_states</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span>
                <span class="p">{</span><span class="s2">&quot;cache_position&quot;</span><span class="p">:</span> <span class="n">cache_position</span><span class="p">},</span>
            <span class="p">)</span>
            <span class="c1"># set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls</span>
            <span class="k">if</span> <span class="n">is_cross_attention</span><span class="p">:</span>
                <span class="n">past_key_value</span><span class="o">.</span><span class="n">is_updated</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># compute scores, equivalent of torch.einsum(&quot;bnqd,bnkd-&gt;bnqk&quot;, query_states, key_states), compatible with onnx op&gt;9</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">position_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">key_length</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="c1"># cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)</span>
        <span class="n">real_seq_length</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">query_length</span> <span class="k">if</span> <span class="n">query_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cache_position</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># type: ignore</span>
        <span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_relative_attention_bias</span><span class="p">:</span>
            <span class="n">position_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">key_length</span><span class="p">),</span>
                <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">position_bias</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">position_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_bias</span><span class="p">(</span>
                <span class="n">real_seq_length</span><span class="p">,</span>
                <span class="n">key_length</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">position_bias</span> <span class="o">=</span> <span class="n">position_bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">seq_length</span><span class="p">:,</span> <span class="p">:]</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
            <span class="n">position_bias</span> <span class="o">=</span> <span class="n">position_bias</span> <span class="o">+</span> <span class="n">causal_mask</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">position_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mask</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">position_bias_masked</span> <span class="o">=</span> <span class="n">position_bias</span><span class="p">[:,</span> <span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">position_bias_masked</span> <span class="o">=</span> <span class="n">position_bias</span>

    <span class="n">scores</span> <span class="o">+=</span> <span class="n">position_bias_masked</span>

    <span class="c1"># (batch_size, n_heads, seq_length, key_length)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_matrix_hook</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
    <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
        <span class="n">attn_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span>
    <span class="p">)</span>

    <span class="c1"># Mask heads if we want to</span>
    <span class="k">if</span> <span class="n">layer_head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">*</span> <span class="n">layer_head_mask</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">value_states</span><span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_dim</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">past_key_value</span><span class="p">,</span> <span class="n">position_bias</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="p">(</span><span class="n">attn_weights</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>